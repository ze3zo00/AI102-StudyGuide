<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI-102 Complete Interactive Study Guide</title>
    <!-- Document metadata -->
    <meta name="author" content="Zezo">
    <meta name="version" content="v3.0.0">
    <meta name="publish-date" content="Nov 29, 2025">
    <style>
        /* CSS Variables for Light/Dark Mode */
        :root {
            --bg-primary: #ffffff;
            --bg-secondary: #f8f9fa;
            --text-primary: #1a1a1a;
            --text-secondary: #666666;
            --border-color: #e0e0e0;
            --code-bg: #f5f5f5;
            --code-border: #e0e0e0;
            --pre-bg: #1a1a1a;
            --table-border: #dddddd;
            --table-header-bg: #f5f5f5;
            --shadow-color: rgba(0, 0, 0, 0.1);
            --accent-color: #ffcc00;
            --sidebar-bg: #1a1a1a;
            --sidebar-text: #ffffff;
            --tip-bg: #fffbea;
            --tip-text: #1a1a1a;
            --warning-bg: #ffe6e6;
            --warning-text: #cc0000;
            --hr-color: #e0e0e0;
        }

        [data-theme="dark"] {
            --bg-primary: #1a1a1a;
            --bg-secondary: #2a2a2a;
            --text-primary: #e0e0e0;
            --text-secondary: #a0a0a0;
            --border-color: #444444;
            --code-bg: #2a2a2a;
            --code-border: #444444;
            --pre-bg: #3a3a3a;
            --table-border: #444444;
            --table-header-bg: #2a2a2a;
            --shadow-color: rgba(0, 0, 0, 0.3);
            --accent-color: #ffcc00;
            --sidebar-bg: #0a0a0a;
            --sidebar-text: #ffffff;
            --tip-bg: rgba(255, 204, 0, 0.15);
            --tip-text: #e0e0e0;
            --warning-bg: rgba(255, 0, 0, 0.15);
            --warning-text: #ff6666;
            --hr-color: #444444;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
            line-height: 1.7;
            color: var(--text-primary);
            background-color: var(--bg-primary);
            transition: background-color 0.3s ease, color 0.3s ease;
        }
        
        /* Sidebar Styles - Black background */
        .sidebar {
            position: fixed;
            left: 0;
            top: 0;
            width: 340px;
            height: 100vh;
            background-color: var(--sidebar-bg);
            color: var(--sidebar-text);
            overflow-y: auto;
            padding: 0;
            transition: background-color 0.3s ease;
            box-shadow: 4px 0 12px rgba(0,0,0,0.3);
            z-index: 1000;
            border-right: 3px solid #333;
        }
        
        .sidebar-header {
            padding: 25px 20px;
            background-color: #000000;
            border-bottom: 3px solid #ffcc00;
            position: sticky;
            top: 0;
            z-index: 10;
        }
        .mobile-close {
            display: none;
            position: absolute;
            right: 18px;
            top: 18px;
            background: transparent;
            color: #ffffff;
            border: none;
            font-size: 20px;
            cursor: pointer;
            z-index: 1105;
        }
                /* Responsive content: prevent horizontal scroll */
                body {
                    overflow-x: hidden;
                }
                /* Make code snippets wrap on small screens */
                pre, code {
                    white-space: pre-wrap !important;
                    word-break: break-word !important;
                    max-width: 100%;
                }
                img, table {
                    max-width: 100%;
                    height: auto;
                }
        @media (max-width: 1024px) {
            .mobile-close { display: block; }
        }

        /* Show mobile menu for touch devices regardless of width */
        @media (hover: none) and (pointer: coarse) {
            .mobile-menu-btn { display: block !important; }
            .mobile-close { display: block !important; }
        }
        
        .sidebar-header h1 {
            font-size: 22px;
            margin-bottom: 8px;
            font-weight: 700;
            color: #ffffff;
        }

        /* Sidebar control buttons (collapse/expand all) */
        .sidebar-controls {
            display: flex;
            gap: 8px;
            margin-top: 10px;
        }
        .sidebar-controls button {
            padding: 6px 10px;
            border-radius: 6px;
            font-size: 13px;
            border: 1px solid rgba(255,255,255,0.06);
            background: rgba(255,255,255,0.02);
            color: #ffffff;
            cursor: pointer;
            font-weight: 700;
        }
        .sidebar-controls button:hover { background: rgba(255,255,255,0.04); }

        /* Theme toggle button */
        .theme-toggle {
            position: fixed;
            bottom: 110px;
            right: 40px;
            background-color: #000000;
            color: #ffcc00;
            width: 56px;
            height: 56px;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            cursor: pointer;
            box-shadow: 0 6px 16px rgba(0,0,0,0.4);
            z-index: 1095;
            transition: all 0.3s;
            font-size: 24px;
            font-weight: bold;
            border: 3px solid #ffcc00;
            opacity: 0;
            visibility: hidden;
        }

        .theme-toggle.visible {
            opacity: 1;
            visibility: visible;
        }

        .theme-toggle:hover {
            transform: translateY(-8px);
            box-shadow: 0 10px 24px rgba(0,0,0,0.5);
            background-color: #ffcc00;
            color: #000000;
        }

        /* Debug mode visual aids */
        .debug-mode .mobile-menu-btn { outline: 3px solid rgba(255,0,0,0.7); box-shadow: 0 6px 18px rgba(255,0,0,0.15); }
        .debug-mode .sidebar { outline: 2px dashed rgba(0,255,0,0.7); }
        .debug-mode .sidebar-overlay { outline: 2px dashed rgba(0,0,255,0.6); }
        
        .sidebar-header .subtitle {
            font-size: 13px;
            color: #cccccc;
            font-weight: 400;
        }
        
        .sidebar-content {
            padding: 15px 0;
        }
        
        .exam-info {
            font-size: 11px;
            background-color: #2a2a2a;
            padding: 12px 15px;
            margin: 0 15px 20px 15px;
            border-radius: 6px;
            line-height: 1.6;
            border-left: 4px solid #ffcc00;
        }
        
        .exam-info strong {
            display: block;
            margin-bottom: 6px;
            font-size: 12px;
            color: #ffcc00;
        }

        /* Search box styles */
        .search-container {
            padding: 0 15px 15px 15px;
            margin-bottom: 10px;
        }

        .search-box {
            width: 100%;
            padding: 10px 12px;
            background-color: #2a2a2a;
            border: 2px solid #444;
            border-radius: 6px;
            color: #ffffff;
            font-size: 14px;
            transition: border-color 0.3s;
        }

        .search-box:focus {
            border-color: #ffcc00;
            outline: none;
        }

        .search-box::placeholder {
            color: #888;
        }

        .search-results-info {
            padding: 8px 15px;
            margin: 0 15px 10px 15px;
            background-color: #2a2a2a;
            border-radius: 6px;
            font-size: 12px;
            color: #ffcc00;
            display: none;
        }

        .search-highlight {
            background-color: #ffcc00;
            color: #000000;
            padding: 2px 4px;
            border-radius: 2px;
            font-weight: bold;
        }

        .nav-subsections a.search-match {
            background-color: rgba(255, 204, 0, 0.15);
            border-left: 3px solid #ffcc00;
        }

        .sidebar::-webkit-scrollbar {
            width: 10px;
        }
        
        .sidebar::-webkit-scrollbar-track {
            background: #0a0a0a;
        }
        
        .sidebar::-webkit-scrollbar-thumb {
            background: #444444;
            border-radius: 5px;
        }
        
        .sidebar::-webkit-scrollbar-thumb:hover {
            background: #666666;
        }
        
        .nav-section {
            margin-bottom: 8px;
        }
        
        .nav-section-title {
            font-size: 13.5px;
            font-weight: 600;
            padding: 10px 20px;
            background-color: #2a2a2a;
            cursor: pointer;
            transition: all 0.2s;
            display: flex;
            align-items: center;
            justify-content: space-between;
            border-left: 4px solid transparent;
            color: #ffffff;
        }
        
        .nav-section-title:hover {
            background-color: #333333;
            border-left-color: #ffcc00;
        }
        
        .nav-section-title .icon {
            font-size: 16px;
            margin-right: 8px;
        }
        
        .nav-section-title .toggle {
            font-size: 10px;
            opacity: 0.7;
            transition: transform 0.2s;
        }
        
        .nav-section-title.collapsed .toggle {
            transform: rotate(-90deg);
        }
        
        .nav-subsections {
            max-height: 2000px;
            overflow: hidden;
            transition: max-height 0.3s ease-out, opacity 0.2s;
            opacity: 1;
            background-color: #0f0f0f;
        }
        
        .nav-subsections.collapsed {
            max-height: 0;
            opacity: 0;
        }
        
        .nav-subsections a {
            display: block;
            color: #cccccc;
            text-decoration: none;
            font-size: 12.5px;
            padding: 8px 20px 8px 45px;
            transition: all 0.15s;
            border-left: 3px solid transparent;
        }
        
        .nav-subsections a:hover {
            background-color: #2a2a2a;
            color: #ffffff;
            border-left-color: #ffcc00;
        }
        
        .nav-subsections a.active {
            background-color: #333333;
            color: #ffcc00;
            font-weight: 600;
            border-left-color: #ffcc00;
        }
        
        .weight-badge {
            font-size: 10px;
            background-color: #ffcc00;
            color: #000000;
            padding: 3px 7px;
            border-radius: 10px;
            font-weight: 700;
            margin-left: 8px;
        }
        
        .high-priority {
            background-color: #ff0000;
            color: #ffffff;
            animation: pulse 2s infinite;
        }
        
        @keyframes pulse {
            0%, 100% { opacity: 1; }
            50% { opacity: 0.8; }
        }
        
        /* Main Content Styles - White background */
        .main-content {
            margin-left: 340px;
            padding: 0;
            background-color: var(--bg-primary);
            min-height: 100vh;
            transition: background-color 0.3s ease;
        }
        .site-footer {
            background: #000000;
            color: #ffcc00;
            padding: 14px 20px;
            text-align: center;
            border-top: 3px solid #222;
        }
        .site-footer .footer-inner { max-width: 1400px; margin: 0 auto; font-size: 14px; }
        
        .content-header {
            background-color: #000000;
            color: #ffffff;
            padding: 50px 60px;
            box-shadow: 0 4px 12px rgba(0,0,0,0.2);
            border-bottom: 5px solid #ffcc00;
        }
        
        .content-header h1 {
            font-size: 38px;
            margin-bottom: 12px;
            font-weight: 700;
            line-height: 1.2;
            color: #ffffff;
        }
        
        .content-header p {
            font-size: 18px;
            color: #cccccc;
            font-weight: 300;
        }
        .doc-meta {
            display: flex;
            gap: 14px;
            margin-top: 10px;
            font-size: 13px;
            color: #d1d5db;
            align-items: center;
        }
        .doc-meta .meta-item {
            background: rgba(255,255,255,0.03);
            padding: 6px 10px;
            border-radius: 8px;
            border: 1px solid rgba(255,255,255,0.04);
            color: #f3f4f6;
            font-weight: 600;
        }
        
        .content-body {
            padding: 40px 60px 80px 60px;
            max-width: 1400px;
        }
        
        section {
            margin-bottom: 50px;
            scroll-margin-top: 20px;
        }
        
        .section-divider {
            height: 3px;
            background-color: var(--border-color);
            margin: 60px 0 40px 0;
        }

        h1 {
            color: var(--text-primary);
            font-size: 36px;
            margin-bottom: 30px;
            padding-bottom: 15px;
            border-bottom: 3px solid var(--border-color);
            font-weight: 700;
        }

        h2 {
            color: var(--text-primary);
            font-size: 30px;
            margin: 40px 0 25px 0;
            padding-bottom: 12px;
            border-bottom: 2px solid var(--border-color);
            font-weight: 600;
        }
        
        h3 {
            color: var(--text-primary);
            font-size: 24px;
            margin-top: 35px;
            margin-bottom: 18px;
            font-weight: 600;
        }

        h4 {
            color: var(--text-primary);
            font-size: 19px;
            margin-top: 25px;
            margin-bottom: 12px;
            font-weight: 600;
        }

        h5 {
            color: var(--text-primary);
            font-size: 16px;
            margin-top: 20px;
            margin-bottom: 10px;
            font-weight: 600;
        }

        p {
            margin-bottom: 18px;
            color: var(--text-primary);
            font-size: 15.5px;
            line-height: 1.75;
        }

        code {
            background-color: var(--code-bg);
            padding: 3px 7px;
            border-radius: 4px;
            font-family: 'Consolas', 'Monaco', 'Courier New', monospace;
            font-size: 14px;
            color: #d63384;
            border: 1px solid var(--code-border);
        }
        
        pre {
            background-color: var(--pre-bg);
            color: #f8f8f2;
            padding: 24px;
            border-radius: 8px;
            overflow-x: auto;
            margin: 25px 0;
            border-left: 5px solid #ffcc00;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }
        
        pre code {
            background: none;
            color: inherit;
            padding: 0;
            border: none;
            font-size: 13.5px;
            line-height: 1.6;
        }
        
        .tip-box {
            background-color: var(--tip-bg);
            border-left: 5px solid #ffcc00;
            padding: 20px 24px;
            margin: 25px 0;
            border-radius: 6px;
            box-shadow: 0 2px 6px rgba(255,204,0,0.2);
        }

        .tip-box strong {
            color: var(--text-primary);
            font-size: 16px;
            display: block;
            margin-bottom: 8px;
        }

        .tip-box p {
            margin-bottom: 0;
            color: var(--tip-text);
        }

        .warning-box {
            background-color: var(--warning-bg);
            border-left: 5px solid #ff0000;
            padding: 20px 24px;
            margin: 25px 0;
            border-radius: 6px;
            box-shadow: 0 2px 6px rgba(255,0,0,0.2);
        }

        .warning-box strong {
            color: var(--warning-text);
            font-size: 16px;
            display: block;
            margin-bottom: 8px;
        }
        
        .warning-box p {
            margin-bottom: 0;
            color: var(--text-primary);
        }
        
        /* Table wrapper for horizontal scrolling on small screens */
        .table-wrapper {
            width: 100%;
            overflow-x: auto;
            margin: 30px 0;
            border-radius: 8px;
            box-shadow: 0 2px 8px var(--shadow-color);
            border: 2px solid var(--border-color);
            -webkit-overflow-scrolling: touch; /* Smooth scrolling on iOS */
        }

        .table-wrapper::-webkit-scrollbar {
            height: 8px;
        }

        .table-wrapper::-webkit-scrollbar-track {
            background: var(--bg-secondary);
            border-radius: 4px;
        }

        .table-wrapper::-webkit-scrollbar-thumb {
            background: #888;
            border-radius: 4px;
        }

        .table-wrapper::-webkit-scrollbar-thumb:hover {
            background: #555;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: 0;
            background-color: var(--bg-primary);
            border-radius: 8px;
            overflow: hidden;
            min-width: 600px; /* Ensures table doesn't compress too much */
        }

        thead {
            background-color: #1a1a1a;
        }

        th {
            background-color: #1a1a1a;
            color: #ffffff;
            padding: 16px 20px;
            text-align: left;
            font-weight: 600;
            font-size: 14px;
            text-transform: uppercase;
            letter-spacing: 0.5px;
            border-bottom: 3px solid #ffcc00;
        }

        td {
            padding: 16px 20px;
            border-bottom: 1px solid var(--table-border);
            font-size: 14.5px;
            color: var(--text-primary);
        }

        tbody tr:nth-child(even) {
            background-color: var(--bg-secondary);
        }

        tbody tr:hover {
            background-color: rgba(255, 204, 0, 0.1);
        }
        
        tr:last-child td {
            border-bottom: none;
        }
        
        ul, ol {
            margin-left: 35px;
            margin-bottom: 20px;
        }
        
        li {
            margin-bottom: 10px;
            color: var(--text-primary);
            font-size: 15.5px;
            line-height: 1.7;
        }

        strong {
            font-weight: 600;
            color: var(--text-primary);
        }

        em {
            font-style: italic;
            color: var(--text-secondary);
        }

        a {
            color: var(--text-primary);
            text-decoration: underline;
            transition: color 0.2s;
        }
        
        a:hover {
            color: #ffcc00;
        }
        
        hr {
            border: none;
            border-top: 2px solid var(--hr-color);
            margin: 40px 0;
        }
        
        .back-to-top {
            position: fixed;
            bottom: 40px;
            right: 40px;
            background-color: #000000;
            color: #ffcc00;
            width: 56px;
            height: 56px;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            cursor: pointer;
            box-shadow: 0 6px 16px rgba(0,0,0,0.4);
            z-index: 1090;
            transition: all 0.3s;
            opacity: 0;
            visibility: hidden;
            font-size: 24px;
            font-weight: bold;
            border: 3px solid #ffcc00;
        }
        
        .back-to-top.visible {
            opacity: 1;
            visibility: visible;
        }
        
        .back-to-top:hover {
            transform: translateY(-8px);
            box-shadow: 0 10px 24px rgba(0,0,0,0.5);
            background-color: #ffcc00;
            color: #000000;
        }
        
        @media print {
            .sidebar, .back-to-top, .mobile-menu-btn, .theme-toggle {
                display: none;
            }
            .main-content {
                margin-left: 0;
            }
            .content-body {
                padding: 20px;
            }
        }
        
        @media (max-width: 1024px) {
            .sidebar {
                width: 280px;
                /* hide sidebar by default on tablet & phone sizes */
                transform: translateX(-100%);
                transition: transform 0.3s;
            }
            .sidebar.mobile-open {
                transform: translateX(0);
            }

        /* Make mobile-open always visible when toggled (safety fallback) */
        .sidebar.mobile-open {
            transform: translateX(0) !important;
        }
            .main-content {
                margin-left: 280px;
            }
            .content-header, .content-body {
                padding-left: 30px;
                padding-right: 30px;
            }
            .mobile-menu-btn {
                display: flex;
                align-items: center;
                justify-content: center;
                position: fixed;
                top: 20px;
                left: 20px;
                z-index: 1100;
                background-color: #000000;
                color: #ffcc00;
                border: 3px solid #ffcc00;
                width: 56px;
                height: 56px;
                border-radius: 50%;
                cursor: pointer;
                font-size: 24px;
                font-weight: bold;
                box-shadow: 0 6px 16px rgba(0,0,0,0.4);
                transition: opacity 0.3s ease, transform 0.3s ease, background-color 0.3s ease;
                opacity: 1;
                visibility: visible;
            }

            /* Fade on scroll */
            .mobile-menu-btn.scrolling {
                opacity: 0.3;
            }

            .mobile-menu-btn.scrolling:hover,
            .mobile-menu-btn.scrolling:focus {
                opacity: 1;
            }

            .mobile-menu-btn:hover {
                transform: translateY(-8px);
                box-shadow: 0 10px 24px rgba(0,0,0,0.5);
                background-color: #ffcc00;
                color: #000000;
            }

            .mobile-menu-btn.active {
                opacity: 1 !important;
                background-color: #ffcc00;
                color: #000000;
            }
        }
        
        @media (max-width: 768px) {
            .sidebar {
                transform: translateX(-100%);
                transition: transform 300ms cubic-bezier(.2,.8,.2,1);
            }
            .sidebar.mobile-open {
                transform: translateX(0);
                transition: transform 300ms cubic-bezier(.2,.8,.2,1);
                will-change: transform;
            }
            .main-content {
                margin-left: 0;
            }
            .content-header {
                padding: 30px 20px;
            }
            .content-header h1 {
                font-size: 28px;
            }
            .content-body {
                padding: 20px;
            }
            .mobile-menu-btn {
                display: flex !important;
                align-items: center;
                justify-content: center;
                position: fixed;
                top: 20px;
                left: 20px;
                z-index: 1100;
                background-color: #000000;
                color: #ffcc00;
                border: 3px solid #ffcc00;
                width: 56px;
                height: 56px;
                border-radius: 50%;
                cursor: pointer;
                font-size: 24px;
                font-weight: bold;
                box-shadow: 0 6px 16px rgba(0,0,0,0.4);
                transition: opacity 0.3s ease, transform 0.3s ease, background-color 0.3s ease;
                opacity: 1;
                visibility: visible;
            }

            /* Fade on scroll */
            .mobile-menu-btn.scrolling {
                opacity: 0.3;
            }

            .mobile-menu-btn.scrolling:hover,
            .mobile-menu-btn.scrolling:focus {
                opacity: 1;
            }

            .mobile-menu-btn:hover {
                transform: translateY(-8px);
                box-shadow: 0 10px 24px rgba(0,0,0,0.5);
                background-color: #ffcc00;
                color: #000000;
            }

            .mobile-menu-btn.active {
                opacity: 1 !important;
                background-color: #ffcc00;
                color: #000000;
            }
        }

        /* Mobile overlay for sidebar (fade in/out) */
        .sidebar-overlay {
            position: fixed;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background: rgba(0,0,0,0.45);
            z-index: 995;
            opacity: 0;
            pointer-events: none;
            transition: opacity 300ms ease-in-out;
        }
        .sidebar-overlay.visible {
            opacity: 1;
            pointer-events: auto;
            transition-delay: 90ms; /* slightly delayed when showing, to sync with slide */
        }
                /* Accessibility: screen-reader-only text */
                .sr-only {
                    position: absolute !important;
                    height: 1px; width: 1px;
                    overflow: hidden;
                    clip: rect(1px, 1px, 1px, 1px);
                    white-space: nowrap; /* added line */
                }
        .no-scroll { overflow: hidden; }
        
        /* menu position: default to left on mobile */
        .menu-position-left .mobile-menu-btn, .menu-position-left .mobile-menu-btn.scrolling, .menu-position-left .mobile-menu-btn.active {
            left: 20px;
            right: auto;
        }
        .menu-position-right .mobile-menu-btn, .menu-position-right .mobile-menu-btn.scrolling, .menu-position-right .mobile-menu-btn.active {
            right: calc(40px + 56px + 12px);
            left: auto;
        }

        .mobile-menu-btn {
            display: none;
        }

        /* Focus styles for accessibility */
        button:focus,
        a:focus,
        [role="button"]:focus,
        input:focus,
        select:focus,
        textarea:focus {
            outline: 3px solid #ffcc00;
            outline-offset: 2px;
        }

        .nav-subsections a:focus {
            outline: 2px solid #ffcc00;
            outline-offset: 1px;
            background-color: rgba(255, 204, 0, 0.1);
        }

        .back-to-top:focus,
        .mobile-menu-btn:focus {
            outline: 3px solid #ffcc00;
            outline-offset: 3px;
        }

        /* Smooth scrolling for anchor navigation */
        html {
            scroll-behavior: smooth;
        }
    </style>
</head>
<body>
    <button class="mobile-menu-btn" id="mobileMenuBtn" onclick="toggleMobileMenu()" aria-label="Toggle navigation" aria-expanded="false">‚ò∞</button>
    
    <!-- Sidebar Navigation -->
    <nav class="sidebar" id="sidebar">
        <div class="sidebar-header">
            <h1>AI-102 Study Guide</h1>
            <div class="subtitle">Complete Interactive Reference</div>
            <div class="sidebar-controls" role="group" aria-label="Sidebar section controls">
                <button class="sidebar-btn" id="expand-all" onclick="expandAllSections()" aria-label="Expand all sections">Expand All</button>
                <button class="sidebar-btn" id="collapse-all" onclick="collapseAllSections()" aria-label="Collapse all sections">Collapse All</button>
            </div>
            <button class="mobile-close" id="mobile-close" onclick="toggleMobileMenu()" aria-label="Close sidebar">‚úï</button>
        </div>
        
        <div class="sidebar-content">
            <div class="search-container">
                <input type="text" class="search-box" id="searchBox" placeholder="üîç Search guide..." aria-label="Search the study guide">
            </div>
            <div class="search-results-info" id="searchResultsInfo"></div>

            <div class="exam-info">
                <strong>üìã Exam Details</strong>
                Questions: 40-60 | Duration: 100-120 min<br>
                Passing Score: 700/1000 | Cost: $165 USD
            </div>

            <div class="nav-section">
                <div class="nav-section-title" onclick="toggleSection(this)">
                    <span><span class="icon">üìã</span>Quick Reference</span>
                    <span class="toggle">‚ñº</span>
                </div>
                <div class="nav-subsections">
                    <a href="#tldr-quick-reference-guide">TL;DR - All Services</a>
                    <a href="#how-to-use-this-guide">How to Use This Guide</a>
                </div>
            </div>
            
            <div class="nav-section">
                <div class="nav-section-title" onclick="toggleSection(this)">
                    <span><span class="icon">üìÅ</span>Section 1: Plan & Manage<span class="weight-badge">15-20%</span></span>
                    <span class="toggle">‚ñº</span>
                </div>
                <div class="nav-subsections">
                    <a href="#section-1-plan-and-manage-an-azure-ai-solution-15-20">Overview</a>
                    <a href="#11-understanding-azure-ai-services">1.1 Azure AI Services</a>
                    <a href="#12-authentication-and-authorization">1.2 Authentication</a>
                    <a href="#13-network-security-for-ai-services">1.3 Network Security</a>
                    <a href="#14-monitoring-and-diagnostics">1.4 Monitoring</a>
                    <a href="#15-cost-management-and-optimization">1.5 Cost Management</a>
                </div>
            </div>
            
            <div class="nav-section">
                <div class="nav-section-title" onclick="toggleSection(this)">
                    <span><span class="icon">üîç</span>Section 2: Decision Support<span class="weight-badge">10-15%</span></span>
                    <span class="toggle">‚ñº</span>
                </div>
                <div class="nav-subsections">
                    <a href="#section-2-implement-decision-support-solutions-10-15">Overview</a>
                    <a href="#21-azure-ai-content-safety-comprehensive-understanding">2.1 Content Safety</a>
                    <a href="#22-anomaly-detector-service-comprehensive-understanding">2.2 Anomaly Detector</a>
                </div>
            </div>
            
            <div class="nav-section">
                <div class="nav-section-title" onclick="toggleSection(this)">
                    <span><span class="icon">üëÅÔ∏è</span>Section 3: Vision Solutions<span class="weight-badge">15-20%</span></span>
                    <span class="toggle">‚ñº</span>
                </div>
                <div class="nav-subsections">
                    <a href="#section-3-implement-azure-ai-vision-solutions-15-20">Overview</a>
                    <a href="#31-computer-vision-service-comprehensive-understanding">3.1 Computer Vision</a>
                    <a href="#32-optical-character-recognition-ocr-read-api">3.2 OCR / Read API</a>
                    <a href="#33-custom-vision-when-general-models-arent-enough">3.3 Custom Vision</a>
                    <a href="#34-face-api-identity-verification-critical-limited-access-required">3.4 Face API</a>
                </div>
            </div>
            
            <div class="nav-section">
                <div class="nav-section-title" onclick="toggleSection(this)">
                    <span><span class="icon">üí¨</span>Section 4: NLP<span class="weight-badge high-priority">30-35%</span></span>
                    <span class="toggle">‚ñº</span>
                </div>
                <div class="nav-subsections">
                    <a href="#section-4-natural-language-processing-30-35">Overview</a>
                    <a href="#41-azure-ai-language-text-analytics-comprehensive-coverage">4.1 Text Analytics</a>
                    <a href="#42-speech-service-speech-recognition-and-synthesis">4.2 Speech Service</a>
                    <a href="#43-translator-service">4.3 Translator</a>
                    <a href="#44-conversational-language-understanding-clu-and-luis">4.4 CLU / LUIS</a>
                    <a href="#45-question-answering-service">4.5 Question Answering</a>
                </div>
            </div>
            
            <div class="nav-section">
                <div class="nav-section-title" onclick="toggleSection(this)">
                    <span><span class="icon">üìö</span>Section 5: Knowledge Mining<span class="weight-badge">10-15%</span></span>
                    <span class="toggle">‚ñº</span>
                </div>
                <div class="nav-subsections">
                    <a href="#section-5-knowledge-mining-and-document-intelligence-10-15">Overview</a>
                    <a href="#51-azure-cognitive-search-ai-powered-search">5.1 Cognitive Search</a>
                    <a href="#52-document-intelligence-form-recognizer">5.2 Document Intelligence</a>
                </div>
            </div>
            
            <div class="nav-section">
                <div class="nav-section-title" onclick="toggleSection(this)">
                    <span><span class="icon">ü§ñ</span>Section 6: Generative AI<span class="weight-badge">10-15%</span></span>
                    <span class="toggle">‚ñº</span>
                </div>
                <div class="nav-subsections">
                    <a href="#section-6-generative-ai-with-azure-openai-10-15">Overview</a>
                    <a href="#61-azure-openai-service-chat-completions">6.1 Azure OpenAI</a>
                    <a href="#62-embeddings-for-semantic-search">6.2 Embeddings</a>
                </div>
            </div>
            
            <div class="nav-section">
                <div class="nav-section-title" onclick="toggleSection(this)">
                    <span><span class="icon">üéØ</span>Exam Preparation</span>
                    <span class="toggle">‚ñº</span>
                </div>
                <div class="nav-subsections">
                    <a href="#final-exam-preparation">Final Preparation</a>
                    <a href="#key-exam-strategies">Key Strategies</a>
                    <a href="#common-exam-question-patterns">Question Patterns</a>
                </div>
            </div>
        </div>
    </nav>
    <div class="sidebar-overlay" id="sidebar-overlay" onclick="toggleMobileMenu()" aria-hidden="true"></div>
    <div id="aria-live" class="sr-only" aria-live="polite" aria-atomic="true"></div>

    <!-- Main Content -->
    <main class="main-content">
        <div class="content-header">
            <h1>AI-102: Designing and Implementing a Microsoft Azure AI Solution</h1>
            <p>Your Complete Interactive Study Guide - Everything You Need to Pass</p>
            <div class="doc-meta" aria-hidden="false">
                <div class="meta-item" id="doc-author">Author: Zezo</div>
                <div class="meta-item" id="doc-version">Version: v3.0.0</div>
                <div class="meta-item" id="doc-publish-date">Published: Nov 29, 2025</div>
            </div>
        </div>

        <div class="content-body">
<section id="ai-102-designing-and-implementing-a-microsoft-azure-ai-solution"><h1>AI-102: Designing and Implementing a Microsoft Azure AI Solution</h1>
<section id="the-ultimate-complete-study-guide-your-only-resource-needed"><h2>The Ultimate Complete Study Guide - Your Only Resource Needed</h2>
<div class="tip-box"><p><strong>Exam Information:</strong></p></div>
<ul>
<li><strong>Passing Score:</strong> 700/1000</li>
<li><strong>Duration:</strong> 100-120 minutes  </li>
<li><strong>Questions:</strong> 40-60 (multiple choice, drag-drop, case studies)</li>
<li><strong>Cost:</strong> $165 USD</li>
<li><strong>Certification:</strong> Microsoft Certified: Azure AI Engineer Associate</li>
</ul>
<div class="tip-box"><p><strong>Exam Weight Distribution:</strong></p></div>
<ul>
<li>Plan and Manage an Azure AI Solution: <strong>15-20%</strong></li>
<li>Implement Decision-Support Solutions: <strong>10-15%</strong></li>
<li>Implement Azure AI Vision Solutions: <strong>15-20%</strong></li>
<li><strong>Implement Natural Language Processing Solutions: 30-35%</strong> ‚ö†Ô∏è HIGHEST WEIGHT</li>
<li>Implement Knowledge Mining and Document Intelligence Solutions: <strong>10-15%</strong></li>
<li>Implement Generative AI Solutions: <strong>10-15%</strong></li>
</ul>
<hr>
<section id="table-of-contents-quick-navigation"><h2>üìë TABLE OF CONTENTS - QUICK NAVIGATION</h2>
<h3 id="toc-quick-reference">Quick Reference</h3>
<ul>
<li><a href="#tldr-quick-reference-guide">TL;DR - All Services Quick Reference</a></li>
</ul>
<h3 id="toc-section-1-plan-and-manage-azure-ai-solutions-15-20">Section 1: Plan and Manage Azure AI Solutions (15-20%)</h3>
<ul>
<li><a href="#11-understanding-azure-ai-services">1.1 Understanding Azure AI Services</a></li>
<li><a href="#what-are-azure-ai-services-and-why-do-they-exist">What Are Azure AI Services</a></li>
            <li><a href="#multi-service-vs-single-service-resources-deep-dive">Multi-Service vs Single-Service Resources</a></li>
<li><a href="#rest-api-architecture-pattern">REST API Architecture Pattern</a></li>
<li><a href="#python-sdk-architecture-pattern">Python SDK Architecture Pattern</a></li>
<li><a href="#pricing-tiers-comprehensive-understanding">Pricing Tiers - Free, Standard, Commitment</a></li>
<li><a href="#12-authentication-and-authorization">1.2 Authentication and Authorization</a></li>
<li><a href="#api-key-authentication-how-it-works">API Key Authentication</a></li>
<li><a href="#azure-ad-managed-identity-authentication-comprehensive-understanding">Azure AD / Managed Identity Authentication</a></li>
<li><a href="#configuring-role-based-access-control-rbac">Role-Based Access Control (RBAC)</a></li>
<li><a href="#13-network-security-for-ai-services">1.3 Network Security</a></li>
<li><a href="#public-endpoint-with-ip-firewall">Public Endpoint with IP Firewall</a></li>
<li><a href="#service-endpoints">Service Endpoints</a></li>
<li><a href="#private-endpoints-comprehensive-deep-dive">Private Endpoints</a></li>
<li><a href="#14-monitoring-and-diagnostics">1.4 Monitoring and Diagnostics</a></li>
<li><a href="#azure-monitor-metrics-what-to-monitor-and-why">Azure Monitor Metrics</a></li>
<li><a href="#diagnostic-logs-comprehensive-understanding">Diagnostic Logs</a></li>
<li><a href="#creating-effective-alerts">Creating Effective Alerts</a></li>
<li><a href="#15-cost-management-and-optimization">1.5 Cost Management</a></li>
<li><a href="#understanding-cost-drivers">Understanding Cost Drivers</a></li>
<li><a href="#tracking-costs-with-azure-cost-management">Tracking Costs</a></li>
<li><a href="#cost-optimization-strategies">Cost Optimization Strategies</a></li>
</ul>
<h3 id="toc-section-2-decision-support-solutions-10-15">Section 2: Decision-Support Solutions (10-15%)</h3>
<ul>
<li><a href="#21-azure-ai-content-safety-comprehensive-understanding">2.1 Azure AI Content Safety</a></li>
<li><a href="#what-is-content-safety-and-why-does-it-exist">What Is Content Safety</a></li>
<li><a href="#content-categories-explained">Content Categories Explained</a></li>
<li><a href="#severity-levels-and-threshold-configuration">Severity Levels and Thresholds</a></li>
<li><a href="#rest-api-implementation">REST API Implementation</a></li>
<li><a href="#python-sdk-implementation">Python SDK Implementation</a></li>
<li><a href="#analyzing-images-for-harmful-content">Analyzing Images</a></li>
<li><a href="#custom-blocklists-advanced-content-moderation">Custom Blocklists</a></li>
<li><a href="#when-to-use-content-safety-exam-decision-framework">When to Use Content Safety</a></li>
<li><a href="#22-anomaly-detector-service-comprehensive-understanding">2.2 Anomaly Detector Service</a></li>
<li><a href="#what-is-anomaly-detector-and-why-does-it-exist">What Is Anomaly Detector</a></li>
<li><a href="#univariate-vs-multivariate-anomaly-detection">Univariate vs Multivariate Detection</a></li>
<li><a href="#univariate-anomaly-detection-implementation">Univariate Implementation</a></li>
<li><a href="#sensitivity-parameter-critical-understanding">Sensitivity Parameter</a></li>
<li><a href="#multivariate-anomaly-detection-advanced-implementation">Multivariate Implementation</a></li>
<li><a href="#when-to-use-anomaly-detector-exam-decision-framework">When to Use Anomaly Detector</a></li>
</ul>
<h3 id="toc-section-3-vision-solutions-15-20">Section 3: Vision Solutions (15-20%)</h3>
<ul>
<li><a href="#31-computer-vision-service-comprehensive-understanding">3.1 Computer Vision Service</a></li>
<li><a href="#what-is-computer-vision-and-what-problems-does-it-solve">What Is Computer Vision</a></li>
<li><a href="#when-to-use-computer-vision-vs-custom-vision-vs-face-api">When to Use Computer Vision vs Custom Vision vs Face API</a></li>
<li><a href="#computer-vision-rest-api-analyze-image">REST API - Analyze Image</a></li>
<li><a href="#python-sdk-image-analysis">Python SDK - Image Analysis</a></li>
<li><a href="#adult-content-detection-important-use-case">Adult Content Detection</a></li>
<li><a href="#32-optical-character-recognition-ocr-read-api">3.2 Optical Character Recognition (OCR)</a></li>
<li><a href="#read-api-vs-legacy-ocr-api">Read API vs Legacy OCR</a></li>
<li><a href="#read-api-complete-workflow">Read API Workflow</a></li>
<li><a href="#python-sdk-read-api-implementation">Python SDK Implementation</a></li>
<li><a href="#ocr-best-practices-and-optimization">OCR Best Practices</a></li>
<li><a href="#33-custom-vision-when-general-models-arent-enough">3.3 Custom Vision</a></li>
<li><a href="#classification-vs-object-detection">Classification vs Object Detection</a></li>
<li><a href="#training-custom-vision-models-complete-workflow">Training Custom Models</a></li>
<li><a href="#using-custom-vision-for-predictions">Using Custom Vision for Predictions</a></li>
<li><a href="#object-detection-with-custom-vision">Object Detection Implementation</a></li>
<li><a href="#improving-model-accuracy">Improving Model Accuracy</a></li>
<li><a href="#34-face-api-identity-verification-critical-limited-access-required">3.4 Face API</a></li>
<li><a href="#face-detection-vs-verification-vs-identification">Face Detection vs Verification vs Identification</a></li>
<li><a href="#rest-api-face-detection">REST API - Face Detection</a></li>
<li><a href="#python-sdk-face-detection">Python SDK - Face Detection</a></li>
<li><a href="#face-verification-requires-limited-access">Face Verification</a></li>
</ul>
<h3 id="toc-section-4-natural-language-processing-30-35-highest-weight">Section 4: Natural Language Processing (30-35%) ‚ö†Ô∏è HIGHEST WEIGHT</h3>
<ul>
<li><a href="#41-azure-ai-language-text-analytics-comprehensive-coverage">4.1 Azure AI Language (Text Analytics)</a></li>
<li><a href="#core-capabilities-decision-matrix">Core Capabilities Decision Matrix</a></li>
<li><a href="#rest-api-sentiment-analysis">REST API - Sentiment Analysis</a></li>
<li><a href="#python-sdk-complete-text-analytics-examples">Python SDK - Complete Examples</a></li>
<li><a href="#key-phrase-extraction">Key Phrase Extraction</a></li>
<li><a href="#named-entity-recognition-ner">Named Entity Recognition</a></li>
<li><a href="#pii-detection-and-redaction">PII Detection and Redaction</a></li>
<li><a href="#42-speech-service-speech-recognition-and-synthesis">4.2 Speech Service</a></li>
<li><a href="#speech-to-text-recognition-complete-implementation">Speech-to-Text Implementation</a></li>
<li><a href="#text-to-speech-synthesis">Text-to-Speech Implementation</a></li>
            <li><a href="#ssml-for-advanced-speech-control">SSML for Advanced Control</a></li>
<li><a href="#speech-translation">Speech Translation</a></li>
<li><a href="#43-translator-service">4.3 Translator Service</a></li>
            <li><a href="#rest-api-translation">REST API - Translation</a></li>
            <li><a href="#python-sdk-translation">Python SDK - Translation</a></li>
            <li><a href="#python-sdk-translation">Python SDK - Translation</a></li>
<li><a href="#44-conversational-language-understanding-clu-and-luis">4.4 Conversational Language Understanding</a></li>
<li><a href="#understanding-intent-and-entity-extraction">Understanding Intent and Entities</a></li>
<li><a href="#clu-vs-luis-which-to-use">CLU vs LUIS</a></li>
<li><a href="#rest-api-clu-prediction">REST API - CLU Prediction</a></li>
<li><a href="#python-sdk-clu-implementation">Python SDK - CLU Implementation</a></li>
<li><a href="#45-question-answering-service">4.5 Question Answering</a></li>
<li><a href="#how-question-answering-works">How Question Answering Works</a></li>
<li><a href="#rest-api-query-knowledge-base">REST API - Query Knowledge Base</a></li>
<li><a href="#python-sdk-question-answering">Python SDK Implementation</a></li>
</ul>
<h3 id="toc-section-5-knowledge-mining-and-document-intelligence-10-15">Section 5: Knowledge Mining and Document Intelligence (10-15%)</h3>
<ul>
<li><a href="#51-azure-cognitive-search-ai-powered-search">5.1 Azure Cognitive Search</a></li>
<li><a href="#the-ai-enrichment-pipeline">The AI Enrichment Pipeline</a></li>
<li><a href="#creating-a-search-index">Creating a Search Index</a></li>
<li><a href="#searching-the-index">Searching the Index</a></li>
<li><a href="#52-document-intelligence-form-recognizer">5.2 Document Intelligence</a></li>
<li><a href="#prebuilt-models-vs-custom-models">Prebuilt vs Custom Models</a></li>
<li><a href="#python-sdk-invoice-processing">Python SDK - Invoice Processing</a></li>
</ul>
<h3 id="toc-section-6-generative-ai-with-azure-openai-10-15">Section 6: Generative AI with Azure OpenAI (10-15%)</h3>
<ul>
<li><a href="#61-azure-openai-service-chat-completions">6.1 Azure OpenAI Service</a></li>
<li><a href="#understanding-chat-completions">Understanding Chat Completions</a></li>
<li><a href="#temperature-and-parameters">Temperature and Parameters</a></li>
<li><a href="#62-embeddings-for-semantic-search">6.2 Embeddings</a></li>
</ul>
<h3 id="toc-final-exam-preparation">Final Exam Preparation</h3>
<ul>
<li><a href="#key-exam-strategies">Key Exam Strategies</a></li>
<li><a href="#common-exam-question-patterns">Common Question Patterns</a></li>
</ul>
<hr>
<section id="how-to-use-this-guide"><h2>üìñ How to Use This Guide</h2>
<p>This guide is designed to be your complete, standalone resource for passing the AI-102 exam. It combines:</p>
<p><strong>Conceptual Understanding:</strong> Every section explains WHAT each service is, WHY it exists, WHEN to use it, and HOW it compares to alternatives. You'll understand the decision-making process, not just memorize facts.</p>
<p><strong>Practical Implementation:</strong> Every service includes both REST API endpoints with example requests AND Python SDK code with full explanations. You'll see how to actually implement solutions.</p>
<div class="tip-box"><p><strong>Exam Focus:</strong> Decision frameworks, comparison tables, common scenarios, and exam tips throughout. You'll know exactly what the exam tests and how questions are structured.</p></div>
<p><strong>Study Approach for Next Week:</strong></p>
<ul>
<li><strong>Days 1-2:</strong> TL;DR + NLP section (30-35% of exam - highest priority)</li>
<li><strong>Days 3-4:</strong> Vision + Knowledge Mining sections  </li>
<li><strong>Day 5:</strong> Azure OpenAI + Decision Support + Management</li>
<li><strong>Day 6:</strong> Review scenarios, practice questions, weak areas</li>
<li><strong>Day 7:</strong> Light review of key concepts, rest well before exam</li>
</ul>
<hr>
<section id="tldr-quick-reference-guide"><h1>üìã TL;DR - QUICK REFERENCE GUIDE</h1>
<section id="all-services-endpoints-and-key-methods-at-a-glance"><h2>All Services, Endpoints, and Key Methods at a Glance</h2>
<p>This section provides a quick lookup table for all Azure AI services. Use this for final review before the exam or when you need to quickly recall an endpoint or method.</p>
<h3 id="vision-services">Vision Services</h3>
<div class="table-wrapper">
<table>
<thead>
<tr>
<th>Service</th>
<th>Primary Use Case</th>
<th>REST Endpoint Pattern</th>
<th>Key SDK Methods</th>
<th>When to Use</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Computer Vision</strong></td>
<td>General image analysis, OCR, object detection</td>
<td><code>https://{endpoint}/vision/v3.2/analyze</code></td>
<td><code>ImageAnalysisClient.analyze()</code>, <code>analyze_from_url()</code></td>
<td>Analyzing images for objects, text, faces, adult content - general purpose vision needs</td>
</tr>
<tr>
<td><strong>Custom Vision</strong></td>
<td>Custom image classification and object detection</td>
<td><code>https://{endpoint}/customvision/v3.3/Prediction/{projectId}</code></td>
<td><code>CustomVisionPredictionClient.classify_image()</code>, <code>detect_image()</code></td>
<td>Recognizing domain-specific items (logos, products, defects) not in pre-built models</td>
</tr>
<tr>
<td><strong>Face API</strong></td>
<td>Face detection, verification, identification</td>
<td><code>https://{endpoint}/face/v1.0/detect</code></td>
<td><code>FaceClient.face.detect_with_stream()</code>, <code>verify_face_to_face()</code></td>
<td>Face-specific tasks like identity verification, face detection (requires Limited Access approval for identification)</td>
</tr>
<tr>
<td><strong>Document Intelligence</strong></td>
<td>Extract data from forms and documents</td>
<td><code>https://{endpoint}/formrecognizer/documentModels/{modelId}:analyze</code></td>
<td><code>DocumentAnalysisClient.begin_analyze_document()</code></td>
<td>Processing invoices, receipts, forms, or any structured document extraction</td>
</tr>
</tbody>
</table>
</div>
<h3 id="language-services">Language Services</h3>
<div class="table-wrapper">
<table>
<thead>
<tr>
<th>Service</th>
<th>Primary Use Case</th>
<th>REST Endpoint Pattern</th>
<th>Key SDK Methods</th>
<th>When to Use</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Text Analytics</strong></td>
<td>Sentiment, entities, key phrases, language detection</td>
<td><code>https://{endpoint}/text/analytics/v3.1/sentiment</code></td>
<td><code>TextAnalyticsClient.analyze_sentiment()</code>, <code>extract_key_phrases()</code>, <code>recognize_entities()</code></td>
<td>Analyzing existing text for sentiment, extracting information, detecting PII</td>
</tr>
<tr>
<td><strong>Translator</strong></td>
<td>Text translation between 100+ languages</td>
<td><code>https://api.cognitive.microsofttranslator.com/translate</code></td>
<td><code>TextTranslationClient.translate()</code></td>
<td>Translating user interfaces, localizing content, multilingual communication</td>
</tr>
<tr>
<td><strong>Speech Service</strong></td>
<td>Speech-to-text, text-to-speech, translation</td>
<td><code>https://{region}.stt.speech.microsoft.com/speech/recognition/</code></td>
<td><code>SpeechRecognizer.recognize_once_async()</code>, <code>SpeechSynthesizer.speak_text_async()</code></td>
<td>Transcribing audio, generating speech, voice commands, accessibility</td>
</tr>
<tr>
<td><strong>CLU / LUIS</strong></td>
<td>Intent and entity extraction from conversations</td>
<td><code>https://{endpoint}/language/:analyze-conversations</code></td>
<td><code>ConversationAnalysisClient.analyze_conversation()</code></td>
<td>Building chatbots, virtual assistants, understanding user intent from natural language</td>
</tr>
<tr>
<td><strong>Question Answering</strong></td>
<td>FAQ bots, knowledge base Q&A</td>
<td><code>https://{endpoint}/language/:query-knowledgebases</code></td>
<td><code>QuestionAnsweringClient.get_answers()</code></td>
<td>Creating FAQ bots, answering questions from documentation or knowledge bases</td>
</tr>
</tbody>
</table>
</div>
<h3 id="search-and-document-services">Search and Document Services</h3>
<div class="table-wrapper">
<table>
<thead>
<tr>
<th>Service</th>
<th>Primary Use Case</th>
<th>REST Endpoint Pattern</th>
<th>Key SDK Methods</th>
<th>When to Use</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Cognitive Search</strong></td>
<td>AI-powered search with enrichment</td>
<td><code>https://{service}.search.windows.net/indexes/</code></td>
<td><code>SearchClient.search()</code>, <code>SearchIndexClient.create_index()</code></td>
<td>Building search over documents with AI enrichment (OCR, entity extraction, key phrases)</td>
</tr>
</tbody>
</table>
</div>
<h3 id="decision-and-content-services">Decision and Content Services</h3>
<div class="table-wrapper">
<table>
<thead>
<tr>
<th>Service</th>
<th>Primary Use Case</th>
<th>REST Endpoint Pattern</th>
<th>Key SDK Methods</th>
<th>When to Use</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Content Safety</strong></td>
<td>Detect harmful content (hate, violence, sexual, self-harm)</td>
<td><code>https://{endpoint}/contentsafety/text:analyze</code></td>
<td><code>ContentSafetyClient.analyze_text()</code>, <code>analyze_image()</code></td>
<td>Moderating user-generated content, ensuring community safety</td>
</tr>
<tr>
<td><strong>Anomaly Detector</strong></td>
<td>Time-series anomaly detection</td>
<td><code>https://{endpoint}/anomalydetector/v1.1/timeseries/entire/detect</code></td>
<td><code>AnomalyDetectorClient.detect_entire_series()</code></td>
<td>Detecting unusual patterns in metrics, fraud detection, equipment monitoring</td>
</tr>
</tbody>
</table>
</div>
<h3 id="generative-ai">Generative AI</h3>
<div class="table-wrapper">
<table>
<thead>
<tr>
<th>Service</th>
<th>Primary Use Case</th>
<th>REST Endpoint Pattern</th>
<th>Key SDK Methods</th>
<th>When to Use</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Azure OpenAI</strong></td>
<td>Chat, completions, embeddings, DALL-E</td>
<td><code>https://{endpoint}/openai/deployments/{deployment}/chat/completions</code></td>
<td><code>AzureOpenAI.chat.completions.create()</code>, <code>embeddings.create()</code></td>
<td>Generative AI, chatbots, content creation, semantic search, code generation</td>
</tr>
</tbody>
</table>
</div>
<hr>
<section id="section-1-plan-and-manage-an-azure-ai-solution-15-20"><h1>SECTION 1: PLAN AND MANAGE AN AZURE AI SOLUTION (15-20%)</h1>
<p>This section tests your ability to make architectural decisions, select appropriate services, configure security, implement monitoring, and manage costs. These skills apply across ALL Azure AI services, so understanding this section is fundamental to passing the exam.</p>
<section id="11-understanding-azure-ai-services"><h2>1.1 Understanding Azure AI Services</h2>
<h3 id="what-are-azure-ai-services-and-why-do-they-exist">What Are Azure AI Services and Why Do They Exist?</h3>
<p>Azure AI Services are Microsoft's collection of pre-built artificial intelligence capabilities delivered as cloud APIs. The fundamental concept is simple: instead of requiring teams to hire data scientists, collect massive training datasets, build machine learning infrastructure, and train models from scratch, Azure AI Services provide production-ready AI capabilities that any developer can integrate with simple API calls.</p>
<p><strong>The Problem They Solve:</strong> Traditional AI development requires specialized expertise. Building a computer vision system that can identify objects in images traditionally requires understanding convolutional neural networks, gathering and labeling thousands of training images, managing GPU infrastructure for training, and optimizing model performance. This is expensive, time-consuming, and requires rare expertise.</p>
<p><strong>The Solution:</strong> Azure AI Services eliminate these barriers. When you call the Computer Vision API to analyze an image, you leverage models trained on millions of images by Microsoft's research teams. You don't need to understand the underlying neural network architectures - you just send an image and receive back structured data describing what's in it. Microsoft handles model training, updates, scaling, and infrastructure management.</p>
<p><strong>Real-World Analogy:</strong> Think of Azure AI Services like using a cloud database service instead of installing and managing database software yourself. Just as Azure SQL Database handles infrastructure, scaling, and maintenance while you focus on your queries and data, Azure AI Services handle AI infrastructure while you focus on using AI capabilities in your applications.</p>
<h3 id="multi-service-vs-single-service-resources-deep-dive">Multi-Service vs Single-Service Resources - Deep Dive</h3>
<p>When deploying Azure AI Services, your first decision is whether to create a <strong>multi-service resource</strong> (one endpoint for multiple services) or <strong>single-service resources</strong> (dedicated endpoint per service). This decision impacts authentication, security, billing, and operations.</p>
<p><strong>Multi-Service Resource (Azure AI Services)</strong></p>
<p>A multi-service resource provides one endpoint and one set of API keys that grant access to multiple AI services. When you create an "Azure AI Services" resource in the portal, that single resource allows calling Computer Vision, Text Analytics, Translator, Speech, and other services using the same authentication.</p>
<p><strong>How It Works Technically:</strong> Azure routes your API requests to the appropriate service based on the endpoint path. If you call <code>https://myresource.cognitiveservices.azure.com/vision/v3.2/analyze</code>, Azure recognizes the <code>/vision/</code> path and routes to Computer Vision. If you call <code>/text/analytics/v3.1/sentiment</code>, it routes to Text Analytics. The routing is transparent - from your perspective, it's all one service.</p>
<p><strong>Key Advantages:</strong></p>
<ul>
<li><strong>Simplified Authentication:</strong> One key for all services means less key management, simpler application configuration, and easier key rotation</li>
<li><strong>Consolidated Billing:</strong> Single line item on your Azure bill aggregating all service usage</li>
<li><strong>Easier Development:</strong> Developers don't need separate credentials for each service during prototyping</li>
<li><strong>Faster Deployment:</strong> Create one resource instead of multiple when building integrated solutions</li>
</ul>
<div class="tip-box"><p><strong>Critical Limitations:</strong></p></div>
<ul>
<li><strong>Security:</strong> The API key grants access to ALL services the resource supports. If your application only needs Text Analytics but uses a multi-service key, that key could also access Computer Vision, Speech, etc. This violates least-privilege security principles.</li>
<li><strong>No Granular RBAC:</strong> You cannot assign different permissions for different services - it's all-or-nothing access</li>
<li><strong>No Per-Service Networking:</strong> Cannot apply different virtual network or firewall rules to individual services</li>
<li><strong>Limited Service Support:</strong> Some services like Custom Vision and Immersive Reader don't support multi-service resources and require dedicated resources</li>
<li><strong>Regional Constraint:</strong> All services route through one region - you cannot deploy different services in different regions</li>
</ul>
<p><strong>When to Use Multi-Service Resources</strong> (Exam Decision Framework):</p>
<ul>
<li>Application uses 3+ different AI services together</li>
<li>Development/testing environments where simplicity outweighs security granularity</li>
<li>All services can operate in the same region</li>
<li>No requirement for service-specific access control or networking</li>
<li>Prototyping where final service selection is uncertain</li>
</ul>
<p><strong>Single-Service Resources</strong></p>
<p>Single-service resources create dedicated endpoints for individual services. A Computer Vision resource provides only Computer Vision capabilities with its own keys and endpoint like <code>https://mycomputervision.cognitiveservices.azure.com/</code>.</p>
<p><strong>Key Advantages:</strong></p>
<ul>
<li><strong>Security Isolation:</strong> Each service has separate credentials - developers/applications access only what they need</li>
<li><strong>Granular RBAC:</strong> Assign different Azure roles per service (Alice gets Contributor on Computer Vision, only User rights on Text Analytics)</li>
<li><strong>Per-Service Networking:</strong> Apply different firewall rules, private endpoints, virtual network configurations to each service</li>
<li><strong>Regional Flexibility:</strong> Deploy Computer Vision in West Europe for EU data residency while Text Analytics runs in East US near your data</li>
<li><strong>Cost Visibility:</strong> Azure bill shows exactly what each service costs, enabling chargeback and optimization</li>
<li><strong>Service Support:</strong> All services including Custom Vision work with dedicated resources</li>
</ul>
<p><strong>Limitations:</strong></p>
<ul>
<li><strong>Management Overhead:</strong> Must create, configure, monitor, and maintain each resource separately</li>
<li><strong>Multiple Credentials:</strong> Application configuration becomes more complex with multiple endpoints and keys</li>
<li><strong>More Complex Deployment:</strong> Infrastructure-as-code templates need to provision multiple resources</li>
</ul>
<p><strong>When to Use Single-Service Resources</strong> (Exam Decision Framework):</p>
<ul>
<li>Regulatory/compliance requires service isolation (e.g., EU data must use EU services only)</li>
<li>Different teams manage different AI capabilities</li>
<li>Need service-specific access control (data scientists get Custom Vision access, app developers get Text Analytics)</li>
<li>Require different networking configurations (Computer Vision in private VNet, Speech on public internet)</li>
<li>Need detailed cost allocation (chargeback to business units)</li>
<li>Using services not supported by multi-service resources (Custom Vision, Immersive Reader)</li>
</ul>
<div class="tip-box">
<p><strong>Exam Scenario Example:</strong></p>
<p><em>"Your company processes customer photos in West Europe to comply with GDPR, but performs sentiment analysis on support tickets from a database in East US. Different teams manage computer vision and natural language processing with separate budgets. Which resource type should you use?"</em></p>
<p><strong>Answer:</strong> Single-service resources. The scenario requires regional distribution (West Europe + East US), separate team management, and cost isolation - all impossible with multi-service resources.</p>
</div>
<h3 id="rest-api-architecture-pattern">REST API Architecture Pattern</h3>
<p>All Azure AI Services follow a consistent REST API pattern that you must understand for the exam. Whether you're calling Computer Vision, Text Analytics, or any other service, the pattern is similar.</p>
<p><strong>Standard Request Structure:</strong></p>
<pre><code class="language-">POST https://{your-endpoint}/{service-path}/{api-version}
Headers:
  Ocp-Apim-Subscription-Key: {your-key}
  Content-Type: application/json
Body:
  {service-specific JSON}</code></pre>
<p><strong>Components Explained:</strong></p>
<p><strong>Endpoint:</strong> Your resource's unique URL like <code>https://myresource.cognitiveservices.azure.com</code>. This endpoint is tied to a specific Azure region where your resource was created.</p>
<p><strong>Service Path:</strong> Identifies which service and operation, like <code>/vision/v3.2/analyze</code> or <code>/text/analytics/v3.1/sentiment</code>. Each service has its own path structure.</p>
<p><strong>API Version:</strong> Specifies which version of the API to use, like <code>?api-version=2023-10-01</code>. This allows Microsoft to update APIs without breaking existing applications.</p>
<p><strong>Authentication Header:</strong> <code>Ocp-Apim-Subscription-Key</code> contains your API key. Alternative is using Azure AD tokens in the <code>Authorization</code> header with managed identities (more on this later).</p>
<p><strong>Request Body:</strong> JSON containing the data to analyze and any parameters. Structure varies by service but follows predictable patterns.</p>
<p><strong>Standard Response Structure:</strong></p>
<pre><code class="language-json">{
  &quot;analysisResults&quot;: { /* service-specific data */ },
  &quot;metadata&quot;: {
    &quot;requestId&quot;: &quot;unique-id&quot;,
    &quot;processingTime&quot;: &quot;124ms&quot;
  }
}</code></pre>
<p>Understanding this pattern means you can work with any Azure AI service even if you've never used it before - the structure is consistent.</p>
<h3 id="python-sdk-architecture-pattern">Python SDK Architecture Pattern</h3>
<p>The Python SDKs for Azure AI Services follow a consistent pattern using the <code>azure-ai-{service}</code> package naming convention.</p>
<p><strong>Standard SDK Pattern:</strong></p>
<pre><code class="language-python">from azure.ai.{service} import {Service}Client
from azure.core.credentials import AzureKeyCredential

# Initialize client
endpoint = &quot;https://your-resource.cognitiveservices.azure.com/&quot;
key = &quot;your-api-key&quot;

client = {Service}Client(endpoint, AzureKeyCredential(key))

# Call service
result = client.{operation}(input_data)</code></pre>
<p><strong>Components Explained:</strong></p>
<p><strong>Import Pattern:</strong> Packages follow <code>azure.ai.textanalytics</code>, <code>azure.ai.vision.imageanalysis</code> naming. This consistency helps you find packages even for services you haven't used.</p>
<p><strong>Client Initialization:</strong> Every service has a <code>{Service}Client</code> class that takes an endpoint and credential. The credential can be an <code>AzureKeyCredential</code> (for API keys) or Azure AD credentials (for managed identities).</p>
<p><strong>Method Pattern:</strong> Client methods follow predictable naming - <code>analyze_<em></code> for analysis operations, <code>recognize_</em></code> for recognition tasks, <code>detect_*</code> for detection operations.</p>
<p><strong>Result Objects:</strong> Methods return strongly-typed result objects with properties for different aspects of the analysis. This is better than working with raw JSON dictionaries.</p>
<h3 id="pricing-tiers-comprehensive-understanding">Pricing Tiers - Comprehensive Understanding</h3>
<p>Azure AI Services offer different pricing tiers optimized for different usage patterns. Selecting the right tier is critical for both cost and capability, and the exam frequently tests this knowledge.</p>
<p><strong>Free Tier (F0) - Detailed Analysis</strong></p>
<p>The free tier provides limited quota at no cost, designed exclusively for learning, development, and proof-of-concept work.</p>
<p><strong>Technical Characteristics:</strong></p>
<ul>
<li><strong>Quota:</strong> Service-specific monthly limits (e.g., 5,000 Computer Vision calls, 5,000 Text Analytics records, 2M Translator characters)</li>
<li><strong>Rate Limits:</strong> Typically 10-20 calls per minute regardless of remaining quota</li>
<li><strong>Infrastructure:</strong> Shared, best-effort resources with no performance guarantees</li>
<li><strong>SLA:</strong> No service level agreement - no uptime guarantees</li>
<li><strong>Features:</strong> Limited to basic features - no advanced options, custom models, or enterprise features</li>
<li><strong>Availability:</strong> Cannot be used if quota exhausted until next month</li>
</ul>
<p><strong>When Free Tier Is Appropriate:</strong></p>
<ul>
<li>Learning how a service works</li>
<li>Building proof-of-concept demonstrations</li>
<li>Developing and testing applications before deployment</li>
<li>Educational projects or tutorials</li>
</ul>
<p><strong>When Free Tier Is NOT Appropriate</strong> (Critical for Exam):</p>
<ul>
<li>Any production use case</li>
<li>Applications serving customers</li>
<li>Scenarios requiring guaranteed availability</li>
<li>Batch processing (rate limits prevent bulk operations)</li>
<li>Testing at scale (quota too small for realistic load testing)</li>
</ul>
<div class="tip-box"><p><strong>Exam Red Flags:</strong> If question mentions "production," "customers," "SLA," "availability requirements," or "processing thousands of items," free tier is never the answer.</p></div>
<p><strong>Standard Tiers (S0, S1, S2, S3) - Comprehensive Analysis</strong></p>
<p>Standard tiers provide pay-as-you-go pricing where you pay for actual usage. Different services structure tiers differently, but all share common characteristics.</p>
<p><strong>Technical Characteristics:</strong></p>
<ul>
<li><strong>Pricing Model:</strong> Pay per transaction/operation with tiered pricing (higher volumes have lower per-unit costs)</li>
<li><strong>Rate Limits:</strong> Much higher TPS (transactions per second) - often 10-100x higher than free tier</li>
<li><strong>Infrastructure:</strong> Dedicated compute with guaranteed performance</li>
<li><strong>SLA:</strong> 99.9% availability guarantee with financial backing</li>
<li><strong>Scaling:</strong> Auto-scales to handle traffic spikes</li>
<li><strong>Features:</strong> Access to all production features including VNet integration, private endpoints, CMK encryption</li>
</ul>
<p><strong>Pricing Examples</strong> (Understand the Pattern):</p>
<ul>
<li>Computer Vision S1: $1.50 per 1,000 transactions for standard analyze, $2.50 per 1,000 for Read API</li>
<li>Text Analytics S: $2.00 per 1,000 text records with volume discounts</li>
<li>Translator S1: $10.00 per 1M characters translated</li>
</ul>
<p><strong>When Standard Tiers Are Appropriate:</strong></p>
<ul>
<li>Production workloads serving customers</li>
<li>Variable traffic patterns (pay for what you use)</li>
<li>New applications without usage history</li>
<li>Applications where load fluctuates (daily/weekly patterns)</li>
<li>When you need flexibility to scale up or down</li>
</ul>
<p><strong>Commitment Tiers (Provisioned Throughput) - Strategic Understanding</strong></p>
<p>Commitment tiers involve pre-purchasing capacity at discounted rates. Instead of paying per transaction, you commit to minimum usage and receive lower per-unit pricing.</p>
<p><strong>Technical Characteristics:</strong></p>
<ul>
<li><strong>Pricing Model:</strong> Pre-purchase specific capacity (e.g., 1M operations/month) at discounted rate</li>
<li><strong>Commitment Period:</strong> Monthly or annual commits</li>
<li><strong>Overage:</strong> Usage beyond commitment billed at standard rates</li>
<li><strong>Savings:</strong> Typically 20-40% discount vs pay-as-you-go</li>
<li><strong>Capacity Guarantee:</strong> Reserved throughput - your requests never throttle</li>
</ul>
<p><strong>Financial Model:</strong></p>
<ul>
<li>Commit to 1M Text Analytics operations at $1.20/1K (vs $2.00/1K standard)</li>
<li>If you use 1.2M operations: pay committed rate for 1M, standard rate for 200K</li>
<li>If you use 800K operations: still pay for full 1M commitment (wasted 200K)</li>
<li>Break-even at ~70-80% utilization due to discount</li>
</ul>
<p><strong>When Commitment Tiers Are Appropriate:</strong></p>
<ul>
<li>Stable, predictable usage patterns over months</li>
<li>High-volume workloads where discount offsets risk</li>
<li>Applications with consistent baseline load</li>
<li>When budget is fixed and optimization is priority</li>
</ul>
<p><strong>When Commitment Tiers Are NOT Appropriate:</strong></p>
<ul>
<li>New applications without usage history</li>
<li>Variable or seasonal workloads</li>
<li>Rapidly growing applications (hard to predict capacity needs)</li>
<li>Development environments (usage varies widely)</li>
</ul>
<div class="tip-box"><p><strong>Exam Decision Framework for Pricing Tiers:</strong></p></div>
<ol>
<li><strong>Is it production?</strong> If no ‚Üí Free tier possible. If yes ‚Üí Standard or Commitment.</li>
<li><strong>Is usage predictable?</strong> If no ‚Üí Standard pay-as-you-go. If yes ‚Üí Consider commitment.</li>
<li><strong>Is cost optimization critical with stable load?</strong> If yes ‚Üí Commitment tier.</li>
<li><strong>Does scenario mention "variable traffic," "growing business," or "new application"?</strong> ‚Üí Standard tier.</li>
<li><strong>Are SLA or performance guarantees mentioned?</strong> ‚Üí Not free tier (must be Standard or Commitment).</li>
</ol>
<div class="tip-box">
<p><strong>Exam Scenario Example:</strong></p>
<p><em>"Your company processes 5 million customer support tickets monthly with minimal variation. The current cost using pay-as-you-go Text Analytics is $10,000/month. The finance team wants to reduce costs. What should you recommend?"</em></p>
<p><strong>Answer:</strong> Commitment tier. Stable usage pattern (5M tickets consistently) and cost reduction goal indicate commitment pricing. Calculate: 5M operations at commitment pricing saves ~30% = $3,000/month savings with minimal risk given stable usage.</p>
</div>
<section id="12-authentication-and-authorization"><h2>1.2 Authentication and Authorization</h2>
<p>Authentication proves who/what is calling the API, while authorization determines what they can do. Understanding both is critical across all AI services.</p>
<h3 id="api-key-authentication-how-it-works">API Key Authentication - How It Works</h3>
<p>API keys are the simplest authentication method. When you create an AI service, Azure generates two keys (Key1 and Key2). Your application includes one key in the HTTP header of every request.</p>
<p><strong>How Key Authentication Works Technically:</strong></p>
<ol>
<li>Your application makes HTTP request with header: <code>Ocp-Apim-Subscription-Key: abc123...</code></li>
<li>Azure validates the key against stored keys for your resource</li>
<li>If key matches ‚Üí request proceeds to service</li>
<li>If key invalid ‚Üí HTTP 401 Unauthorized response</li>
</ol>
<p><strong>Why Two Keys?:</strong> To enable zero-downtime key rotation. You can regenerate Key1 while applications still use Key2, update applications to use new Key1, then regenerate Key2. Without two keys, regenerating the only key would immediately break all applications.</p>
<p><strong>Key Advantages:</strong></p>
<ul>
<li>Simple to implement - just add header to requests</li>
<li>No dependency on Azure AD</li>
<li>Works from any environment (on-premises, other clouds, local machines)</li>
<li>Quick to get started during development</li>
</ul>
<div class="tip-box"><p><strong>Critical Security Limitations:</strong></p></div>
<ul>
<li><strong>Full Access:</strong> A key grants complete access to the service - no granular permissions</li>
<li><strong>No Expiration:</strong> Keys don't expire automatically - a compromised key remains valid until manually regenerated</li>
<li><strong>No Audit Trail:</strong> Cannot determine who used a key (multiple apps might share same key)</li>
<li><strong>Shared Secret Risk:</strong> Keys stored in application code/config can be leaked via source control, logs, etc.</li>
</ul>
<p><strong>When API Keys Are Appropriate</strong> (Exam Perspective):</p>
<ul>
<li>Development and testing environments</li>
<li>Simple applications where security overhead isn't justified</li>
<li>Applications running outside Azure where managed identities aren't available</li>
<li>Quick prototyping and demonstrations</li>
</ul>
<p><strong>When API Keys Are NOT Appropriate</strong> (Exam Red Flags):</p>
<ul>
<li>Production enterprise applications (use managed identities instead)</li>
<li>Scenarios mentioning "principle of least privilege"</li>
<li>Requirements for audit logging of access</li>
<li>Compliance/security-focused questions</li>
</ul>
<p><strong>REST API Example with Key Authentication:</strong></p>
<pre><code class="language-bash"># Analyze image using Computer Vision
curl -X POST &quot;https://myresource.cognitiveservices.azure.com/vision/v3.2/analyze?visualFeatures=Objects,Tags&quot; \
  -H &quot;Ocp-Apim-Subscription-Key: your-api-key-here&quot; \
  -H &quot;Content-Type: application/json&quot; \
  -d &#x27;{
    &quot;url&quot;: &quot;https://example.com/image.jpg&quot;
  }&#x27;</code></pre>
<p><strong>Python SDK Example with Key Authentication:</strong></p>
<pre><code class="language-python">from azure.ai.textanalytics import TextAnalyticsClient
from azure.core.credentials import AzureKeyCredential

# Initialize with API key
endpoint = &quot;https://myresource.cognitiveservices.azure.com/&quot;
key = &quot;your-api-key-here&quot;  # ‚ùå Hardcoding keys is bad practice!

# Better: Read from environment variable
import os
key = os.environ.get(&quot;COGNITIVE_SERVICE_KEY&quot;)

credential = AzureKeyCredential(key)
client = TextAnalyticsClient(endpoint, credential)

# Use the client
documents = [&quot;I love this product!&quot;]
result = client.analyze_sentiment(documents)</code></pre>
<h3 id="azure-ad-managed-identity-authentication-comprehensive-understanding">Azure AD / Managed Identity Authentication - Comprehensive Understanding</h3>
<p>Azure AD (Microsoft Entra ID) authentication uses tokens instead of static keys, providing significantly better security through managed identities, role-based access control, and comprehensive auditing.</p>
<p><strong>What Is a Managed Identity?</strong></p>
<p>A managed identity is an Azure AD identity automatically managed by Azure and tied to a specific resource like a Virtual Machine, App Service, or Azure Function. The key innovation: NO credentials in your code or configuration files.</p>
<p><strong>How Managed Identities Work (Technical Flow):</strong></p>
<ol>
<li>You enable managed identity on your Azure resource (e.g., App Service)</li>
<li>Azure automatically creates an Azure AD service principal for that resource</li>
<li>When your code requests a token, Azure SDK contacts Azure Instance Metadata Service (IMDS)</li>
<li>IMDS validates the request comes from the managed identity-enabled resource</li>
<li>IMDS returns an Azure AD token valid for your target resource (the AI service)</li>
<li>Your code includes this token in the <code>Authorization: Bearer {token}</code> header</li>
<li>AI service validates the token with Azure AD and processes the request</li>
</ol>
<p><strong>The key insight:</strong> Your application code NEVER sees or stores credentials. Azure handles the entire authentication flow automatically.</p>
<p><strong>Types of Managed Identities:</strong></p>
<p><strong>System-Assigned Managed Identity:</strong></p>
<ul>
<li>Created when you enable managed identity on a resource</li>
<li>Lifecycle tied to the resource - deleted when resource is deleted</li>
<li>Unique to that one resource - cannot be shared</li>
<li>Best for applications where each deployment instance needs separate identity</li>
</ul>
<p><strong>User-Assigned Managed Identity:</strong></p>
<ul>
<li>Created as standalone Azure resource</li>
<li>Can be assigned to multiple resources (multiple VMs can share same identity)</li>
<li>Independent lifecycle - persists even if resources using it are deleted</li>
<li>Best when multiple resources need same permissions</li>
</ul>
<p id="configuring-role-based-access-control-rbac"><strong>Configuring Role-Based Access Control (RBAC):</strong></p>
<p>After creating a managed identity, you grant it permissions on the AI service using Azure RBAC roles.</p>
<p><strong>Common Built-In Roles for AI Services:</strong></p>
<p><strong>Cognitive Services User:</strong></p>
<ul>
<li>Can call API operations (read, write, delete, list)</li>
<li>Cannot manage the resource itself (change pricing tier, delete resource, etc.)</li>
<li>Appropriate for application code that uses the service</li>
</ul>
<p><strong>Cognitive Services Contributor:</strong></p>
<ul>
<li>All User permissions PLUS can manage the service configuration</li>
<li>Can change pricing tiers, regenerate keys, configure networking</li>
<li>Appropriate for DevOps automation that deploys/configures services</li>
</ul>
<p><strong>Cognitive Services Data Reader (Speech):</strong></p>
<ul>
<li>Read-only access to service data</li>
<li>Cannot call APIs that modify data</li>
<li>Appropriate for monitoring/auditing applications</li>
</ul>
<p><strong>How to Assign Roles (Exam Knowledge):</strong></p>
<p>Via Azure Portal:</p>
<ol>
<li>Navigate to AI service resource</li>
<li>Select "Access control (IAM)"</li>
<li>Click "Add role assignment"</li>
<li>Choose role (e.g., Cognitive Services User)</li>
<li>Select managed identity (e.g., your App Service)</li>
</ol>
<p>Via Azure CLI:</p>
<pre><code class="language-bash"># Get the managed identity&#x27;s principal ID
PRINCIPAL_ID=$(az webapp identity show \
  --resource-group myRG \
  --name myApp \
  --query principalId -o tsv)

# Get the AI service resource ID
RESOURCE_ID=$(az cognitiveservices account show \
  --resource-group myRG \
  --name myTextAnalytics \
  --query id -o tsv)

# Assign Cognitive Services User role
az role assignment create \
  --assignee $PRINCIPAL_ID \
  --role &quot;Cognitive Services User&quot; \
  --scope $RESOURCE_ID</code></pre>
<p><strong>Python SDK with Managed Identity:</strong></p>
<pre><code class="language-python">from azure.identity import DefaultAzureCredential
from azure.ai.textanalytics import TextAnalyticsClient

# DefaultAzureCredential automatically uses managed identity when running in Azure
# Falls back to other methods (CLI, VS Code) during local development
credential = DefaultAzureCredential()

endpoint = &quot;https://myresource.cognitiveservices.azure.com/&quot;
client = TextAnalyticsClient(endpoint, credential)

# Use normally - authentication happens automatically
documents = [&quot;Great product!&quot;, &quot;Terrible experience&quot;]
results = client.analyze_sentiment(documents)

for idx, doc in enumerate(results):
    print(f&quot;Document {idx}: {doc.sentiment}&quot;)</code></pre>
<p><strong>When Managed Identities Are Appropriate</strong> (Exam Decision Framework):</p>
<ul>
<li>‚úÖ Any production application running in Azure</li>
<li>‚úÖ Scenarios requiring audit logging (Azure AD logs all token requests)</li>
<li>‚úÖ Questions mentioning "principle of least privilege"</li>
<li>‚úÖ Multi-tenant applications where different instances need different permissions</li>
<li>‚úÖ Compliance requirements for credential-less authentication</li>
<li>‚úÖ Microservices where each service should have minimal permissions</li>
</ul>
<p><strong>When Managed Identities Are NOT Required:</strong></p>
<ul>
<li>Applications running outside Azure (on-premises, other clouds) - use API keys or service principals</li>
<li>Simple development/test scenarios where security overhead isn't justified</li>
<li>Extremely simple applications (though managed identities should still be preferred)</li>
</ul>
<div class="tip-box">
<p><strong>Exam Scenario Example:</strong></p>
<p><em>"Your company's security policy prohibits storing credentials in application code or configuration files. Your App Service needs to call Text Analytics to analyze customer feedback. How should you implement authentication?"</em></p>
<p><strong>Answer:</strong> Enable system-assigned managed identity on the App Service, grant it the Cognitive Services User role on the Text Analytics resource, and use DefaultAzureCredential in code. This eliminates credentials entirely while providing granular access control and full audit logging.</p>
</div>
<section id="13-network-security-for-ai-services"><h2>1.3 Network Security for AI Services</h2>
<p>Securing network access to AI services prevents unauthorized access and data exfiltration. The exam tests understanding of public endpoints with firewalls, service endpoints, and private endpoints.</p>
<h3 id="public-endpoint-with-ip-firewall">Public Endpoint with IP Firewall</h3>
<p>By default, AI service resources are accessible from any internet-connected client. You can restrict access by configuring IP-based firewall rules.</p>
<p><strong>How IP Firewalls Work:</strong></p>
<ul>
<li>You specify allowed IP addresses or CIDR ranges</li>
<li>Azure firewall checks the source IP of each request</li>
<li>If source IP matches an allowed rule ‚Üí request proceeds</li>
<li>If source IP not allowed ‚Üí HTTP 403 Forbidden</li>
</ul>
<p><strong>Configuration via Azure Portal:</strong></p>
<ol>
<li>Navigate to AI service resource</li>
<li>Select "Networking" under Settings</li>
<li>Choose "Selected Networks and Private Endpoints"</li>
<li>Add IP ranges under "Firewall"</li>
<li>Save configuration</li>
</ol>
<p><strong>When to Use IP Firewall:</strong></p>
<ul>
<li>You know the public IP addresses of legitimate clients</li>
<li>Simple protection for resources accessed from fixed locations (office, data center)</li>
<li>First layer of defense before implementing more sophisticated controls</li>
</ul>
<p><strong>Limitations:</strong></p>
<ul>
<li>Public endpoint still exists (just firewalled)</li>
<li>Source IPs can change (especially for cloud services)</li>
<li>Doesn't protect against threats from allowed IPs</li>
<li>Management overhead if many IPs/ranges need to be allowed</li>
</ul>
<div class="tip-box"><p><strong>Exam Tip:</strong> IP firewalls are a basic control. For scenarios emphasizing strong security or compliance, private endpoints are the answer.</p></div>
<h3 id="service-endpoints">Service Endpoints</h3>
<p>Service endpoints provide optimized routing from Azure Virtual Networks to Azure services, with traffic staying on the Azure backbone network rather than traversing the internet.</p>
<p><strong>How Service Endpoints Work:</strong></p>
<ol>
<li>You enable service endpoint for Microsoft.CognitiveServices on a VNet subnet</li>
<li>Azure updates routing so traffic to Cognitive Services from that subnet uses Azure backbone</li>
<li>You configure the AI service firewall to allow that specific subnet</li>
<li>Traffic from the subnet reaches the service through Azure's internal network</li>
</ol>
<p><strong>Key Characteristics:</strong></p>
<ul>
<li>Service still has public IP and public endpoint</li>
<li>Traffic doesn't go over public internet (uses Azure backbone)</li>
<li>Simpler than private endpoints (no DNS changes required)</li>
<li>Free - no additional cost for service endpoints</li>
</ul>
<p><strong>When to Use Service Endpoints:</strong></p>
<ul>
<li>VMs or other resources in Azure need to access AI services</li>
<li>Want to avoid public internet for security/performance</li>
<li>Don't need complete isolation (private endpoints)</li>
<li>Cost-sensitive scenarios</li>
</ul>
<p><strong>Limitations vs Private Endpoints:</strong></p>
<ul>
<li>Service still has public endpoint accessible from internet (though firewalled)</li>
<li>Don't provide a private IP address</li>
<li>Less isolation than private endpoints</li>
</ul>
<h3 id="private-endpoints-comprehensive-deep-dive">Private Endpoints - Comprehensive Deep Dive</h3>
<p>Private endpoints bring Azure AI services into your virtual network with a private IP address, eliminating public internet exposure entirely.</p>
<p><strong>How Private Endpoints Work (Detailed Technical Flow):</strong></p>
<ol>
<li><strong>Create Private Endpoint Resource:</strong> </li>
</ol>
<ul>
<li>Links your AI service to a subnet in your VNet</li>
<li>Azure allocates a private IP from the subnet (e.g., 10.0.1.4)</li>
<li>Creates a network interface in your subnet</li>
</ul>
<ol>
<li><strong>Private Link Connection:</strong></li>
</ol>
<ul>
<li>Azure establishes a Private Link connection from your VNet to the AI service</li>
<li>All traffic flows through Azure's private backbone network</li>
<li>Service becomes accessible via the private IP</li>
</ul>
<ol>
<li><strong>DNS Resolution:</strong></li>
</ol>
<ul>
<li>Public DNS for <code>myresource.cognitiveservices.azure.com</code> still resolves to public IP</li>
<li>You configure Private DNS Zone to override this within your VNet</li>
<li>Private DNS resolves <code>myresource.cognitiveservices.azure.com</code> to private IP (10.0.1.4)</li>
<li>Clients in VNet automatically use private IP</li>
<li>Clients outside VNet (without access to Private DNS) use public IP</li>
</ul>
<ol>
<li><strong>Network Security:</strong></li>
</ol>
<ul>
<li>Optionally disable public network access entirely</li>
<li>Service only accessible via private endpoint</li>
<li>Network Security Groups can further restrict access</li>
</ul>
<p><strong>Complete Private Endpoint Setup:</strong></p>
<pre><code class="language-python"># Step 1: Create Private DNS Zone
az network private-dns zone create \
  --resource-group myRG \
  --name &quot;privatelink.cognitiveservices.azure.com&quot;

# Step 2: Link DNS zone to VNet
az network private-dns link vnet create \
  --resource-group myRG \
  --zone-name &quot;privatelink.cognitiveservices.azure.com&quot; \
  --name MyDNSLink \
  --virtual-network myVNet \
  --registration-enabled false

# Step 3: Create Private Endpoint
az network private-endpoint create \
  --resource-group myRG \
  --name myPrivateEndpoint \
  --vnet-name myVNet \
  --subnet mySubnet \
  --private-connection-resource-id $(az cognitiveservices account show \
    --name myTextAnalytics \
    --resource-group myRG \
    --query id -o tsv) \
  --connection-name myConnection \
  --group-id account

# Step 4: Create DNS Zone Group (auto-creates DNS records)
az network private-endpoint dns-zone-group create \
  --resource-group myRG \
  --endpoint-name myPrivateEndpoint \
  --name MyZoneGroup \
  --private-dns-zone &quot;privatelink.cognitiveservices.azure.com&quot; \
  --zone-name cognitiveservices</code></pre>
<p><strong>When Private Endpoints Are Required</strong> (Exam Decision Criteria):</p>
<ul>
<li>‚úÖ Regulatory compliance requiring no public internet exposure</li>
<li>‚úÖ Processing highly sensitive data (healthcare, financial)</li>
<li>‚úÖ Questions mentioning "data exfiltration prevention"</li>
<li>‚úÖ Hybrid scenarios connecting on-premises to Azure via VPN/ExpressRoute</li>
<li>‚úÖ Zero-trust network architecture</li>
<li>‚úÖ Scenarios stating "service must only be accessible from corporate network"</li>
</ul>
<p><strong>Architecture Patterns with Private Endpoints:</strong></p>
<p><strong>Hub-and-Spoke Topology:</strong></p>
<ul>
<li>Hub VNet contains shared services including private endpoints</li>
<li>Spoke VNets peer to hub</li>
<li>All spokes can access AI services through hub's private endpoints</li>
<li>Centralizes private endpoint management</li>
</ul>
<p><strong>Multiple Region Deployment:</strong></p>
<ul>
<li>Create private endpoints in each region's VNet</li>
<li>Configure private DNS zones in each region</li>
<li>Ensures local access path (no cross-region traffic)</li>
</ul>
<p><strong>Hybrid Cloud with On-Premises:</strong></p>
<ul>
<li>Private endpoint in Azure VNet</li>
<li>VPN or ExpressRoute connects on-premises to Azure</li>
<li>On-premises DNS forwards queries to Azure Private DNS</li>
<li>On-premises systems access AI services via private endpoint</li>
</ul>
<p><strong>Common Exam Scenario:</strong></p>
<p><em>"Your company has a corporate policy that all Azure services must be inaccessible from the public internet. Applications running in your Azure VNet need to call Text Analytics. On-premises systems connected via ExpressRoute also need access. How should you configure networking?"</em></p>
<p><strong>Answer:</strong> </p>
<ol>
<li>Create private endpoint for Text Analytics in Azure VNet subnet</li>
<li>Configure Private DNS zone and link to VNet</li>
<li>Disable public network access on Text Analytics resource</li>
<li>Configure on-premises DNS to forward <code>*.cognitiveservices.azure.com</code> queries to Azure Private DNS</li>
<li>Azure systems use private endpoint via VNet, on-premises systems use via ExpressRoute</li>
</ol>
<section id="14-monitoring-and-diagnostics"><h2>1.4 Monitoring and Diagnostics</h2>
<p>Effective monitoring ensures AI services meet performance requirements and enables quick problem resolution. The exam tests knowledge of metrics, logs, alerts, and Application Insights integration.</p>
<h3 id="azure-monitor-metrics-what-to-monitor-and-why">Azure Monitor Metrics - What to Monitor and Why</h3>
<p>Azure AI Services automatically publish metrics to Azure Monitor. Understanding which metrics matter and what they indicate is critical for both exam and real-world operations.</p>
<p><strong>Core Metrics for All AI Services:</strong></p>
<p><strong>Total Calls (Count):</strong></p>
<ul>
<li><strong>What:</strong> Number of API calls over time period</li>
<li><strong>Why It Matters:</strong> Trending usage patterns, capacity planning, detecting anomalies</li>
<li><strong>Exam Insight:</strong> Sudden drops might indicate application failures, spikes might indicate unexpected load or potential abuse</li>
</ul>
<p><strong>Successful Calls (Count):</strong></p>
<ul>
<li><strong>What:</strong> API calls that completed successfully (HTTP 200-299)</li>
<li><strong>Why It Matters:</strong> Combined with Total Calls gives success rate</li>
<li><strong>Formula:</strong> Success Rate = (Successful Calls / Total Calls) √ó 100</li>
<li><strong>Exam Insight:</strong> Success rate < 99% indicates problems worth investigating</li>
</ul>
<p><strong>Total Errors (Count):</strong></p>
<ul>
<li><strong>What:</strong> Failed API calls (HTTP 400-599)</li>
<li><strong>Why It Matters:</strong> Indicates client errors (4xx) or service issues (5xx)</li>
<li><strong>Exam Insight:</strong> Spike in errors correlates with application bugs or service problems</li>
</ul>
<p><strong>Data In/Out (Bytes):</strong></p>
<ul>
<li><strong>What:</strong> Volume of data sent to/from service</li>
<li><strong>Why It Matters:</strong> Cost prediction, bandwidth optimization opportunities</li>
<li><strong>Exam Insight:</strong> High data transfer with low call counts suggests sending unnecessarily large payloads</li>
</ul>
<p><strong>Latency (Milliseconds):</strong></p>
<ul>
<li><strong>What:</strong> Time service takes to process requests</li>
<li><strong>Measured:</strong> P50 (median), P95 (95th percentile), P99 (99th percentile)</li>
<li><strong>Why It Matters:</strong> User experience, SLA compliance</li>
<li><strong>Exam Insight:</strong> Monitor P95 not average - average hides outliers that affect users</li>
</ul>
<p><strong>Throttled Requests (Count):</strong></p>
<ul>
<li><strong>What:</strong> Requests denied due to rate limit exceeded (HTTP 429)</li>
<li><strong>Why It Matters:</strong> Indicates need to upgrade tier or reduce request rate</li>
<li><strong>Exam Insight:</strong> Any throttling indicates your tier doesn't match your usage pattern</li>
</ul>
<p><strong>Service-Specific Metrics Examples:</strong></p>
<p><strong>Computer Vision:</strong></p>
<ul>
<li>Transactions by Feature (analyze vs OCR vs custom model)</li>
<li>Image Size Processed (helps optimize cost by resizing images)</li>
</ul>
<p><strong>Text Analytics:</strong></p>
<ul>
<li>Records Processed (counts toward quota)</li>
<li>Operations by Type (sentiment vs entities vs key phrases)</li>
</ul>
<p><strong>Speech:</strong></p>
<ul>
<li>Audio Duration Processed</li>
<li>Transactions by Operation (STT vs TTS)</li>
</ul>
<h3 id="diagnostic-logs-comprehensive-understanding">Diagnostic Logs - Comprehensive Understanding</h3>
<p>Metrics tell you "what" happened (request failed), logs tell you "why" (authentication error, invalid input, timeout).</p>
<p><strong>How Diagnostic Logging Works:</strong></p>
<ol>
<li><strong>Enable Diagnostic Settings:</strong> Configure which log categories to collect and where to send them</li>
<li><strong>Logs Generated:</strong> Service writes detailed operation logs</li>
<li><strong>Logs Routed:</strong> Sent to configured destinations (Log Analytics, Storage, Event Hub)</li>
<li><strong>Query/Analyze:</strong> Use destination tools to analyze logs</li>
</ol>
<p><strong>Log Categories</strong> (vary by service but follow patterns):</p>
<p><strong>Audit Logs:</strong></p>
<ul>
<li>WHO accessed the service (if using Azure AD auth)</li>
<li>WHEN they accessed it</li>
<li>WHAT operation they performed</li>
<li>Result (success/failure)</li>
</ul>
<p><strong>Request/Response Logs:</strong></p>
<ul>
<li>Full request details (headers, parameters, body size)</li>
<li>Processing duration</li>
<li>Response status code</li>
<li>Error messages if failed</li>
</ul>
<p><strong>Diagnostic Logs:</strong></p>
<ul>
<li>Internal service operations</li>
<li>Performance metrics per request</li>
<li>Detailed error information</li>
</ul>
<p><strong>Destination Options:</strong></p>
<p><strong>Log Analytics Workspace:</strong></p>
<ul>
<li><strong>Best For:</strong> Querying, analysis, visualization, alerts</li>
<li><strong>Capabilities:</strong> Kusto Query Language (KQL) for complex queries</li>
<li><strong>Cost:</strong> Pay per GB ingested and retained</li>
<li><strong>Retention:</strong> Configurable (30-730 days)</li>
<li><strong>Exam Use:</strong> Primary destination for production monitoring</li>
</ul>
<p><strong>Azure Storage Account:</strong></p>
<ul>
<li><strong>Best For:</strong> Long-term retention, compliance archival</li>
<li><strong>Capabilities:</strong> Cheap storage, but difficult to query</li>
<li><strong>Cost:</strong> Very low (cents per GB/month)</li>
<li><strong>Retention:</strong> Unlimited (configure lifecycle policies)</li>
<li><strong>Exam Use:</strong> Compliance scenarios requiring multi-year log retention</li>
</ul>
<p><strong>Event Hub:</strong></p>
<ul>
<li><strong>Best For:</strong> Streaming to external systems (Splunk, Elastic, etc.)</li>
<li><strong>Capabilities:</strong> Real-time log streaming</li>
<li><strong>Cost:</strong> Based on throughput units</li>
<li><strong>Exam Use:</strong> Integrating with existing SIEM or monitoring solutions</li>
</ul>
<p><strong>Configuring Diagnostic Settings (Python SDK):</strong></p>
<pre><code class="language-python">from azure.mgmt.monitor import MonitorManagementClient
from azure.identity import DefaultAzureCredential

credential = DefaultAzureCredential()
monitor_client = MonitorManagementClient(credential, subscription_id)

# Configure diagnostic settings
diagnostic_settings = {
    &#x27;workspace_id&#x27;: &#x27;/subscriptions/{sub}/resourceGroups/{rg}/providers/Microsoft.OperationalInsights/workspaces/{workspace}&#x27;,
    &#x27;logs&#x27;: [
        {
            &#x27;category&#x27;: &#x27;Audit&#x27;,  # Capture audit logs
            &#x27;enabled&#x27;: True,
            &#x27;retentionPolicy&#x27;: {
                &#x27;enabled&#x27;: True,
                &#x27;days&#x27;: 90  # Retain for 90 days
            }
        },
        {
            &#x27;category&#x27;: &#x27;RequestResponse&#x27;,  # Capture request/response details
            &#x27;enabled&#x27;: True,
            &#x27;retentionPolicy&#x27;: {
                &#x27;enabled&#x27;: True,
                &#x27;days&#x27;: 30
            }
        }
    ],
    &#x27;metrics&#x27;: [
        {
            &#x27;category&#x27;: &#x27;AllMetrics&#x27;,  # Capture all metrics
            &#x27;enabled&#x27;: True,
            &#x27;retentionPolicy&#x27;: {
                &#x27;enabled&#x27;: False  # Metrics don&#x27;t need long retention
            }
        }
    ]
}

# Apply settings to Computer Vision resource
monitor_client.diagnostic_settings.create_or_update(
    resource_uri=&#x27;/subscriptions/{sub}/resourceGroups/{rg}/providers/Microsoft.CognitiveServices/accounts/{resource}&#x27;,
    name=&#x27;default&#x27;,
    parameters=diagnostic_settings
)</code></pre>
<h3 id="querying-logs-with-kusto-kql">Querying Logs with Kusto (KQL)</h3>
<p>Log Analytics uses KQL (Kusto Query Language) for querying. The exam may test basic KQL understanding.</p>
<p><strong>Find All Failed Requests in Last 24 Hours:</strong></p>
<pre><code class="language-kql">AzureDiagnostics
| where TimeGenerated &gt; ago(24h)
| where ResultCode &gt;= 400
| project TimeGenerated, OperationName, ResultCode, DurationMs, CallerIPAddress
| order by TimeGenerated desc</code></pre>
<p><strong>Calculate Success Rate by Hour:</strong></p>
<pre><code class="language-kql">AzureDiagnostics
| where TimeGenerated &gt; ago(7d)
| summarize 
    TotalCalls = count(),
    SuccessfulCalls = countif(ResultCode &lt; 400)
    by bin(TimeGenerated, 1h)
| extend SuccessRate = (SuccessfulCalls * 100.0) / TotalCalls
| project TimeGenerated, TotalCalls, SuccessRate
| render timechart</code></pre>
<p><strong>Find Most Common Errors:</strong></p>
<pre><code class="language-kql">AzureDiagnostics
| where ResultCode &gt;= 400
| summarize Count = count() by ResultCode, ErrorMessage
| order by Count desc
| take 10</code></pre>
<div class="tip-box"><p><strong>Exam Tip:</strong> You don't need to write KQL on the exam, but understanding what KQL can do helps answer monitoring/troubleshooting questions. If a question asks "how would you identify the most common API errors," knowing you'd query logs is the answer.</p></div>
<h3 id="creating-effective-alerts">Creating Effective Alerts</h3>
<p>Alerts proactively notify you when metrics exceed thresholds or conditions occur, enabling response before users are significantly affected.</p>
<p><strong>Alert Rule Components:</strong></p>
<ol>
<li><strong>Resource:</strong> Which service to monitor</li>
<li><strong>Condition:</strong> What to check (metric threshold, log query result)</li>
<li><strong>Action Group:</strong> What to do when condition met (email, SMS, webhook, Azure Function)</li>
<li><strong>Alert Rule Logic:</strong> How often to evaluate, aggregation period</li>
</ol>
<div class="tip-box"><p><strong>Example Alert Scenarios:</strong></p></div>
<p><strong>High Error Rate Alert:</strong></p>
<pre><code class="language-python"># Create alert when error rate exceeds 5%
alert_condition = {
    &#x27;metric_name&#x27;: &#x27;TotalErrors&#x27;,
    &#x27;operator&#x27;: &#x27;GreaterThan&#x27;,
    &#x27;threshold&#x27;: 5,  # 5% error rate
    &#x27;time_aggregation&#x27;: &#x27;Average&#x27;,
    &#x27;window_size&#x27;: &#x27;PT5M&#x27;  # 5-minute window
}</code></pre>
<p><strong>Approaching Quota Alert:</strong></p>
<pre><code class="language-python"># Alert at 80% of monthly quota
alert_condition = {
    &#x27;metric_name&#x27;: &#x27;TotalCalls&#x27;,
    &#x27;operator&#x27;: &#x27;GreaterThan&#x27;,
    &#x27;threshold&#x27;: 4000,  # 80% of 5000 free tier quota
    &#x27;time_aggregation&#x27;: &#x27;Total&#x27;,
    &#x27;window_size&#x27;: &#x27;P1M&#x27;  # Monthly
}</code></pre>
<p><strong>Latency SLA Violation Alert:</strong></p>
<pre><code class="language-python"># Alert when P95 latency exceeds 2 seconds
alert_condition = {
    &#x27;metric_name&#x27;: &#x27;Latency&#x27;,
    &#x27;operator&#x27;: &#x27;GreaterThan&#x27;,
    &#x27;threshold&#x27;: 2000,  # milliseconds
    &#x27;time_aggregation&#x27;: &#x27;P95&#x27;,  # 95th percentile
    &#x27;window_size&#x27;: &#x27;PT15M&#x27;  # 15 minutes
}</code></pre>
<p><strong>Action Groups</strong> - What Happens When Alert Fires:</p>
<p><strong>Email/SMS Notification:</strong></p>
<ul>
<li>Sends notification to specified email addresses or phone numbers</li>
<li>Best for: Informational alerts, business hours monitoring</li>
</ul>
<p><strong>Azure Function/Logic App:</strong></p>
<ul>
<li>Triggers automated remediation</li>
<li>Example: Auto-scale up pricing tier when throttling detected</li>
<li>Best for: Automatable responses</li>
</ul>
<p><strong>Webhook:</strong></p>
<ul>
<li>Posts alert data to external system</li>
<li>Best for: Integrating with ServiceNow, PagerDuty, Slack, Teams</li>
</ul>
<div class="tip-box"><p><strong>Exam Alert Best Practices:</strong></p></div>
<ul>
<li>‚úÖ Alert on trends, not single data points (5-15 minute windows, not 1 minute)</li>
<li>‚úÖ Use appropriate severity (Critical for service down, Warning for degradation)</li>
<li>‚úÖ Alert on leading indicators (latency increasing) not just failures</li>
<li>‚úÖ Prevent alert fatigue - set thresholds that indicate real problems</li>
<li>‚úÖ Include runbook links in alert descriptions for faster resolution</li>
</ul>
<section id="15-cost-management-and-optimization"><h2>1.5 Cost Management and Optimization</h2>
<p>Understanding and controlling AI service costs is tested throughout the exam. You must know cost models, tracking methods, and optimization strategies.</p>
<h3 id="understanding-cost-drivers">Understanding Cost Drivers</h3>
<p><strong>Primary Cost Factors:</strong></p>
<p><strong>1. Transaction Volume:</strong> Most services charge per operation</p>
<ul>
<li>Computer Vision: Per 1,000 images analyzed</li>
<li>Text Analytics: Per 1,000 text records</li>
<li>Translator: Per 1M characters</li>
<li>Speech: Per audio hour (STT) or per 1M characters (TTS)</li>
</ul>
<p><strong>2. Feature Complexity:</strong> Advanced features cost more</p>
<ul>
<li>Computer Vision basic analyze: $1.50/1K</li>
<li>Computer Vision Read API (OCR): $2.50/1K</li>
<li>Custom Vision training: $20/hour</li>
<li>Custom Vision prediction: $1.40/1K</li>
</ul>
<p><strong>3. Data Transfer:</strong> Cross-region transfer incurs charges</p>
<ul>
<li>Within same region: Free</li>
<li>Between regions: $0.02-0.08 per GB depending on regions</li>
<li>Egress to internet: Varies by volume tier</li>
</ul>
<p><strong>4. Storage</strong> (for services that store data):</p>
<ul>
<li>Custom Vision: Stores training images</li>
<li>Document Intelligence: Stores training documents</li>
<li>Costs: Standard Azure Storage rates (~$0.02/GB/month)</li>
</ul>
<p id="tracking-costs-with-azure-cost-management"><strong>Tracking Costs with Azure Cost Management:</strong></p>
<p><strong>View Spending by Resource:</strong></p>
<ol>
<li>Navigate to Cost Management + Billing in Azure Portal</li>
<li>Select "Cost Analysis"</li>
<li>Filter by:</li>
</ol>
<ul>
<li>Resource Group: See all AI services in a group</li>
<li>Resource: See specific service costs</li>
<li>Service: See all Computer Vision costs across resources</li>
<li>Tag: Use tags to allocate costs (department, application, environment)</li>
</ul>
<p><strong>Create Budgets:</strong></p>
<pre><code class="language-python"># Budget Alert Example (understanding, not code you&#x27;d write)
budget = {
    &#x27;amount&#x27;: 1000,  # $1,000 monthly budget
    &#x27;timeGrain&#x27;: &#x27;Monthly&#x27;,
    &#x27;timePeriod&#x27;: {
        &#x27;startDate&#x27;: &#x27;2025-01-01&#x27;,
        &#x27;endDate&#x27;: &#x27;2025-12-31&#x27;
    },
    &#x27;notifications&#x27;: {
        &#x27;Actual_GreaterThan_80_Percent&#x27;: {
            &#x27;enabled&#x27;: True,
            &#x27;operator&#x27;: &#x27;GreaterThan&#x27;,
            &#x27;threshold&#x27;: 80,  # Alert at 80% of budget
            &#x27;contactEmails&#x27;: [&#x27;finance@company.com&#x27;, &#x27;devteam@company.com&#x27;]
        },
        &#x27;Forecasted_GreaterThan_100_Percent&#x27;: {
            &#x27;enabled&#x27;: True,
            &#x27;operator&#x27;: &#x27;GreaterThan&#x27;,
            &#x27;threshold&#x27;: 100,  # Alert if forecasted to exceed budget
            &#x27;contactEmails&#x27;: [&#x27;finance@company.com&#x27;]
        }
    }
}</code></pre>
<p id="cost-optimization-strategies"><strong>Cost Optimization Strategies:</strong></p>
<p><strong>1. Right-Size Resources:</strong></p>
<ul>
<li><strong>Problem:</strong> Using S3 tier when S1 tier sufficient</li>
<li><strong>Solution:</strong> Analyze actual usage, downgrade if under-utilized</li>
<li><strong>Savings:</strong> Up to 50-70% depending on tier difference</li>
</ul>
<p><strong>2. Use Commitment Pricing for Stable Workloads:</strong></p>
<ul>
<li><strong>Problem:</strong> Paying $10,000/month pay-as-you-go for consistent 5M operations</li>
<li><strong>Solution:</strong> Commit to 5M operations at 30% discount = $7,000/month</li>
<li><strong>Savings:</strong> $3,000/month ($36,000/year)</li>
</ul>
<p><strong>3. Optimize Data Sent to Services:</strong></p>
<ul>
<li><strong>Problem:</strong> Sending 4MB images when service only needs 500KB</li>
<li><strong>Solution:</strong> Resize images client-side before upload</li>
<li><strong>Impact:</strong> </li>
<li>Reduces data transfer costs</li>
<li>Faster upload times</li>
<li>Lower processing costs (some services charge by data volume)</li>
<li>Better user experience</li>
</ul>
<p><strong>4. Implement Client-Side Caching:</strong></p>
<ul>
<li><strong>Problem:</strong> Analyzing same product image 1000 times/day</li>
<li><strong>Solution:</strong> Cache Computer Vision results for 24 hours</li>
<li><strong>Savings:</strong> 999 API calls eliminated per image = significant cost reduction</li>
</ul>
<p><strong>5. Batch Operations When Supported:</strong></p>
<ul>
<li><strong>Problem:</strong> Making 1000 individual Text Analytics calls</li>
<li><strong>Solution:</strong> Batch up to 10 documents per call = 100 calls instead of 1000</li>
<li><strong>Savings:</strong> Reduces billable transactions, improves performance</li>
</ul>
<p><strong>6. Use Appropriate Features:</strong></p>
<ul>
<li><strong>Problem:</strong> Using Custom Vision when Computer Vision's pre-built model works</li>
<li><strong>Solution:</strong> Use Computer Vision (cheaper, no training needed)</li>
<li><strong>Savings:</strong> Avoid custom model training costs ($20/hour) and higher prediction costs</li>
</ul>
<p><strong>7. Regional Cost Optimization:</strong></p>
<ul>
<li><strong>Problem:</strong> Running Computer Vision in expensive region (Australia)</li>
<li><strong>Solution:</strong> Move to cheaper region if compliance allows (East US)</li>
<li><strong>Savings:</strong> ~10-15% cost reduction</li>
<li><strong>Consideration:</strong> Ensure latency acceptable after move</li>
</ul>
<div class="tip-box"><p><strong>Exam Cost Optimization Scenarios:</strong></p></div>
<p><strong>Scenario 1:</strong> <em>"Your Text Analytics costs doubled last month. How do you investigate?"</em></p>
<p><strong>Answer Approach:</strong></p>
<ol>
<li>Check Azure Cost Management for transaction volume trends</li>
<li>Review metrics in Azure Monitor - did total calls double?</li>
<li>If calls didn't double, check if pricing tier changed (e.g., S1 ‚Üí S3)</li>
<li>Review application logs - any changes to usage patterns?</li>
<li>Check if new feature enabled (e.g., started using PII detection which costs more)</li>
</ol>
<p><strong>Scenario 2:</strong> <em>"Your application processes 10M images monthly with Computer Vision. Usage is stable. Current cost is $15,000/month. How can you reduce costs?"</em></p>
<p><strong>Answer:</strong> </p>
<ul>
<li>Current: Pay-as-you-go at $1.50/1K images = $15,000</li>
<li>Solution: Evaluate commitment tier for 10M operations</li>
<li>Commitment tier discount: ~30% = $10,500/month</li>
<li>Additional: Analyze if images can be resized (smaller images = faster processing)</li>
<li>Result: Commitment tier saves $4,500/month with minimal risk given stable usage</li>
</ul>
<p><strong>Scenario 3:</strong> <em>"You need to process support tickets in multiple languages. Should you use Translator before Text Analytics or does Text Analytics auto-detect language?"</em></p>
<p><strong>Answer:</strong> Text Analytics auto-detects language, so translating first wastes money on Translator calls. Use Text Analytics directly - it analyzes text in 100+ languages natively. This saves translation costs entirely.</p>
<p>This completes Section 1 foundation. The next sections will cover all specific AI services with the same level of detail - concepts, when to use, REST APIs, SDK examples, and exam scenarios.</p>
<p>Would you like me to continue with Section 2 (Decision-Support Solutions), or should I pause here for you to review what I've created so far?</p>
<hr>
<section id="section-2-implement-decision-support-solutions-10-15"><h1>SECTION 2: IMPLEMENT DECISION-SUPPORT SOLUTIONS (10-15%)</h1>
<p>This section covers Azure AI services that help applications make intelligent decisions about content safety and data anomalies. While this section represents a smaller portion of the exam compared to NLP or Vision, the services covered here are critical for building production-ready AI applications that handle user-generated content safely and monitor data for unusual patterns.</p>
<section id="21-azure-ai-content-safety-comprehensive-understanding"><h2>2.1 Azure AI Content Safety - Comprehensive Understanding</h2>
<h3 id="what-is-content-safety-and-why-does-it-exist">What Is Content Safety and Why Does It Exist?</h3>
<p>Azure AI Content Safety analyzes text and images for potentially harmful content across four primary categories: hate speech, violence, sexual content, and self-harm. The service exists to solve a critical problem in modern applications: user-generated content can contain harmful material that violates community standards, legal requirements, or ethical guidelines.</p>
<p><strong>The Problem Content Safety Solves:</strong> Social platforms, content sharing sites, gaming communities, and any application accepting user input face the challenge of moderating content at scale. Manual human moderation cannot keep pace with millions of uploads daily. Content that slips through moderation can expose users to harmful material, create legal liability, damage brand reputation, and violate regulatory requirements in various jurisdictions.</p>
<p><strong>How Content Safety Works Conceptually:</strong> The service uses machine learning models trained on vast datasets of harmful and safe content. When you submit text or an image, the models analyze it across multiple dimensions and assign severity scores for each harmful content category. The scores range from zero (safe) to seven (extremely harmful), allowing you to make decisions based on your application's tolerance levels.</p>
<p><strong>Important Distinction:</strong> Content Safety detects potentially harmful content but does not make the final decision about what to allow or block. You configure thresholds and rules based on your application's requirements, community standards, and legal obligations. Different applications have different tolerance levels - a children's education app requires stricter moderation than an adult-focused social network.</p>
<h3 id="content-categories-explained">Content Categories Explained</h3>
<p>Understanding what each category detects helps you configure appropriate thresholds and explain moderation decisions to users.</p>
<p><strong>Hate Speech:</strong> Detects content attacking people or groups based on protected characteristics including race, ethnicity, religion, gender, sexual orientation, disability, or national origin. This includes slurs, dehumanizing language, stereotypes presented as facts, and calls for discrimination or violence against groups.</p>
<p>Examples the service detects as hate speech:</p>
<ul>
<li>Racial or ethnic slurs and derogatory terms</li>
<li>Content promoting discrimination against religious groups</li>
<li>Dehumanizing language comparing groups to animals or objects</li>
<li>Content advocating violence or harm based on protected characteristics</li>
</ul>
<p>The severity scoring considers both explicit hate speech (severity six to seven) and subtle biased language (severity two to three), allowing you to moderate based on your community standards.</p>
<p><strong>Violence:</strong> Detects content depicting, glorifying, or inciting violence. This includes graphic violence, weapons in threatening contexts, descriptions of violent acts, and content encouraging physical harm to people or animals.</p>
<p>Examples the service detects:</p>
<ul>
<li>Graphic descriptions or images of violence, injury, or death</li>
<li>Content depicting weapons being used to harm people</li>
<li>Instructions for creating weapons or carrying out violent acts</li>
<li>Glorification of mass violence or terrorism</li>
<li>Animal cruelty or abuse</li>
</ul>
<p>The service distinguishes between educational content about violence (discussing history, news) versus glorifying or inciting violence, though this requires careful threshold setting.</p>
<p><strong>Sexual Content:</strong> Detects sexually explicit material including nudity, sexual acts, and sexually suggestive content. The service calibrates detection based on context - medical or educational sexual content receives lower severity scores than pornographic material.</p>
<p>Examples the service detects:</p>
<ul>
<li>Explicit sexual imagery or descriptions</li>
<li>Pornographic content</li>
<li>Sexual solicitation or exploitation</li>
<li>Content sexualizing minors (severity automatically maximized)</li>
</ul>
<p>The severity levels help you distinguish between content that is merely suggestive (lower severity) versus explicitly pornographic (higher severity), enabling appropriate moderation for your audience.</p>
<p><strong>Self-Harm:</strong> Detects content related to suicide, self-injury, eating disorders, or other self-destructive behaviors. This is particularly sensitive because such content can trigger vulnerable individuals or provide harmful instructions.</p>
<p>Examples the service detects:</p>
<ul>
<li>Descriptions or depictions of self-harm methods</li>
<li>Suicidal ideation or plans</li>
<li>Content glorifying or encouraging eating disorders</li>
<li>Instructions for self-injury</li>
<li>Content romanticizing self-destructive behaviors</li>
</ul>
<p>The challenge with self-harm content is distinguishing between harmful content versus people seeking help or support resources discussing these topics constructively. The service provides severity scores that help you make nuanced decisions - someone posting "I'm having suicidal thoughts and need help" should be connected with support resources, not simply blocked.</p>
<h3 id="severity-levels-and-threshold-configuration">Severity Levels and Threshold Configuration</h3>
<p>Content Safety assigns severity scores from zero to seven for each category. Understanding how to interpret and configure thresholds is critical for the exam.</p>
<p><strong>Severity Score Interpretation:</strong></p>
<ul>
<li><strong>0:</strong> No harmful content detected in this category</li>
<li><strong>1-2:</strong> Minimal concern - mild language or very subtle content</li>
<li><strong>3-4:</strong> Moderate concern - content clearly in the category but not extreme</li>
<li><strong>5-6:</strong> High concern - severe or explicit harmful content</li>
<li><strong>7:</strong> Extreme concern - maximally harmful content</li>
</ul>
<p><strong>Configuring Thresholds:</strong> You decide the minimum severity level that triggers a moderation action. If you set a threshold of four for hate speech, content with severity zero through three passes through while four and above gets blocked or flagged for review.</p>
<p><strong>Threshold Strategy Examples:</strong></p>
<p><strong>Strict Moderation (Children's App):</strong></p>
<ul>
<li>Hate Speech: Block at severity 2</li>
<li>Violence: Block at severity 2</li>
<li>Sexual Content: Block at severity 1</li>
<li>Self-Harm: Block at severity 2</li>
<li>Rationale: Protect young users from any potentially harmful content, accepting higher false positive rates</li>
</ul>
<p><strong>Balanced Moderation (General Social Platform):</strong></p>
<ul>
<li>Hate Speech: Block at severity 4, flag for review at 3</li>
<li>Violence: Block at severity 5, flag at 4</li>
<li>Sexual Content: Block at severity 4</li>
<li>Self-Harm: Block at severity 3 (lower threshold due to sensitivity)</li>
<li>Rationale: Allow robust discussion while protecting users from severe content</li>
</ul>
<p><strong>Permissive Moderation (Adult Discussion Forum):</strong></p>
<ul>
<li>Hate Speech: Block at severity 5, flag at 4</li>
<li>Violence: Block at severity 6</li>
<li>Sexual Content: Block at severity 6</li>
<li>Self-Harm: Block at severity 4</li>
<li>Rationale: Maximize free expression while removing extreme content</li>
</ul>
<p>The exam may present scenarios describing user populations and ask you to recommend appropriate thresholds. Consider the vulnerability of the audience, legal requirements, and community standards when making threshold decisions.</p>
<h3 id="rest-api-implementation">REST API Implementation</h3>
<p><strong>Analyze Text for Harmful Content:</strong></p>
<pre><code class="language-bash">POST https://{endpoint}/contentsafety/text:analyze?api-version=2023-10-01
Content-Type: application/json
Ocp-Apim-Subscription-Key: {your-key}

{
  &quot;text&quot;: &quot;You&#x27;re such an idiot! Go kill yourself!&quot;,
  &quot;categories&quot;: [&quot;Hate&quot;, &quot;SelfHarm&quot;, &quot;Sexual&quot;, &quot;Violence&quot;],
  &quot;blocklistNames&quot;: [&quot;CustomProfanityList&quot;],
  &quot;haltOnBlocklistHit&quot;: false,
  &quot;outputType&quot;: &quot;FourSeverityLevels&quot;
}</code></pre>
<p><strong>Response Example:</strong></p>
<pre><code class="language-json">{
  &quot;blocklistsMatch&quot;: [
    {
      &quot;blocklistName&quot;: &quot;CustomProfanityList&quot;,
      &quot;blocklistItemId&quot;: &quot;item_123&quot;,
      &quot;blocklistItemText&quot;: &quot;idiot&quot;
    }
  ],
  &quot;categoriesAnalysis&quot;: [
    {
      &quot;category&quot;: &quot;Hate&quot;,
      &quot;severity&quot;: 2
    },
    {
      &quot;category&quot;: &quot;SelfHarm&quot;, 
      &quot;severity&quot;: 6
    },
    {
      &quot;category&quot;: &quot;Sexual&quot;,
      &quot;severity&quot;: 0
    },
    {
      &quot;category&quot;: &quot;Violence&quot;,
      &quot;severity&quot;: 0
    }
  ]
}</code></pre>
<p><strong>Understanding the Response:</strong></p>
<ul>
<li>The text matched a custom blocklist item ("idiot")</li>
<li>Hate speech severity is 2 (minimal - "idiot" is rude but not severe hate speech)</li>
<li>Self-harm severity is 6 (high - "kill yourself" is severe self-harm content)</li>
<li>No sexual or violence content detected</li>
</ul>
<p>Your application would typically block or flag this content due to the high self-harm severity.</p>
<h3 id="python-sdk-implementation">Python SDK Implementation</h3>
<p><strong>Basic Text Analysis:</strong></p>
<pre><code class="language-python">from azure.ai.contentsafety import ContentSafetyClient
from azure.ai.contentsafety.models import AnalyzeTextOptions
from azure.core.credentials import AzureKeyCredential

# Initialize client
endpoint = &quot;https://your-resource.cognitiveservices.azure.com/&quot;
key = &quot;your-api-key&quot;
client = ContentSafetyClient(endpoint, AzureKeyCredential(key))

# Analyze text
text_to_analyze = &quot;This is sample text to check for harmful content.&quot;

request = AnalyzeTextOptions(text=text_to_analyze)
response = client.analyze_text(request)

# Process results
for category_analysis in response.categories_analysis:
    print(f&quot;{category_analysis.category}: Severity {category_analysis.severity}&quot;)
    
    # Apply your moderation logic
    if category_analysis.category == &quot;Hate&quot; and category_analysis.severity &gt;= 4:
        print(&quot;BLOCKED: Hate speech detected&quot;)
    elif category_analysis.category == &quot;SelfHarm&quot; and category_analysis.severity &gt;= 3:
        print(&quot;BLOCKED: Self-harm content detected&quot;)</code></pre>
<p><strong>Implementing Moderation Decision Logic:</strong></p>
<pre><code class="language-python">def moderate_content(text, thresholds):
    &quot;&quot;&quot;
    Moderates content based on configurable thresholds.
    
    Args:
        text: The text to analyze
        thresholds: Dict mapping categories to severity thresholds
                   e.g., {&quot;Hate&quot;: 4, &quot;Violence&quot;: 5, &quot;Sexual&quot;: 4, &quot;SelfHarm&quot;: 3}
    
    Returns:
        Dict with decision and reasons
    &quot;&quot;&quot;
    request = AnalyzeTextOptions(text=text)
    response = client.analyze_text(request)
    
    blocked_categories = []
    
    for analysis in response.categories_analysis:
        category = analysis.category
        severity = analysis.severity
        threshold = thresholds.get(category, 7)  # Default: only block severity 7
        
        if severity &gt;= threshold:
            blocked_categories.append({
                &quot;category&quot;: category,
                &quot;severity&quot;: severity,
                &quot;threshold&quot;: threshold
            })
    
    if blocked_categories:
        return {
            &quot;decision&quot;: &quot;BLOCKED&quot;,
            &quot;reason&quot;: f&quot;Content violated {len(blocked_categories)} categories&quot;,
            &quot;violations&quot;: blocked_categories
        }
    else:
        return {
            &quot;decision&quot;: &quot;APPROVED&quot;,
            &quot;reason&quot;: &quot;No harmful content detected above thresholds&quot;
        }

# Example usage
thresholds = {
    &quot;Hate&quot;: 4,
    &quot;Violence&quot;: 5, 
    &quot;Sexual&quot;: 4,
    &quot;SelfHarm&quot;: 3
}

result = moderate_content(&quot;User submitted content here&quot;, thresholds)
print(f&quot;Decision: {result[&#x27;decision&#x27;]}&quot;)
print(f&quot;Reason: {result[&#x27;reason&#x27;]}&quot;)</code></pre>
<h3 id="analyzing-images-for-harmful-content">Analyzing Images for Harmful Content</h3>
<p>Content Safety also analyzes images for harmful visual content using the same four categories.</p>
<p><strong>REST API - Image Analysis:</strong></p>
<pre><code class="language-bash">POST https://{endpoint}/contentsafety/image:analyze?api-version=2023-10-01
Content-Type: application/json
Ocp-Apim-Subscription-Key: {your-key}

{
  &quot;image&quot;: {
    &quot;content&quot;: &quot;base64-encoded-image-data&quot;
  },
  &quot;categories&quot;: [&quot;Hate&quot;, &quot;SelfHarm&quot;, &quot;Sexual&quot;, &quot;Violence&quot;],
  &quot;outputType&quot;: &quot;FourSeverityLevels&quot;
}</code></pre>
<p><strong>Python SDK - Image Analysis:</strong></p>
<pre><code class="language-python">from azure.ai.contentsafety.models import AnalyzeImageOptions, ImageData
import base64

# Read and encode image
with open(&quot;user_uploaded_image.jpg&quot;, &quot;rb&quot;) as f:
    image_bytes = f.read()

# Create request
request = AnalyzeImageOptions(
    image=ImageData(content=image_bytes)
)

# Analyze image
response = client.analyze_image(request)

for analysis in response.categories_analysis:
    print(f&quot;Image {analysis.category}: Severity {analysis.severity}&quot;)
    
    # Apply moderation
    if analysis.severity &gt;= 4:
        print(f&quot;WARNING: Image contains {analysis.category} content&quot;)</code></pre>
<h3 id="custom-blocklists-advanced-content-moderation">Custom Blocklists - Advanced Content Moderation</h3>
<p>Custom blocklists allow you to define specific terms, phrases, or patterns that should be blocked beyond the AI model's general detection. This addresses domain-specific moderation needs.</p>
<p><strong>Use Cases for Custom Blocklists:</strong></p>
<ul>
<li>Company-specific profanity policies (words you consider inappropriate that aren't generally offensive)</li>
<li>Competitor brand names you don't want mentioned in user content</li>
<li>Specific phrases that violate your terms of service</li>
<li>Domain-specific terminology that is problematic in your context</li>
</ul>
<p><strong>Creating and Managing Blocklists:</strong></p>
<pre><code class="language-python"># Create a blocklist
blocklist_name = &quot;company_terms_blocklist&quot;
blocklist_description = &quot;Terms specific to our community standards&quot;

blocklist = client.create_or_update_text_blocklist(
    blocklist_name=blocklist_name,
    options={&quot;description&quot;: blocklist_description}
)

# Add items to blocklist
blocklist_items = [
    {&quot;description&quot;: &quot;Competitor reference&quot;, &quot;text&quot;: &quot;CompetitorBrandName&quot;},
    {&quot;description&quot;: &quot;Prohibited phrase&quot;, &quot;text&quot;: &quot;specific banned phrase&quot;},
    {&quot;description&quot;: &quot;Company policy violation&quot;, &quot;text&quot;: &quot;internal term&quot;}
]

client.add_or_update_blocklist_items(
    blocklist_name=blocklist_name,
    options={&quot;blocklistItems&quot;: blocklist_items}
)

# Use blocklist in analysis
request = AnalyzeTextOptions(
    text=&quot;User content mentioning CompetitorBrandName&quot;,
    blocklist_names=[&quot;company_terms_blocklist&quot;],
    halt_on_blocklist_hit=True  # Stop analysis immediately if blocklist match found
)

response = client.analyze_text(request)

# Check for blocklist matches
if response.blocklists_match:
    for match in response.blocklists_match:
        print(f&quot;Blocked term found: {match.blocklist_item_text}&quot;)
        print(f&quot;From blocklist: {match.blocklist_name}&quot;)</code></pre>
<h3 id="when-to-use-content-safety-exam-decision-framework">When to Use Content Safety - Exam Decision Framework</h3>
<p><strong>Use Content Safety When:</strong></p>
<ul>
<li>‚úÖ Application accepts user-generated content (posts, comments, uploads)</li>
<li>‚úÖ Need to moderate content at scale (thousands/millions of items daily)</li>
<li>‚úÖ Requirements mention "community safety," "content moderation," or "harmful content detection"</li>
<li>‚úÖ Building social platforms, forums, chat applications, content sharing sites</li>
<li>‚úÖ Need to comply with regulations around harmful content (especially for minors)</li>
<li>‚úÖ Want to detect content before storing or displaying it</li>
</ul>
<p><strong>Content Safety Is NOT Required When:</strong></p>
<ul>
<li>Applications only display curated content (no user uploads)</li>
<li>Content comes from trusted, pre-moderated sources</li>
<li>Manual human moderation is feasible and sufficient (very low volume)</li>
</ul>
<div class="tip-box">
<p><strong>Exam Scenario Example:</strong></p>
<p><em>"You are building a social platform where users can post text and images. The platform serves users globally including minors. Regulatory requirements mandate detecting and blocking harmful content. What service should you use?"</em></p>
<p><strong>Answer:</strong> Azure AI Content Safety. The scenario explicitly mentions user-generated content, minors (requiring strict moderation), and regulatory requirements for harmful content detection. Configure Content Safety with strict thresholds given the minor population, analyze both text and images before storing/displaying, and implement human review workflows for borderline content (severity 3-4).</p>
</div>
<section id="22-anomaly-detector-service-comprehensive-understanding"><h2>2.2 Anomaly Detector Service - Comprehensive Understanding</h2>
<h3 id="what-is-anomaly-detector-and-why-does-it-exist">What Is Anomaly Detector and Why Does It Exist?</h3>
<p>Anomaly Detector identifies unusual patterns in time-series data using machine learning. The service solves a fundamental problem: detecting anomalies in metrics is critical for applications, infrastructure, and business operations, but defining what constitutes "unusual" is challenging when normal patterns vary over time and include trends, seasonality, and complex patterns.</p>
<p><strong>The Problem Anomaly Detector Solves:</strong> Traditional monitoring uses static thresholds (alert if CPU exceeds eighty percent, alert if sales drop below one hundred units). These approaches fail because normal patterns vary - website traffic might be high on weekends, sales have seasonal patterns, server load varies by time of day. Static thresholds generate false alarms when variation is normal or miss true anomalies that fall within normal ranges at the wrong time.</p>
<p><strong>How Anomaly Detector Works Conceptually:</strong> The service analyzes historical time-series data to learn normal patterns including trends (gradual increase/decrease over time), seasonality (recurring patterns like daily or weekly cycles), and normal variance ranges. When analyzing new data points, it determines if they deviate significantly from expected patterns given the learned behavior. Anomalies are flagged when values are statistically improbable given the historical context.</p>
<p><strong>Key Insight:</strong> Anomaly Detector adapts to your data automatically. You do not manually configure what is normal - the service learns from the data you provide. This makes it powerful for diverse use cases from server monitoring to business metrics to IoT sensor data.</p>
<h3 id="univariate-vs-multivariate-anomaly-detection">Univariate vs Multivariate Anomaly Detection</h3>
<p>Anomaly Detector supports two modes that address different use cases. Understanding when to use each is critical for the exam.</p>
<p><strong>Univariate Anomaly Detection:</strong> Analyzes a single metric over time. You provide a time series of values (website traffic per hour, temperature readings per minute, sales per day), and the service identifies points that are anomalous for that single metric.</p>
<p><strong>When to Use Univariate:</strong></p>
<ul>
<li>Monitoring single metrics (CPU usage, request count, temperature)</li>
<li>Each metric is independently meaningful</li>
<li>Simple anomaly detection needs</li>
</ul>
<div class="tip-box"><p><strong>Example Univariate Use Case:</strong> Detecting unusual spikes in API request volume. You track requests per minute and want to alert when volume deviates significantly from normal patterns accounting for time-of-day and day-of-week variations.</p></div>
<p><strong>Multivariate Anomaly Detection:</strong> Analyzes multiple related metrics simultaneously and detects anomalies in the relationships between metrics. You provide multiple time series that are related (CPU, memory, disk I/O, network traffic), and the service identifies when the pattern across all metrics is unusual, even if individual metrics are within normal ranges.</p>
<p><strong>When to Use Multivariate:</strong></p>
<ul>
<li>Monitoring complex systems with interdependent metrics</li>
<li>Anomalies manifest as unusual patterns across multiple metrics</li>
<li>Need to detect subtle issues that wouldn't appear in single metrics</li>
</ul>
<div class="tip-box"><p><strong>Example Multivariate Use Case:</strong> Detecting equipment failures in manufacturing. A machine might have normal temperature, pressure, vibration individually, but the combination of slightly elevated temperature with decreased pressure and increased vibration indicates early bearing failure. Univariate detection would miss this because each metric alone is within acceptable range.</p></div>
<div class="tip-box"><p><strong>Exam Decision Criteria:</strong></p></div>
<ul>
<li>Question mentions "single metric," "one variable," or "individual measurement" ‚Üí Univariate</li>
<li>Question mentions "multiple metrics," "relationships between variables," "correlated data," or "complex system" ‚Üí Multivariate</li>
<li>Question describes detecting issues that appear across multiple measurements ‚Üí Multivariate</li>
</ul>
<h3 id="univariate-anomaly-detection-implementation">Univariate Anomaly Detection - Implementation</h3>
<p><strong>REST API - Detect Entire Series:</strong></p>
<pre><code class="language-bash">POST https://{endpoint}/anomalydetector/v1.1/timeseries/entire/detect
Content-Type: application/json
Ocp-Apim-Subscription-Key: {your-key}

{
  &quot;series&quot;: [
    {&quot;timestamp&quot;: &quot;2024-01-01T00:00:00Z&quot;, &quot;value&quot;: 100.5},
    {&quot;timestamp&quot;: &quot;2024-01-01T01:00:00Z&quot;, &quot;value&quot;: 105.2},
    {&quot;timestamp&quot;: &quot;2024-01-01T02:00:00Z&quot;, &quot;value&quot;: 503.8},
    {&quot;timestamp&quot;: &quot;2024-01-01T03:00:00Z&quot;, &quot;value&quot;: 102.1}
  ],
  &quot;granularity&quot;: &quot;hourly&quot;,
  &quot;sensitivity&quot;: 95,
  &quot;maxAnomalyRatio&quot;: 0.25
}</code></pre>
<p><strong>Response Example:</strong></p>
<pre><code class="language-json">{
  &quot;period&quot;: 24,
  &quot;expectedValues&quot;: [100.2, 105.0, 103.5, 102.0],
  &quot;upperMargins&quot;: [10.0, 10.5, 10.4, 10.2],
  &quot;lowerMargins&quot;: [10.0, 10.5, 10.4, 10.2],
  &quot;isAnomaly&quot;: [false, false, true, false],
  &quot;isNegativeAnomaly&quot;: [false, false, false, false],
  &quot;isPositiveAnomaly&quot;: [false, false, true, false],
  &quot;severity&quot;: [0, 0, 0.85, 0]
}</code></pre>
<p><strong>Understanding the Response:</strong></p>
<ul>
<li><strong>period:</strong> Detected seasonality (24 = daily pattern for hourly data)</li>
<li><strong>expectedValues:</strong> What the model predicted for each point</li>
<li><strong>upperMargins/lowerMargins:</strong> Acceptable deviation range</li>
<li><strong>isAnomaly:</strong> Boolean array indicating anomalies</li>
<li><strong>isPositiveAnomaly:</strong> Anomaly is above expected (spike)</li>
<li><strong>isNegativeAnomaly:</strong> Anomaly is below expected (drop)</li>
<li><strong>severity:</strong> How anomalous (0-1 scale)</li>
</ul>
<p>In this example, the third data point (503.8) is flagged as a positive anomaly with high severity because it deviates significantly from the expected 103.5.</p>
<p><strong>Python SDK - Batch Detection:</strong></p>
<pre><code class="language-python">from azure.ai.anomalydetector import AnomalyDetectorClient
from azure.ai.anomalydetector.models import DetectRequest, TimeSeriesPoint, TimeGranularity
from azure.core.credentials import AzureKeyCredential
from datetime import datetime, timedelta

# Initialize client
endpoint = &quot;https://your-resource.cognitiveservices.azure.com/&quot;
key = &quot;your-api-key&quot;
client = AnomalyDetectorClient(endpoint, AzureKeyCredential(key))

# Prepare time series data
# In real scenarios, this would come from your monitoring system
series = []
start_time = datetime(2024, 1, 1)

for hour in range(72):  # 3 days of hourly data
    timestamp = start_time + timedelta(hours=hour)
    # Simulate normal pattern with an anomaly at hour 50
    value = 100 + (hour % 24) * 2  # Daily pattern
    if hour == 50:
        value = 500  # Anomaly spike
    
    series.append(TimeSeriesPoint(timestamp=timestamp, value=value))

# Create detection request
request = DetectRequest(
    series=series,
    granularity=TimeGranularity.HOURLY,
    sensitivity=95  # 95 = more sensitive, 50 = less sensitive
)

# Detect anomalies in entire series
response = client.detect_entire_series(request)

# Process results
print(f&quot;Detected period/seasonality: {response.period}&quot;)

for idx, (is_anomaly, severity) in enumerate(zip(response.is_anomaly, response.severity)):
    if is_anomaly:
        actual_value = series[idx].value
        expected_value = response.expected_values[idx]
        print(f&quot;Anomaly at index {idx} (time {series[idx].timestamp})&quot;)
        print(f&quot;  Actual: {actual_value}, Expected: {expected_value}&quot;)
        print(f&quot;  Severity: {severity:.2f}&quot;)
        print(f&quot;  Type: {&#x27;Spike&#x27; if response.is_positive_anomaly[idx] else &#x27;Drop&#x27;}&quot;)</code></pre>
<p><strong>Streaming Anomaly Detection (Real-Time):</strong></p>
<p>For real-time monitoring where you receive data points continuously, use the "detect last point" API that analyzes just the latest point in context of historical data.</p>
<pre><code class="language-python">from azure.ai.anomalydetector.models import DetectRequest

# Assume you have historical data
historical_series = [...]  # Your existing time series

# New data point arrives
new_point = TimeSeriesPoint(
    timestamp=datetime.now(),
    value=new_sensor_reading
)

# Add to series
current_series = historical_series + [new_point]

# Create request
request = DetectRequest(
    series=current_series,
    granularity=TimeGranularity.HOURLY
)

# Detect if latest point is anomalous
response = client.detect_last_point(request)

if response.is_anomaly:
    print(f&quot;ALERT: Current value {new_point.value} is anomalous!&quot;)
    print(f&quot;Expected: {response.expected_value}&quot;)
    print(f&quot;Deviation: {abs(new_point.value - response.expected_value)}&quot;)
    
    # Trigger your alerting system
    send_alert(
        metric=&quot;sensor_reading&quot;,
        actual=new_point.value,
        expected=response.expected_value
    )</code></pre>
<h3 id="sensitivity-parameter-critical-understanding">Sensitivity Parameter - Critical Understanding</h3>
<p>The sensitivity parameter (0-99) controls how strict anomaly detection is. This is frequently tested on the exam.</p>
<p><strong>High Sensitivity (80-99):</strong></p>
<ul>
<li>Detects more anomalies (more sensitive to deviations)</li>
<li>Higher false positive rate (normal variations flagged as anomalies)</li>
<li>Use when missing anomalies is costly (safety-critical systems, fraud detection)</li>
</ul>
<p><strong>Medium Sensitivity (50-79):</strong></p>
<ul>
<li>Balanced detection (moderate false positive/negative rates)</li>
<li>Use for general monitoring where some missed anomalies are acceptable</li>
</ul>
<p><strong>Low Sensitivity (0-49):</strong></p>
<ul>
<li>Detects only clear, significant anomalies</li>
<li>Lower false positive rate (fewer false alarms)</li>
<li>Use when alert fatigue is a concern, only want obvious anomalies</li>
</ul>
<div class="tip-box"><p><strong>Exam Scenario:</strong> <em>"Your IoT sensors sometimes have normal measurement variations due to environmental factors. You want to detect only significant equipment failures. What sensitivity should you use?"</em></p></div>
<p><strong>Answer:</strong> Low sensitivity (30-40 range). The scenario indicates normal variation is expected and you only want clear anomalies (equipment failures), suggesting you should reduce false alarms from normal variation.</p>
<h3 id="multivariate-anomaly-detection-advanced-implementation">Multivariate Anomaly Detection - Advanced Implementation</h3>
<p>Multivariate detection analyzes multiple metrics together to find anomalies in their relationships and patterns.</p>
<p><strong>Training a Multivariate Model:</strong></p>
<pre><code class="language-python">from azure.ai.anomalydetector.models import ModelInfo, DataSchema
from datetime import datetime

# Define training data source
# Data must be in Azure Blob Storage with specific structure
model_info = ModelInfo(
    start_time=datetime(2024, 1, 1),
    end_time=datetime(2024, 3, 31),  # 3 months of training data
    source=&quot;https://mystorageaccount.blob.core.windows.net/training-data/&quot;,
    data_schema=DataSchema.MULTI_TABLE,  # Or ONE_TABLE depending on your data structure
    display_name=&quot;Server Metrics Anomaly Model&quot;,
    sliding_window=200  # Number of data points in sliding window
)

# Train model (this is async and takes time)
model = client.train_multivariate_model(model_info)
model_id = model.model_id

print(f&quot;Model training started. Model ID: {model_id}&quot;)

# Check training status
import time
while True:
    model_status = client.get_multivariate_model(model_id)
    print(f&quot;Training status: {model_status.model_info.status}&quot;)
    
    if model_status.model_info.status == &quot;READY&quot;:
        print(&quot;Model training completed successfully!&quot;)
        break
    elif model_status.model_info.status == &quot;FAILED&quot;:
        print(f&quot;Training failed: {model_status.model_info.errors}&quot;)
        break
    
    time.sleep(30)  # Check every 30 seconds</code></pre>
<p><strong>Using Multivariate Model for Detection:</strong></p>
<pre><code class="language-python"># Define detection data source
detection_request = {
    &quot;source&quot;: &quot;https://mystorageaccount.blob.core.windows.net/detection-data/&quot;,
    &quot;startTime&quot;: &quot;2024-04-01T00:00:00Z&quot;,
    &quot;endTime&quot;: &quot;2024-04-30T23:59:59Z&quot;
}

# Start detection job
result = client.detect_anomaly(model_id, detection_request)
result_id = result.result_id

# Wait for detection to complete
while True:
    detection_status = client.get_detection_result(result_id)
    
    if detection_status.summary.status == &quot;SUCCEEDED&quot;:
        # Process anomaly results
        for anomaly in detection_status.results:
            print(f&quot;Anomaly detected at {anomaly.timestamp}&quot;)
            print(f&quot;Severity: {anomaly.severity}&quot;)
            
            # Which variables contributed to the anomaly?
            for interpretation in anomaly.interpretation:
                print(f&quot;  Variable: {interpretation.variable}&quot;)
                print(f&quot;  Contribution: {interpretation.contribution_score}&quot;)
        break
    elif detection_status.summary.status == &quot;FAILED&quot;:
        print(&quot;Detection failed&quot;)
        break
    
    time.sleep(10)</code></pre>
<p><strong>Data Format for Multivariate Detection:</strong></p>
<p>Your data in Azure Blob Storage should be CSV files with structure:</p>
<pre><code class="language-csv">timestamp,cpu_usage,memory_usage,disk_io,network_traffic
2024-01-01T00:00:00Z,45.2,68.5,120.3,850.2
2024-01-01T01:00:00Z,48.1,70.2,125.8,920.5
...</code></pre>
<p>Each row is a timestamp with values for all metrics. The service learns the normal relationships between these metrics and detects when patterns deviate.</p>
<h3 id="when-to-use-anomaly-detector-exam-decision-framework">When to Use Anomaly Detector - Exam Decision Framework</h3>
<p><strong>Use Anomaly Detector When:</strong></p>
<ul>
<li>‚úÖ Need to detect unusual patterns in time-series data</li>
<li>‚úÖ Normal patterns vary (seasonality, trends)</li>
<li>‚úÖ Static thresholds don't work (what's normal changes over time)</li>
<li>‚úÖ Applications: monitoring metrics, fraud detection, equipment failure prediction, business metric alerting</li>
<li>‚úÖ Have sufficient historical data (minimum 12 points for univariate, typically hundreds for accurate multivariate models)</li>
</ul>
<p><strong>Do NOT Use Anomaly Detector When:</strong></p>
<ul>
<li>Static thresholds work fine (simple limits that don't change)</li>
<li>Not time-series data (individual values without temporal relationship)</li>
<li>Insufficient historical data (cannot learn patterns from 5 data points)</li>
<li>Real-time decision needed on single value without historical context</li>
</ul>
<div class="tip-box">
<p><strong>Exam Scenario Example:</strong></p>
<p><em>"Your company monitors server CPU, memory, disk I/O, and network metrics. Servers sometimes have high CPU with low memory (normal for compute workloads) or low CPU with high disk I/O (normal for data workloads). You need to detect when the overall pattern indicates a problem. What should you use?"</em></p>
<p><strong>Answer:</strong> Multivariate Anomaly Detector. The scenario describes multiple related metrics where anomalies appear in relationships between metrics, not individual thresholds. Train a multivariate model on historical server metrics, then use it to detect when the pattern across all four metrics deviates from learned normal behavior.</p>
</div>
<hr>
<p>This completes Section 2. Now I'll continue with Section 3 (Vision Solutions), Section 4 (NLP - the largest section), Section 5 (Knowledge Mining), and Section 6 (Generative AI). Would you like me to continue adding these sections now?</p>
<section id="section-3-implement-azure-ai-vision-solutions-15-20"><h1>SECTION 3: IMPLEMENT AZURE AI VISION SOLUTIONS (15-20%)</h1>
<p>This section covers all computer vision services including general image analysis, OCR, custom models, face detection, and video analysis. Vision solutions represent fifteen to twenty percent of the exam, making this a critical area to master. The exam tests not just how to use vision APIs, but when to choose each service and how to combine them for complete solutions.</p>
<section id="31-computer-vision-service-comprehensive-understanding"><h2>3.1 Computer Vision Service - Comprehensive Understanding</h2>
<h3 id="what-is-computer-vision-and-what-problems-does-it-solve">What Is Computer Vision and What Problems Does It Solve?</h3>
<p>Azure Computer Vision provides general-purpose image analysis capabilities trained on millions of images. The service solves the fundamental challenge of making sense of visual content programmatically without requiring computer vision expertise or infrastructure.</p>
<p><strong>Core Capabilities Overview:</strong></p>
<ul>
<li><strong>Object Detection:</strong> Identify and locate objects within images</li>
<li><strong>Tagging:</strong> Generate descriptive tags for image content</li>
<li><strong>Image Captioning:</strong> Create natural language descriptions of images</li>
<li><strong>OCR (Read API):</strong> Extract text from images and documents  </li>
<li><strong>Adult Content Detection:</strong> Identify potentially inappropriate content</li>
<li><strong>Celebrity/Landmark Recognition:</strong> Identify known people and places</li>
<li><strong>Color Analysis:</strong> Extract dominant colors and determine if image is black/white</li>
<li><strong>Face Detection:</strong> Locate faces (basic detection, not identification)</li>
</ul>
<p>The service handles diverse image types, sizes, and qualities, making it suitable for everything from social media content analysis to document processing to accessibility features.</p>
<h3 id="when-to-use-computer-vision-vs-custom-vision-vs-face-api">When to Use Computer Vision vs Custom Vision vs Face API</h3>
<p>This decision framework appears frequently on the exam. Understanding the boundaries between services is critical.</p>
<p><strong>Use Computer Vision When:</strong></p>
<ul>
<li>Need general object detection (cars, trees, buildings, animals - common objects)</li>
<li>Generating accessibility descriptions for images</li>
<li>Analyzing image characteristics (colors, composition, adult content)</li>
<li>Extracting text from images (OCR/Read API)</li>
<li>Detecting presence of faces (not identifying who they are)</li>
<li>Processing diverse image types without domain-specific needs</li>
</ul>
<p><strong>Use Custom Vision Instead When:</strong></p>
<ul>
<li>Detecting domain-specific objects Computer Vision doesn't recognize (company products, specific equipment, medical conditions, logos, manufacturing defects)</li>
<li>Need higher accuracy for specific visual categories than general models provide</li>
<li>Willing to provide training images and train custom models</li>
</ul>
<p><strong>Use Face API Instead When:</strong></p>
<ul>
<li>Need face verification (is this the same person?)</li>
<li>Need face identification (who is this person from a known group?)</li>
<li>Analyzing facial features, emotions, or attributes in detail</li>
<li>Building identity verification systems</li>
</ul>
<div class="tip-box"><p><strong>Exam Tip:</strong> If question mentions "common objects," "general image analysis," or "accessibility," choose Computer Vision. If it mentions "company-specific items," "specialized recognition," or "custom categories," choose Custom Vision. If it specifically mentions "facial recognition" or "identity verification," choose Face API.</p></div>
<h3 id="computer-vision-rest-api-analyze-image">Computer Vision REST API - Analyze Image</h3>
<p>The Analyze Image API is the primary Computer Vision operation, providing multiple analysis features in one call.</p>
<p><strong>REST API Structure:</strong></p>
<pre><code class="language-bash">POST https://{endpoint}/vision/v3.2/analyze?visualFeatures={features}&amp;details={details}&amp;language={lang}
Content-Type: application/json
Ocp-Apim-Subscription-Key: {your-key}

# For images accessible via URL:
{
  &quot;url&quot;: &quot;https://example.com/image.jpg&quot;
}

# For binary image data:
Content-Type: application/octet-stream
[Binary image data in request body]</code></pre>
<p><strong>Visual Features Parameter</strong> (what to analyze):</p>
<ul>
<li><code>Categories</code> - Classify image into predefined taxonomy</li>
<li><code>Tags</code> - Generate descriptive tags</li>
<li><code>Description</code> - Create natural language caption</li>
<li><code>Faces</code> - Detect and locate faces</li>
<li><code>Objects</code> - Detect and locate objects with bounding boxes</li>
<li><code>Brands</code> - Identify commercial brands/logos</li>
<li><code>Color</code> - Analyze color scheme</li>
<li><code>ImageType</code> - Determine if clipart, line drawing, etc.</li>
<li><code>Adult</code> - Detect adult, racy, or gory content</li>
</ul>
<p><strong>Details Parameter</strong> (additional analysis):</p>
<ul>
<li><code>Celebrities</code> - Identify known public figures</li>
<li><code>Landmarks</code> - Identify famous landmarks</li>
</ul>
<p><strong>Complete Example:</strong></p>
<pre><code class="language-bash">POST https://mycomputervision.cognitiveservices.azure.com/vision/v3.2/analyze?visualFeatures=Objects,Tags,Description,Faces&amp;details=Landmarks&amp;language=en
Ocp-Apim-Subscription-Key: abc123xyz789
Content-Type: application/json

{
  &quot;url&quot;: &quot;https://upload.wikimedia.org/wikipedia/commons/thumb/4/47/New_york_times_square-terabass.jpg/1200px-New_york_times_square-terabass.jpg&quot;
}</code></pre>
<p><strong>Response Example</strong> (simplified):</p>
<pre><code class="language-json">{
  &quot;categories&quot;: [
    {
      &quot;name&quot;: &quot;outdoor_street&quot;,
      &quot;score&quot;: 0.98
    }
  ],
  &quot;tags&quot;: [
    {&quot;name&quot;: &quot;building&quot;, &quot;confidence&quot;: 0.99},
    {&quot;name&quot;: &quot;city&quot;, &quot;confidence&quot;: 0.97},
    {&quot;name&quot;: &quot;street&quot;, &quot;confidence&quot;: 0.95},
    {&quot;name&quot;: &quot;outdoor&quot;, &quot;confidence&quot;: 0.94}
  ],
  &quot;description&quot;: {
    &quot;tags&quot;: [&quot;building&quot;, &quot;outdoor&quot;, &quot;street&quot;, &quot;city&quot;],
    &quot;captions&quot;: [
      {
        &quot;text&quot;: &quot;a busy city street filled with tall buildings&quot;,
        &quot;confidence&quot;: 0.89
      }
    ]
  },
  &quot;objects&quot;: [
    {
      &quot;rectangle&quot;: {&quot;x&quot;: 120, &quot;y&quot;: 80, &quot;w&quot;: 200, &quot;h&quot;: 350},
      &quot;object&quot;: &quot;building&quot;,
      &quot;confidence&quot;: 0.92
    },
    {
      &quot;rectangle&quot;: {&quot;x&quot;: 450, &quot;y&quot;: 300, &quot;w&quot;: 80, &quot;h&quot;: 120},
      &quot;object&quot;: &quot;car&quot;,
      &quot;confidence&quot;: 0.88
    }
  ],
  &quot;faces&quot;: [
    {
      &quot;age&quot;: 32,
      &quot;gender&quot;: &quot;Male&quot;,
      &quot;faceRectangle&quot;: {&quot;left&quot;: 340, &quot;top&quot;: 220, &quot;width&quot;: 55, &quot;height&quot;: 55}
    }
  ],
  &quot;landmarks&quot;: [
    {
      &quot;name&quot;: &quot;Times Square&quot;,
      &quot;confidence&quot;: 0.95
    }
  ],
  &quot;metadata&quot;: {
    &quot;width&quot;: 1200,
    &quot;height&quot;: 800,
    &quot;format&quot;: &quot;Jpeg&quot;
  }
}</code></pre>
<h3 id="python-sdk-image-analysis">Python SDK - Image Analysis</h3>
<pre><code class="language-python">from azure.ai.vision.imageanalysis import ImageAnalysisClient
from azure.ai.vision.imageanalysis.models import VisualFeatures
from azure.core.credentials import AzureKeyCredential

# Initialize client
endpoint = &quot;https://mycomputervision.cognitiveservices.azure.com/&quot;
key = &quot;your-api-key&quot;
client = ImageAnalysisClient(endpoint, AzureKeyCredential(key))

# Analyze image from URL
image_url = &quot;https://example.com/photo.jpg&quot;

result = client.analyze_from_url(
    image_url=image_url,
    visual_features=[
        VisualFeatures.CAPTION,      # Generate description
        VisualFeatures.READ,          # Extract text (OCR)
        VisualFeatures.TAGS,          # Generate tags
        VisualFeatures.OBJECTS,       # Detect objects
        VisualFeatures.PEOPLE         # Detect people
    ],
    language=&quot;en&quot;
)

# Process caption
if result.caption:
    print(f&quot;Image description: {result.caption.text}&quot;)
    print(f&quot;Confidence: {result.caption.confidence:.2%}&quot;)

# Process tags
if result.tags:
    print(&quot;\nTags found:&quot;)
    for tag in result.tags.list:
        print(f&quot;  - {tag.name} (confidence: {tag.confidence:.2%})&quot;)

# Process objects
if result.objects:
    print(&quot;\nObjects detected:&quot;)
    for obj in result.objects.list:
        bbox = obj.bounding_box
        print(f&quot;  - {obj.tags[0].name} at ({bbox.x}, {bbox.y}), size {bbox.width}x{bbox.height}&quot;)
        print(f&quot;    Confidence: {obj.tags[0].confidence:.2%}&quot;)

# Process OCR text
if result.read:
    print(&quot;\nText found in image:&quot;)
    for block in result.read.blocks:
        for line in block.lines:
            print(f&quot;  Line: &#x27;{line.text}&#x27;&quot;)
            # Each line contains individual words with bounding boxes
            for word in line.words:
                print(f&quot;    Word: &#x27;{word.text}&#x27; (confidence: {word.confidence:.2%})&quot;)</code></pre>
<p><strong>Analyzing Local Images:</strong></p>
<pre><code class="language-python"># Read image from file
with open(&quot;local_image.jpg&quot;, &quot;rb&quot;) as f:
    image_data = f.read()

# Analyze binary image data
result = client.analyze(
    image_data=image_data,
    visual_features=[VisualFeatures.CAPTION, VisualFeatures.TAGS]
)

print(f&quot;Local image analysis: {result.caption.text}&quot;)</code></pre>
<h3 id="adult-content-detection-important-use-case">Adult Content Detection - Important Use Case</h3>
<p>Detecting adult, racy, or gory content is critical for user-generated content platforms.</p>
<p><strong>Content Categories:</strong></p>
<ul>
<li><strong>Adult:</strong> Explicitly sexual or pornographic content</li>
<li><strong>Racy:</strong> Sexually suggestive but not explicitly pornographic</li>
<li><strong>Gory:</strong> Graphic violence, blood, gore</li>
</ul>
<pre><code class="language-python"># Analyze for adult content
result = client.analyze_from_url(
    image_url=&quot;https://example.com/user_upload.jpg&quot;,
    visual_features=[VisualFeatures.CAPTION],  # Still analyze normally
    # Adult analysis is always included, no need to specify
)

# Check adult content flags
if hasattr(result, &#x27;adult&#x27;):
    print(f&quot;Adult content: {result.adult.is_adult_content}&quot;)
    print(f&quot;Adult score: {result.adult.adult_score:.2f}&quot;)
    print(f&quot;Racy content: {result.adult.is_racy_content}&quot;)
    print(f&quot;Racy score: {result.adult.racy_score:.2f}&quot;)
    print(f&quot;Gory content: {result.adult.is_gory_content}&quot;)
    print(f&quot;Gory score: {result.adult.gore_score:.2f}&quot;)
    
    # Apply moderation logic
    if result.adult.is_adult_content or result.adult.adult_score &gt; 0.8:
        print(&quot;BLOCKED: Explicit content detected&quot;)
    elif result.adult.is_racy_content and result.adult.racy_score &gt; 0.6:
        print(&quot;FLAGGED: Suggestive content for review&quot;)</code></pre>
<section id="32-optical-character-recognition-ocr-read-api"><h2>3.2 Optical Character Recognition (OCR) - Read API</h2>
<p>OCR extracts text from images and documents. Computer Vision's Read API is optimized for document processing and handles both printed and handwritten text.</p>
<h3 id="read-api-vs-legacy-ocr-api">Read API vs Legacy OCR API</h3>
<p><strong>Read API</strong> (Current, Recommended):</p>
<ul>
<li>Handles large documents (multi-page PDFs)</li>
<li>Better accuracy on handwriting</li>
<li>Supports more languages</li>
<li>Async processing for large documents</li>
<li>Returns text in reading order</li>
</ul>
<p><strong>Legacy OCR API</strong> (Deprecated):</p>
<ul>
<li>Synchronous only</li>
<li>Limited document size</li>
<li>Lower accuracy</li>
<li>Being phased out</li>
</ul>
<div class="tip-box"><p><strong>Exam Tip:</strong> Always use Read API for OCR scenarios. Legacy OCR is deprecated.</p></div>
<h3 id="read-api-complete-workflow">Read API - Complete Workflow</h3>
<p>The Read API is asynchronous: submit document, receive operation ID, poll for results.</p>
<p><strong>Step 1: Submit Document for Reading:</strong></p>
<pre><code class="language-bash">POST https://{endpoint}/vision/v3.2/read/analyze?language=en&amp;pages=1-3
Ocp-Apim-Subscription-Key: {your-key}
Content-Type: application/json

{
  &quot;url&quot;: &quot;https://example.com/document.pdf&quot;
}

# Response: 
HTTP/1.1 202 Accepted
Operation-Location: https://{endpoint}/vision/v3.2/read/analyzeResults/{operationId}</code></pre>
<p><strong>Step 2: Poll for Results:</strong></p>
<pre><code class="language-bash">GET https://{endpoint}/vision/v3.2/read/analyzeResults/{operationId}
Ocp-Apim-Subscription-Key: {your-key}

# Response when complete:
{
  &quot;status&quot;: &quot;succeeded&quot;,
  &quot;createdDateTime&quot;: &quot;2024-01-15T10:30:00Z&quot;,
  &quot;lastUpdatedDateTime&quot;: &quot;2024-01-15T10:30:05Z&quot;,
  &quot;analyzeResult&quot;: {
    &quot;version&quot;: &quot;3.2&quot;,
    &quot;readResults&quot;: [
      {
        &quot;page&quot;: 1,
        &quot;angle&quot;: 0.5,
        &quot;width&quot;: 8.5,
        &quot;height&quot;: 11,
        &quot;unit&quot;: &quot;inch&quot;,
        &quot;lines&quot;: [
          {
            &quot;boundingBox&quot;: [120, 80, 450, 80, 450, 120, 120, 120],
            &quot;text&quot;: &quot;Invoice #12345&quot;,
            &quot;words&quot;: [
              {&quot;boundingBox&quot;: [120, 80, 250, 80, 250, 120, 120, 120], &quot;text&quot;: &quot;Invoice&quot;, &quot;confidence&quot;: 0.98},
              {&quot;boundingBox&quot;: [260, 80, 450, 80, 450, 120, 260, 120], &quot;text&quot;: &quot;#12345&quot;, &quot;confidence&quot;: 0.99}
            ]
          }
        ]
      }
    ]
  }
}</code></pre>
<h3 id="python-sdk-read-api-implementation">Python SDK - Read API Implementation</h3>
<pre><code class="language-python">from azure.ai.vision.imageanalysis import ImageAnalysisClient
from azure.ai.vision.imageanalysis.models import VisualFeatures
import time

# For Image Analysis 4.0 (newer SDK):
result = client.analyze_from_url(
    image_url=&quot;https://example.com/document.jpg&quot;,
    visual_features=[VisualFeatures.READ]
)

# Extract all text
if result.read:
    full_text = []
    for block in result.read.blocks:
        for line in block.lines:
            full_text.append(line.text)
    
    # Join all lines
    document_text = &quot;\n&quot;.join(full_text)
    print(&quot;Extracted Text:&quot;)
    print(document_text)
    
    # Get structured data with positions
    for block in result.read.blocks:
        for line in block.lines:
            print(f&quot;\nLine at ({line.bounding_polygon}): &#x27;{line.text}&#x27;&quot;)
            for word in line.words:
                print(f&quot;  Word: &#x27;{word.text}&#x27; confidence: {word.confidence:.2%}&quot;)</code></pre>
<p><strong>Using Legacy Computer Vision SDK</strong> (for reference, if exam shows older code):</p>
<pre><code class="language-python">from azure.cognitiveservices.vision.computervision import ComputerVisionClient
from msrest.authentication import CognitiveServicesCredentials
import time

# Initialize legacy client
credentials = CognitiveServicesCredentials(key)
client = ComputerVisionClient(endpoint, credentials)

# Submit read operation
read_response = client.read(
    url=&quot;https://example.com/document.pdf&quot;,
    language=&quot;en&quot;,
    raw=True
)

# Extract operation ID from response headers
operation_location = read_response.headers[&quot;Operation-Location&quot;]
operation_id = operation_location.split(&quot;/&quot;)[-1]

# Poll for completion
while True:
    result = client.get_read_result(operation_id)
    
    if result.status not in [&#x27;notStarted&#x27;, &#x27;running&#x27;]:
        break
    
    print(&quot;Processing...&quot;)
    time.sleep(2)

# Extract text when complete
if result.status == &#x27;succeeded&#x27;:
    for page in result.analyze_result.read_results:
        print(f&quot;\n--- Page {page.page} ---&quot;)
        for line in page.lines:
            print(line.text)
else:
    print(f&quot;OCR failed: {result.status}&quot;)</code></pre>
<h3 id="ocr-best-practices-and-optimization">OCR Best Practices and Optimization</h3>
<p><strong>Image Quality Recommendations:</strong></p>
<ul>
<li>Minimum resolution: 50x50 pixels</li>
<li>Maximum size: 50 MB</li>
<li>Supported formats: JPEG, PNG, BMP, PDF (up to 2000 pages)</li>
<li>Clear, high-contrast text works best</li>
<li>Avoid heavy compression that creates artifacts</li>
</ul>
<p><strong>Performance Optimization:</strong></p>
<pre><code class="language-python"># For large documents, specify pages to process
result = client.read(
    url=&quot;https://example.com/200-page-document.pdf&quot;,
    pages=&quot;1-10,50,100-110&quot;,  # Only process specific pages
    language=&quot;en&quot;
)

# This reduces processing time and cost when you don&#x27;t need all pages</code></pre>
<p><strong>Language Support:</strong></p>
<ul>
<li>Specify language when known for better accuracy</li>
<li>Read API auto-detects if not specified</li>
<li>Supports 100+ languages including handwriting in English, Chinese, Japanese</li>
</ul>
<div class="tip-box"><p><strong>Exam Scenario:</strong> <em>"You need to extract text from scanned medical records containing both printed and handwritten notes. Documents are multi-page PDFs. What service should you use?"</em></p></div>
<p><strong>Answer:</strong> Computer Vision Read API. It handles multi-page PDFs, supports both printed and handwritten text, and provides asynchronous processing suitable for large documents.</p>
<section id="ai-102-ultimate-study-guide-part-2"><h1>AI-102 Ultimate Study Guide - Part 2</h1>
<section id="sections-3-6-vision-nlp-knowledge-mining-and-generative-ai"><h2>Sections 3-6: Vision, NLP, Knowledge Mining, and Generative AI</h2>
<p><strong>Use this guide in combination with Part 1 (AI-102_Ultimate_Study_Guide.md)</strong></p>
<p>This document completes your comprehensive exam preparation by covering the remaining sections with the same depth as Part 1. The sections here represent approximately 70% of your exam content, with NLP alone being 30-35% of all questions.</p>
<hr>
<section id="section-3-continued-vision-solutions"><h1>SECTION 3 CONTINUED: VISION SOLUTIONS</h1>
<section id="33-custom-vision-when-general-models-arent-enough"><h2>3.3 Custom Vision - When General Models Aren't Enough</h2>
<p>Custom Vision enables training image classification and object detection models on your own labeled data. Use this when Computer Vision's general models don't recognize your specific visual categories.</p>
<h3 id="classification-vs-object-detection">Classification vs Object Detection</h3>
<p><strong>Image Classification:</strong> Assigns labels to entire images</p>
<ul>
<li>"This image contains a dog"</li>
<li>"This X-ray shows pneumonia"  </li>
<li>"This product is defective"</li>
<li>Returns: Label + confidence score</li>
</ul>
<p><strong>Object Detection:</strong> Identifies objects AND their locations with bounding boxes</p>
<ul>
<li>"There is a dog at coordinates (120,80) and a cat at (450,200)"</li>
<li>"Two tumors detected at these locations in the scan"</li>
<li>"Three defects found at these positions on the product"</li>
<li>Returns: Label + confidence + bounding box coordinates</li>
</ul>
<p><strong>Decision Framework:</strong></p>
<ul>
<li>Need to know IF something is in image ‚Üí Classification</li>
<li>Need to know WHERE things are in image ‚Üí Object Detection</li>
<li>Need to count multiple instances ‚Üí Object Detection</li>
</ul>
<h3 id="training-custom-vision-models-complete-workflow">Training Custom Vision Models - Complete Workflow</h3>
<p><strong>Prerequisites:</strong></p>
<ul>
<li>Minimum 30-50 images per category for classification</li>
<li>Minimum 50-100 images per class for object detection</li>
<li>Images should show variety (angles, lighting, backgrounds)</li>
<li>More images = better accuracy</li>
</ul>
<p><strong>REST API - Create Project:</strong></p>
<pre><code class="language-bash">POST https://{endpoint}/customvision/v3.3/Training/projects
Training-Key: {your-training-key}
Content-Type: application/json

{
  &quot;name&quot;: &quot;Product Defect Detector&quot;,
  &quot;description&quot;: &quot;Detects manufacturing defects&quot;,
  &quot;classificationType&quot;: &quot;Multiclass&quot;,  # or &quot;Multilabel&quot;
  &quot;domainId&quot;: &quot;{domain-id-for-general-or-specific-domain}&quot;
}</code></pre>
<p><strong>Python SDK - Complete Training Workflow:</strong></p>
<pre><code class="language-python">from azure.cognitiveservices.vision.customvision.training import CustomVisionTrainingClient
from azure.cognitiveservices.vision.customvision.training.models import ImageFileCreateBatch, ImageFileCreateEntry, Region
from msrest.authentication import ApiKeyCredentials
import time

# Initialize training client
training_endpoint = &quot;https://{resource}.cognitiveservices.azure.com/&quot;
training_key = &quot;your-training-key&quot;
credentials = ApiKeyCredentials(in_headers={&quot;Training-key&quot;: training_key})
trainer = CustomVisionTrainingClient(training_endpoint, credentials)

# Step 1: Create project
project = trainer.create_project(
    name=&quot;Manufacturing Defect Detector&quot;,
    classification_type=&quot;Multiclass&quot;  # One label per image
)

# Step 2: Create tags (categories)
scratch_tag = trainer.create_tag(project.id, &quot;Scratch&quot;)
dent_tag = trainer.create_tag(project.id, &quot;Dent&quot;)
good_tag = trainer.create_tag(project.id, &quot;Good&quot;)

# Step 3: Upload and tag images
print(&quot;Uploading images...&quot;)
for i in range(50):  # Upload 50 images per category
    with open(f&quot;training_data/scratches/image_{i}.jpg&quot;, &quot;rb&quot;) as f:
        trainer.create_images_from_data(project.id, f.read(), [scratch_tag.id])
    
    with open(f&quot;training_data/dents/image_{i}.jpg&quot;, &quot;rb&quot;) as f:
        trainer.create_images_from_data(project.id, f.read(), [dent_tag.id])
    
    with open(f&quot;training_data/good/image_{i}.jpg&quot;, &quot;rb&quot;) as f:
        trainer.create_images_from_data(project.id, f.read(), [good_tag.id])

# Step 4: Train model
print(&quot;Training model...&quot;)
iteration = trainer.train_project(project.id)

# Step 5: Wait for training to complete
while iteration.status != &quot;Completed&quot;:
    iteration = trainer.get_iteration(project.id, iteration.id)
    print(f&quot;Training status: {iteration.status}&quot;)
    time.sleep(5)

print(f&quot;Training completed! Accuracy: {iteration.metrics.precision:.2%}&quot;)

# Step 6: Publish iteration to prediction endpoint
prediction_resource_id = &quot;/subscriptions/{sub}/resourceGroups/{rg}/providers/Microsoft.CognitiveServices/accounts/{prediction-resource}&quot;
publish_name = &quot;ProductionModel_v1&quot;

trainer.publish_iteration(
    project.id,
    iteration.id,
    publish_name,
    prediction_resource_id
)

print(f&quot;Model published as &#x27;{publish_name}&#x27;&quot;)</code></pre>
<h3 id="using-custom-vision-for-predictions">Using Custom Vision for Predictions</h3>
<pre><code class="language-python">from azure.cognitiveservices.vision.customvision.prediction import CustomVisionPredictionClient

# Initialize prediction client  
prediction_endpoint = &quot;https://{prediction-resource}.cognitiveservices.azure.com/&quot;
prediction_key = &quot;your-prediction-key&quot;
pred_credentials = ApiKeyCredentials(in_headers={&quot;Prediction-key&quot;: prediction_key})
predictor = CustomVisionPredictionClient(prediction_endpoint, pred_credentials)

# Predict from URL
results = predictor.classify_image_url(
    project_id=project.id,
    published_name=&quot;ProductionModel_v1&quot;,
    url=&quot;https://example.com/test-product.jpg&quot;
)

# Process results
print(&quot;\nPrediction Results:&quot;)
for prediction in results.predictions:
    if prediction.probability &gt; 0.7:  # 70% confidence threshold
        print(f&quot;  {prediction.tag_name}: {prediction.probability:.2%}&quot;)

# Predict from local file
with open(&quot;local-product.jpg&quot;, &quot;rb&quot;) as image:
    results = predictor.classify_image(
        project_id=project.id,
        published_name=&quot;ProductionModel_v1&quot;,
        image_data=image
    )</code></pre>
<h3 id="object-detection-with-custom-vision">Object Detection with Custom Vision</h3>
<p>For object detection, you need to provide bounding boxes during training:</p>
<pre><code class="language-python"># Upload image with regions (bounding boxes)
with open(&quot;image_with_defects.jpg&quot;, &quot;rb&quot;) as image:
    # Define regions (bounding boxes) for objects in image
    # Coordinates are normalized (0.0 to 1.0)
    regions = [
        Region(
            tag_id=scratch_tag.id,
            left=0.1,    # 10% from left edge
            top=0.2,     # 20% from top
            width=0.15,  # 15% of image width
            height=0.1   # 10% of image height  
        ),
        Region(
            tag_id=dent_tag.id,
            left=0.6,
            top=0.5,
            width=0.2,
            height=0.15
        )
    ]
    
    trainer.create_images_from_data(
        project.id,
        image.read(),
        regions=regions
    )

# After training, use detect_image instead of classify_image
results = predictor.detect_image_url(
    project_id=project.id,
    published_name=&quot;DefectDetectorModel&quot;,
    url=&quot;https://example.com/test-image.jpg&quot;
)

# Process detections
for detection in results.predictions:
    if detection.probability &gt; 0.5:
        print(f&quot;Found {detection.tag_name}:&quot;)
        print(f&quot;  Confidence: {detection.probability:.2%}&quot;)
        print(f&quot;  Location: ({detection.bounding_box.left}, {detection.bounding_box.top})&quot;)
        print(f&quot;  Size: {detection.bounding_box.width} x {detection.bounding_box.height}&quot;)</code></pre>
<h3 id="improving-model-accuracy">Improving Model Accuracy</h3>
<p><strong>Iterative Improvement Process:</strong></p>
<ol>
<li>Train initial model with minimum images (30-50 per category)</li>
<li>Test on new images</li>
<li>Find images where model makes mistakes</li>
<li>Add those images to training set with correct labels</li>
<li>Retrain model</li>
<li>Repeat until accuracy is acceptable</li>
</ol>
<pre><code class="language-python"># Example: Adding feedback to improve model
# User reports model incorrectly classified a scratch as good

with open(&quot;incorrectly_classified.jpg&quot;, &quot;rb&quot;) as f:
    # Add to training set with CORRECT label
    trainer.create_images_from_data(
        project.id,
        f.read(),
        [scratch_tag.id]  # Correct label (was misclassified as &quot;good&quot;)
    )

# Retrain with updated training set
new_iteration = trainer.train_project(project.id)

# Wait for training and publish updated model
# (same process as initial training)</code></pre>
<div class="tip-box"><p><strong>Exam Tip:</strong> Custom Vision requires iterative refinement. Initial accuracy might be 70-80%, improving to 90%+ with more diverse training images and feedback from misclassifications.</p></div>
<hr>
<section id="34-face-api-identity-verification-critical-limited-access-required"><h2>3.4 Face API - Identity Verification (CRITICAL: Limited Access Required)</h2>
<div class="tip-box"><p><strong>IMPORTANT EXAM KNOWLEDGE:</strong> Face API's identification and verification features require Microsoft's Limited Access approval. You must apply and demonstrate legitimate use case. Face detection features are available without approval.</p></div>
<h3 id="face-detection-vs-verification-vs-identification">Face Detection vs Verification vs Identification</h3>
<p><strong>Face Detection:</strong> Locate faces in images, extract attributes</p>
<ul>
<li>No approval needed</li>
<li>Returns: face location, age estimate, gender, emotions, glasses, etc.</li>
<li>Use for: counting people, analyzing demographics, detecting face presence</li>
</ul>
<p><strong>Face Verification:</strong> Compare two faces - are they the same person?</p>
<ul>
<li>Requires Limited Access approval</li>
<li>Returns: confidence score that faces match</li>
<li>Use for: identity verification (login, access control)</li>
</ul>
<p><strong>Face Identification:</strong> Who is this person from a known group?</p>
<ul>
<li>Requires Limited Access approval  </li>
<li>Returns: person ID and confidence from pre-enrolled group</li>
<li>Use for: identifying individuals, attendance systems</li>
</ul>
<h3 id="rest-api-face-detection">REST API - Face Detection</h3>
<pre><code class="language-bash">POST https://{endpoint}/face/v1.0/detect?returnFaceId=true&amp;returnFaceLandmarks=false&amp;returnFaceAttributes=age,gender,emotion,glasses
Content-Type: application/json
Ocp-Apim-Subscription-Key: {your-key}

{
  &quot;url&quot;: &quot;https://example.com/photo.jpg&quot;
}</code></pre>
<p><strong>Response:</strong></p>
<pre><code class="language-json">[
  {
    &quot;faceId&quot;: &quot;abc-123-def-456&quot;,
    &quot;faceRectangle&quot;: {
      &quot;top&quot;: 120,
      &quot;left&quot;: 250,
      &quot;width&quot;: 180,
      &quot;height&quot;: 180
    },
    &quot;faceAttributes&quot;: {
      &quot;age&quot;: 32.5,
      &quot;gender&quot;: &quot;male&quot;,
      &quot;emotion&quot;: {
        &quot;happiness&quot;: 0.85,
        &quot;neutral&quot;: 0.10,
        &quot;surprise&quot;: 0.02,
        &quot;sadness&quot;: 0.01,
        &quot;anger&quot;: 0.01,
        &quot;contempt&quot;: 0.01,
        &quot;disgust&quot;: 0.00,
        &quot;fear&quot;: 0.00
      },
      &quot;glasses&quot;: &quot;ReadingGlasses&quot;
    }
  }
]</code></pre>
<h3 id="python-sdk-face-detection">Python SDK - Face Detection</h3>
<pre><code class="language-python">from azure.cognitiveservices.vision.face import FaceClient
from msrest.authentication import CognitiveServicesCredentials
from azure.cognitiveservices.vision.face.models import FaceAttributeType

endpoint = &quot;https://your-face-resource.cognitiveservices.azure.com/&quot;
key = &quot;your-api-key&quot;

client = FaceClient(endpoint, CognitiveServicesCredentials(key))

# Detect faces with attributes
face_attributes = [
    FaceAttributeType.age,
    FaceAttributeType.gender,
    FaceAttributeType.emotion,
    FaceAttributeType.glasses,
    FaceAttributeType.hair,
    FaceAttributeType.facial_hair
]

detected_faces = client.face.detect_with_url(
    url=&quot;https://example.com/group-photo.jpg&quot;,
    return_face_attributes=face_attributes
)

print(f&quot;Detected {len(detected_faces)} faces&quot;)

for idx, face in enumerate(detected_faces):
    print(f&quot;\nFace {idx + 1}:&quot;)
    print(f&quot;  Location: ({face.face_rectangle.left}, {face.face_rectangle.top})&quot;)
    print(f&quot;  Size: {face.face_rectangle.width} x {face.face_rectangle.height}&quot;)
    print(f&quot;  Age: ~{face.face_attributes.age} years&quot;)
    print(f&quot;  Gender: {face.face_attributes.gender}&quot;)
    
    # Find dominant emotion
    emotions = face.face_attributes.emotion
    dominant_emotion = max(
        emotions.__dict__.items(),
        key=lambda x: x[1] if isinstance(x[1], float) else 0
    )
    print(f&quot;  Emotion: {dominant_emotion[0]} ({dominant_emotion[1]:.2%})&quot;)</code></pre>
<h3 id="face-verification-requires-limited-access">Face Verification (Requires Limited Access)</h3>
<pre><code class="language-python"># Detect faces in two images
face1 = client.face.detect_with_url(url=&quot;person_photo1.jpg&quot;)[0]
face2 = client.face.detect_with_url(url=&quot;person_photo2.jpg&quot;)[0]

# Verify if same person
verify_result = client.face.verify_face_to_face(
    face1.face_id,
    face2.face_id
)

print(f&quot;Same person: {verify_result.is_identical}&quot;)
print(f&quot;Confidence: {verify_result.confidence:.2%}&quot;)

# Typical threshold: 0.7+ confidence = same person
if verify_result.is_identical and verify_result.confidence &gt; 0.7:
    print(&quot;VERIFIED: High confidence match&quot;)
elif verify_result.confidence &gt; 0.5:
    print(&quot;POSSIBLE MATCH: Review needed&quot;)
else:
    print(&quot;NOT A MATCH: Different people&quot;)</code></pre>
<div class="tip-box"><p><strong>Exam Scenario:</strong> <em>"Your company needs to verify employee identity when accessing secure facilities by comparing their face to their employee ID photo. What Azure service should you use?"</em></p></div>
<p><strong>Answer:</strong> Face API with face verification. This is a 1:1 comparison (employee's live photo vs stored ID photo) requiring face verification capability. Note: Requires Limited Access approval from Microsoft for production use.</p>
<hr>
<section id="section-4-natural-language-processing-30-35"><h1>SECTION 4: NATURAL LANGUAGE PROCESSING (30-35%)</h1>
<div class="warning-box"><p><strong>CRITICAL</strong> This is the HIGHEST WEIGHTED section of the exam. Expect 18-21 questions on NLP services. Master this section thoroughly.</p></div>
<section id="41-azure-ai-language-text-analytics-comprehensive-coverage"><h2>4.1 Azure AI Language (Text Analytics) - Comprehensive Coverage</h2>
<p>Text Analytics analyzes existing text for insights. Unlike CLU/LUIS which understand conversational intent, Text Analytics extracts information from any text.</p>
<h3 id="core-capabilities-decision-matrix">Core Capabilities Decision Matrix</h3>
<div class="table-wrapper">
<table>
<thead>
<tr>
<th>Capability</th>
<th>What It Does</th>
<th>When to Use</th>
<th>Input</th>
<th>Output</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Sentiment Analysis</strong></td>
<td>Determines if text is positive, negative, or neutral</td>
<td>Analyzing reviews, feedback, social media</td>
<td>Text documents</td>
<td>Overall sentiment + confidence scores</td>
</tr>
<tr>
<td><strong>Opinion Mining</strong></td>
<td>Extracts what aspects are viewed positively/negatively</td>
<td>Understanding specific feedback targets</td>
<td>Text with opinions</td>
<td>Targets + assessments with sentiment</td>
</tr>
<tr>
<td><strong>Key Phrase Extraction</strong></td>
<td>Identifies main talking points</td>
<td>Summarizing content, finding topics</td>
<td>Any text</td>
<td>List of key phrases</td>
</tr>
<tr>
<td><strong>Entity Recognition</strong></td>
<td>Finds and categorizes entities (people, places, organizations, dates, etc.)</td>
<td>Extracting structured data from unstructured text</td>
<td>Text with entities</td>
<td>Entities with categories and confidence</td>
</tr>
<tr>
<td><strong>Entity Linking</strong></td>
<td>Connects entities to knowledge bases (Wikipedia)</td>
<td>Disambiguating entities, enriching with external knowledge</td>
<td>Text with well-known entities</td>
<td>Entities with Wikipedia links</td>
</tr>
<tr>
<td><strong>Language Detection</strong></td>
<td>Identifies what language text is written in</td>
<td>Routing multilingual content, preprocessing for translation</td>
<td>Text in any of 120+ languages</td>
<td>Language code + confidence</td>
</tr>
<tr>
<td><strong>PII Detection</strong></td>
<td>Finds personally identifiable information</td>
<td>Data privacy, anonymization</td>
<td>Text with potential PII</td>
<td>PII entities + redacted text</td>
</tr>
</tbody>
</table>
</div>
<h3 id="rest-api-sentiment-analysis">REST API - Sentiment Analysis</h3>
<pre><code class="language-bash">POST https://{endpoint}/text/analytics/v3.1/sentiment
Content-Type: application/json
Ocp-Apim-Subscription-Key: {your-key}

{
  &quot;documents&quot;: [
    {
      &quot;id&quot;: &quot;1&quot;,
      &quot;language&quot;: &quot;en&quot;,
      &quot;text&quot;: &quot;I love this product! The customer service was amazing. However, the delivery took too long.&quot;
    },
    {
      &quot;id&quot;: &quot;2&quot;,
      &quot;language&quot;: &quot;en&quot;,
      &quot;text&quot;: &quot;Terrible experience. Would not recommend.&quot;
    }
  ]
}</code></pre>
<p><strong>Response with Opinion Mining:</strong></p>
<pre><code class="language-json">{
  &quot;documents&quot;: [
    {
      &quot;id&quot;: &quot;1&quot;,
      &quot;sentiment&quot;: &quot;mixed&quot;,
      &quot;confidenceScores&quot;: {
        &quot;positive&quot;: 0.65,
        &quot;neutral&quot;: 0.10,
        &quot;negative&quot;: 0.25
      },
      &quot;sentences&quot;: [
        {
          &quot;text&quot;: &quot;I love this product!&quot;,
          &quot;sentiment&quot;: &quot;positive&quot;,
          &quot;confidenceScores&quot;: {&quot;positive&quot;: 0.99, &quot;neutral&quot;: 0.00, &quot;negative&quot;: 0.01},
          &quot;minedOpinions&quot;: [
            {
              &quot;target&quot;: {
                &quot;text&quot;: &quot;product&quot;,
                &quot;sentiment&quot;: &quot;positive&quot;
              },
              &quot;assessments&quot;: [
                {
                  &quot;text&quot;: &quot;love&quot;,
                  &quot;sentiment&quot;: &quot;positive&quot;
                }
              ]
            }
          ]
        },
        {
          &quot;text&quot;: &quot;The customer service was amazing.&quot;,
          &quot;sentiment&quot;: &quot;positive&quot;
        },
        {
          &quot;text&quot;: &quot;However, the delivery took too long.&quot;,
          &quot;sentiment&quot;: &quot;negative&quot;,
          &quot;minedOpinions&quot;: [
            {
              &quot;target&quot;: {
                &quot;text&quot;: &quot;delivery&quot;,
                &quot;sentiment&quot;: &quot;negative&quot;
              },
              &quot;assessments&quot;: [
                {
                  &quot;text&quot;: &quot;took too long&quot;,
                  &quot;sentiment&quot;: &quot;negative&quot;
                }
              ]
            }
          ]
        }
      ]
    }
  ]
}</code></pre>
<h3 id="python-sdk-complete-text-analytics-examples">Python SDK - Complete Text Analytics Examples</h3>
<pre><code class="language-python">from azure.ai.textanalytics import TextAnalyticsClient
from azure.core.credentials import AzureKeyCredential

endpoint = &quot;https://your-resource.cognitiveservices.azure.com/&quot;
key = &quot;your-api-key&quot;

client = TextAnalyticsClient(endpoint, AzureKeyCredential(key))

# Sentiment Analysis with Opinion Mining
documents = [
    &quot;The hotel room was spacious and clean, but the staff was rude.&quot;,
    &quot;Great food but terrible service!&quot;,
    &quot;Perfect experience all around. Highly recommend!&quot;
]

result = client.analyze_sentiment(
    documents,
    show_opinion_mining=True  # Enable detailed opinion analysis
)

for idx, doc in enumerate(result):
    print(f&quot;\nDocument {idx}:&quot;)
    print(f&quot;Overall sentiment: {doc.sentiment}&quot;)
    print(f&quot;Scores - Positive: {doc.confidence_scores.positive:.2%}, &quot;
          f&quot;Neutral: {doc.confidence_scores.neutral:.2%}, &quot;
          f&quot;Negative: {doc.confidence_scores.negative:.2%}&quot;)
    
    # Show sentence-level sentiment
    for sentence in doc.sentences:
        print(f&quot;\n  Sentence: &#x27;{sentence.text}&#x27;&quot;)
        print(f&quot;  Sentiment: {sentence.sentiment}&quot;)
        
        # Show opinion mining results
        for opinion in sentence.mined_opinions:
            target = opinion.target
            print(f&quot;    Target: &#x27;{target.text}&#x27; ‚Üí {target.sentiment}&quot;)
            for assessment in opinion.assessments:
                print(f&quot;      Assessment: &#x27;{assessment.text}&#x27; ‚Üí {assessment.sentiment}&quot;)</code></pre>
<h3 id="key-phrase-extraction">Key Phrase Extraction</h3>
<pre><code class="language-python"># Extract key phrases
documents = [
    &quot;Microsoft was founded by Bill Gates and Paul Allen in 1975.&quot;,
    &quot;The company is headquartered in Redmond, Washington.&quot;,
    &quot;Azure is Microsoft&#x27;s cloud computing platform.&quot;
]

result = client.extract_key_phrases(documents)

for idx, doc in enumerate(result):
    print(f&quot;\nDocument {idx} key phrases:&quot;)
    for phrase in doc.key_phrases:
        print(f&quot;  - {phrase}&quot;)

# Output:
# Document 0 key phrases:
#   - Microsoft  
#   - Bill Gates
#   - Paul Allen
#   - 1975</code></pre>
<h3 id="named-entity-recognition-ner">Named Entity Recognition (NER)</h3>
<pre><code class="language-python"># Recognize entities
documents = [
    &quot;John Smith flew from Seattle to New York on January 15th.&quot;,
    &quot;The meeting is scheduled for next Tuesday at Microsoft headquarters.&quot;
]

result = client.recognize_entities(documents)

for idx, doc in enumerate(result):
    print(f&quot;\nDocument {idx} entities:&quot;)
    for entity in doc.entities:
        print(f&quot;  - &#x27;{entity.text}&#x27;: {entity.category} &quot;
              f&quot;(subcategory: {entity.subcategory}) &quot;
              f&quot;confidence: {entity.confidence_score:.2%}&quot;)

# Output example:
# Document 0 entities:
#   - &#x27;John Smith&#x27;: Person confidence: 99%
#   - &#x27;Seattle&#x27;: Location (GPE) confidence: 98%
#   - &#x27;New York&#x27;: Location (GPE) confidence: 99%
#   - &#x27;January 15th&#x27;: DateTime (Date) confidence: 95%</code></pre>
<h3 id="pii-detection-and-redaction">PII Detection and Redaction</h3>
<pre><code class="language-python"># Detect and redact PII
documents = [
    &quot;My email is john.smith@example.com and my phone is 555-1234.&quot;,
    &quot;SSN: 123-45-6789, Credit Card: 1234-5678-9012-3456&quot;
]

result = client.recognize_pii_entities(documents)

for idx, doc in enumerate(result):
    print(f&quot;\nDocument {idx}:&quot;)
    print(f&quot;Redacted text: {doc.redacted_text}&quot;)
    print(&quot;PII found:&quot;)
    for entity in doc.entities:
        print(f&quot;  - {entity.category}: &#x27;{entity.text}&#x27; &quot;
              f&quot;(confidence: {entity.confidence_score:.2%})&quot;)

# Output:
# Document 0:
# Redacted text: My email is ******************** and my phone is ********
# PII found:
#   - Email: &#x27;john.smith@example.com&#x27; (confidence: 99%)
#   - PhoneNumber: &#x27;555-1234&#x27; (confidence: 95%)</code></pre>
<section id="42-speech-service-speech-recognition-and-synthesis"><h2>4.2 Speech Service - Speech Recognition and Synthesis</h2>
<p>Speech services handle speech-to-text (recognition), text-to-speech (synthesis), and speech translation.</p>
<h3 id="speech-to-text-recognition-complete-implementation">Speech-to-Text (Recognition) - Complete Implementation</h3>
<p><strong>REST API - Not recommended for speech (use SDK):</strong> Speech services primarily use WebSocket connections for real-time streaming, making SDK the standard approach.</p>
<p><strong>Python SDK - Speech Recognition:</strong></p>
<pre><code class="language-python">import azure.cognitiveservices.speech as speechsdk

# Configuration
speech_key = &quot;your-speech-key&quot;
service_region = &quot;eastus&quot;

speech_config = speechsdk.SpeechConfig(
    subscription=speech_key,
    region=service_region
)

# Recognize from file
audio_config = speechsdk.AudioConfig(filename=&quot;audio.wav&quot;)
speech_recognizer = speechsdk.SpeechRecognizer(
    speech_config=speech_config,
    audio_config=audio_config
)

# One-time recognition (for short audio &lt; 15 seconds)
result = speech_recognizer.recognize_once()

if result.reason == speechsdk.ResultReason.RecognizedSpeech:
    print(f&quot;Recognized: {result.text}&quot;)
elif result.reason == speechsdk.ResultReason.NoMatch:
    print(&quot;No speech could be recognized&quot;)
elif result.reason == speechsdk.ResultReason.Canceled:
    cancellation = result.cancellation_details
    print(f&quot;Recognition canceled: {cancellation.reason}&quot;)
    if cancellation.reason == speechsdk.CancellationReason.Error:
        print(f&quot;Error: {cancellation.error_details}&quot;)</code></pre>
<p><strong>Continuous Recognition (for longer audio):</strong></p>
<pre><code class="language-python">import time

def recognized_cb(evt):
    &quot;&quot;&quot;Called when speech is recognized&quot;&quot;&quot;
    print(f&quot;Recognized: {evt.result.text}&quot;)

def stop_cb(evt):
    &quot;&quot;&quot;Called when recognition stops&quot;&quot;&quot;
    print(&quot;Recognition stopped&quot;)
    nonlocal done
    done = True

# Setup continuous recognition
speech_recognizer = speechsdk.SpeechRecognizer(
    speech_config=speech_config,
    audio_config=audio_config
)

# Connect callbacks
done = False
speech_recognizer.recognized.connect(recognized_cb)
speech_recognizer.session_stopped.connect(stop_cb)
speech_recognizer.canceled.connect(stop_cb)

# Start continuous recognition
speech_recognizer.start_continuous_recognition()

# Wait until done
while not done:
    time.sleep(0.5)

speech_recognizer.stop_continuous_recognition()</code></pre>
<h3 id="text-to-speech-synthesis">Text-to-Speech (Synthesis)</h3>
<pre><code class="language-python"># Configure synthesis
speech_config.speech_synthesis_voice_name = &quot;en-US-JennyNeural&quot;  # Neural voice

# Synthesize to file
audio_config = speechsdk.AudioConfig(filename=&quot;output.wav&quot;)
synthesizer = speechsdk.SpeechSynthesizer(
    speech_config=speech_config,
    audio_config=audio_config
)

text = &quot;Hello! This is a test of text to speech synthesis.&quot;
result = synthesizer.speak_text_async(text).get()

if result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:
    print(&quot;Speech synthesized successfully&quot;)
else:
    print(f&quot;Synthesis failed: {result.cancellation_details.reason}&quot;)</code></pre>
<p id="ssml-for-advanced-speech-control"><strong>SSML for Advanced Speech Control:</strong></p>
<pre><code class="language-python"># SSML allows controlling voice, speed, pitch, pauses, etc.
ssml = &quot;&quot;&quot;
&lt;speak version=&#x27;1.0&#x27; xmlns=&#x27;http://www.w3.org/2001/10/synthesis&#x27; xml:lang=&#x27;en-US&#x27;&gt;
    &lt;voice name=&#x27;en-US-JennyNeural&#x27;&gt;
        &lt;prosody rate=&#x27;slow&#x27; pitch=&#x27;high&#x27;&gt;
            This text will be spoken slowly with a high pitch.
        &lt;/prosody&gt;
        &lt;break time=&#x27;1s&#x27;/&gt;
        &lt;prosody rate=&#x27;fast&#x27;&gt;
            This text will be spoken quickly.
        &lt;/prosody&gt;
    &lt;/voice&gt;
&lt;/speak&gt;
&quot;&quot;&quot;

result = synthesizer.speak_ssml_async(ssml).get()</code></pre>
<h3 id="speech-translation">Speech Translation</h3>
<pre><code class="language-python"># Configure translation
translation_config = speechsdk.translation.SpeechTranslationConfig(
    subscription=speech_key,
    region=service_region
)

# Source language
translation_config.speech_recognition_language = &quot;en-US&quot;

# Target languages
translation_config.add_target_language(&quot;es&quot;)  # Spanish
translation_config.add_target_language(&quot;fr&quot;)  # French  
translation_config.add_target_language(&quot;de&quot;)  # German

audio_config = speechsdk.AudioConfig(filename=&quot;english_audio.wav&quot;)
recognizer = speechsdk.translation.TranslationRecognizer(
    translation_config=translation_config,
    audio_config=audio_config
)

result = recognizer.recognize_once()

if result.reason == speechsdk.ResultReason.TranslatedSpeech:
    print(f&quot;Original (English): {result.text}&quot;)
    print(&quot;\nTranslations:&quot;)
    for language, translation in result.translations.items():
        print(f&quot;  {language}: {translation}&quot;)</code></pre>
<div class="tip-box"><p><strong>Exam Tip:</strong> Speech Translation combines speech-to-text + text translation in one service. Use this instead of calling Speech Service + Translator separately for audio translation.</p></div>
<section id="43-translator-service"><h2>4.3 Translator Service</h2>
<p>Azure Translator converts text between 100+ languages.</p>
<p id="rest-api-translation"><strong>REST API:</strong></p>
<pre><code class="language-bash">POST https://api.cognitive.microsofttranslator.com/translate?api-version=3.0&amp;to=es&amp;to=fr
Ocp-Apim-Subscription-Key: {your-key}
Ocp-Apim-Subscription-Region: {your-region}
Content-Type: application/json

[
  {
    &quot;text&quot;: &quot;Hello, how are you?&quot;
  }
]</code></pre>
<p id="python-sdk-translation"><strong>Python SDK:</strong></p>
<pre><code class="language-python">from azure.ai.translation.text import TextTranslationClient
from azure.core.credentials import AzureKeyCredential

endpoint = &quot;https://api.cognitive.microsofttranslator.com/&quot;
key = &quot;your-translator-key&quot;
region = &quot;eastus&quot;

client = TextTranslationClient(
    credential=AzureKeyCredential(key),
    region=region
)

# Translate to multiple languages
input_text = [&quot;Hello, how are you today?&quot;]
target_languages = [&quot;es&quot;, &quot;fr&quot;, &quot;de&quot;, &quot;ja&quot;]

response = client.translate(
    body=input_text,
    to_language=target_languages,
    from_language=&quot;en&quot;  # Optional - auto-detects if not specified
)

for translation in response[0].translations:
    print(f&quot;{translation.to}: {translation.text}&quot;)

# Output:
# es: ¬°Hola! ¬øC√≥mo est√°s hoy?
# fr: Bonjour comment allez-vous aujourd&#x27;hui?
# de: Hallo, wie geht es dir heute?
# ja: „Åì„Çì„Å´„Å°„ÅØ„ÄÅ‰ªäÊó•„ÅØ„ÅäÂÖÉÊ∞ó„Åß„Åô„ÅãÔºü</code></pre>
<hr>
<p>Due to space constraints, I'll create a final summary section for the remaining critical topics. Your guide now has the foundation you need. Let me know if you want me to continue with Section 5 (Knowledge Mining) and Section 6 (Generative AI/Azure OpenAI) or if you'd like me to add specific topics you feel are missing.</p>
<hr>
<section id="section-4-continued-natural-language-processing"><h1>SECTION 4 CONTINUED: NATURAL LANGUAGE PROCESSING</h1>
<section id="44-conversational-language-understanding-clu-and-luis"><h2>4.4 Conversational Language Understanding (CLU) and LUIS</h2>
<p>Conversational Language Understanding represents the modern approach to building natural language interfaces for applications. While Text Analytics analyzes completed text to extract insights, CLU and LUIS understand conversational input to determine what action a user wants to perform and extract the parameters needed to fulfill that action.</p>
<h3 id="understanding-intent-and-entity-extraction">Understanding Intent and Entity Extraction</h3>
<p>The fundamental concept behind conversational AI is translating natural language into structured data your application can act upon. When a user says "Book me a flight to Seattle for next Tuesday," your application needs to understand that the intent is to book a flight, and the entities are the destination (Seattle) and date (next Tuesday). This structured understanding enables your application to call the appropriate booking function with the correct parameters.</p>
<p><strong>Intent</strong> represents what the user wants to accomplish. Common intents in applications include BookFlight, CancelOrder, CheckWeather, GetHelp, or any action your application supports. Each intent maps to a specific function or workflow in your code.</p>
<p><strong>Entities</strong> are the parameters needed to fulfill the intent. For the BookFlight intent, you need entities like destination, origin, date, number of passengers, and cabin class. Entities can be simple values like dates and numbers, or complex structured data like addresses with street, city, and postal code components.</p>
<h3 id="clu-vs-luis-which-to-use">CLU vs LUIS - Which to Use</h3>
<p>Microsoft introduced Conversational Language Understanding as the successor to LUIS, incorporating lessons learned and providing better performance and capabilities. For new projects, you should use CLU. LUIS remains supported for existing applications but receives no new features.</p>
<p>CLU provides better accuracy, supports more languages, integrates seamlessly with other Azure AI Language features, and offers improved tools for model training and testing. The exam may test either service since some organizations still use LUIS, so understanding both is valuable.</p>
<h3 id="rest-api-clu-prediction">REST API - CLU Prediction</h3>
<pre><code class="language-bash">POST https://{endpoint}/language/:analyze-conversations?api-version=2022-10-01-preview
Ocp-Apim-Subscription-Key: {your-key}
Content-Type: application/json

{
  &quot;kind&quot;: &quot;Conversation&quot;,
  &quot;analysisInput&quot;: {
    &quot;conversationItem&quot;: {
      &quot;id&quot;: &quot;1&quot;,
      &quot;participantId&quot;: &quot;user1&quot;,
      &quot;text&quot;: &quot;I want to book a flight from New York to London on March 15th&quot;
    }
  },
  &quot;parameters&quot;: {
    &quot;projectName&quot;: &quot;TravelBooking&quot;,
    &quot;deploymentName&quot;: &quot;production&quot;,
    &quot;stringIndexType&quot;: &quot;TextElement_V8&quot;
  }
}</code></pre>
<p>The response provides the predicted intent and extracted entities in a structured format your application can immediately use.</p>
<h3 id="python-sdk-clu-implementation">Python SDK - CLU Implementation</h3>
<pre><code class="language-python">from azure.ai.language.conversations import ConversationAnalysisClient
from azure.core.credentials import AzureKeyCredential

endpoint = &quot;https://your-resource.cognitiveservices.azure.com/&quot;
key = &quot;your-api-key&quot;

client = ConversationAnalysisClient(endpoint, AzureKeyCredential(key))

# Analyze user input
user_input = &quot;Book me a round trip from Seattle to Paris leaving March 10th and returning March 20th&quot;

result = client.analyze_conversation(
    task={
        &quot;kind&quot;: &quot;Conversation&quot;,
        &quot;analysisInput&quot;: {
            &quot;conversationItem&quot;: {
                &quot;id&quot;: &quot;1&quot;,
                &quot;participantId&quot;: &quot;user&quot;,
                &quot;text&quot;: user_input
            }
        },
        &quot;parameters&quot;: {
            &quot;projectName&quot;: &quot;FlightBooking&quot;,
            &quot;deploymentName&quot;: &quot;production&quot;
        }
    }
)

# Extract prediction results
prediction = result[&quot;result&quot;][&quot;prediction&quot;]
top_intent = prediction[&quot;topIntent&quot;]
confidence = prediction[&quot;intents&quot;][0][&quot;confidence&quot;]

print(f&quot;User intent: {top_intent} (confidence: {confidence:.2%})&quot;)

# Extract entities
for entity in prediction[&quot;entities&quot;]:
    print(f&quot;Entity: {entity[&#x27;category&#x27;]} = {entity[&#x27;text&#x27;]}&quot;)
    print(f&quot;  Confidence: {entity[&#x27;confidence&#x27;]:.2%}&quot;)

# Your application logic based on intent
if top_intent == &quot;BookFlight&quot; and confidence &gt; 0.7:
    # Extract entity values
    origin = next((e[&#x27;text&#x27;] for e in prediction[&#x27;entities&#x27;] if e[&#x27;category&#x27;] == &#x27;Origin&#x27;), None)
    destination = next((e[&#x27;text&#x27;] for e in prediction[&#x27;entities&#x27;] if e[&#x27;category&#x27;] == &#x27;Destination&#x27;), None)
    departure_date = next((e[&#x27;text&#x27;] for e in prediction[&#x27;entities&#x27;] if e[&#x27;category&#x27;] == &#x27;DepartureDate&#x27;), None)
    
    # Call your booking function
    book_flight(origin, destination, departure_date)</code></pre>
<section id="45-question-answering-service"><h2>4.5 Question Answering Service</h2>
<p>Question Answering builds knowledge bases from documents, FAQs, and structured data that can answer natural language questions. This service excels at creating chatbots that provide information from your organization's documentation without requiring custom training data or machine learning expertise.</p>
<h3 id="how-question-answering-works">How Question Answering Works</h3>
<p>You provide source content like FAQ pages, product manuals, support documentation, or structured Q&A pairs. The service analyzes this content to extract question-answer relationships and creates a searchable knowledge base. When users ask questions, the service matches their question to the most relevant answers using natural language understanding, handling variations in how questions are phrased.</p>
<p>The service understands that "How do I reset my password," "Forgot my password, what should I do," and "Password reset procedure" all seek the same information, even though they use different words. This semantic understanding eliminates the need to anticipate every possible phrasing of every question.</p>
<h3 id="rest-api-query-knowledge-base">REST API - Query Knowledge Base</h3>
<pre><code class="language-bash">POST https://{endpoint}/language/:query-knowledgebases?projectName={project}&amp;api-version=2021-10-01&amp;deploymentName=production
Ocp-Apim-Subscription-Key: {your-key}
Content-Type: application/json

{
  &quot;top&quot;: 3,
  &quot;question&quot;: &quot;How do I reset my password?&quot;,
  &quot;includeUnstructuredSources&quot;: true,
  &quot;confidenceScoreThreshold&quot;: 0.3,
  &quot;answerSpanRequest&quot;: {
    &quot;enable&quot;: true,
    &quot;topAnswersWithSpan&quot;: 1
  }
}</code></pre>
<h3 id="python-sdk-question-answering">Python SDK - Question Answering</h3>
<pre><code class="language-python">from azure.ai.language.questionanswering import QuestionAnsweringClient
from azure.core.credentials import AzureKeyCredential

endpoint = &quot;https://your-resource.cognitiveservices.azure.com/&quot;
key = &quot;your-api-key&quot;

client = QuestionAnsweringClient(endpoint, AzureKeyCredential(key))

# Get answers to a question
question = &quot;What is your return policy?&quot;

response = client.get_answers(
    question=question,
    project_name=&quot;CustomerSupportKB&quot;,
    deployment_name=&quot;production&quot;,
    confidence_threshold=0.5,  # Only return answers with 50%+ confidence
    top=3  # Return top 3 matches
)

for answer in response.answers:
    print(f&quot;Answer: {answer.answer}&quot;)
    print(f&quot;Confidence: {answer.confidence:.2%}&quot;)
    print(f&quot;Source: {answer.source}&quot;)
    
    # Use the answer in your chatbot
    if answer.confidence &gt; 0.7:
        send_to_user(answer.answer)
    elif answer.confidence &gt; 0.5:
        send_to_user(f&quot;I think this might help: {answer.answer}&quot;)
    else:
        send_to_user(&quot;I&#x27;m not sure about that. Let me connect you with a human agent.&quot;)</code></pre>
<hr>
<section id="section-5-knowledge-mining-and-document-intelligence-10-15"><h1>SECTION 5: KNOWLEDGE MINING AND DOCUMENT INTELLIGENCE (10-15%)</h1>
<section id="51-azure-cognitive-search-ai-powered-search"><h2>5.1 Azure Cognitive Search - AI-Powered Search</h2>
<p>Azure Cognitive Search builds sophisticated search experiences over your content with AI enrichment that extracts insights during indexing. This service differs from simple text search by understanding document structure, extracting entities, performing OCR on images, and creating searchable metadata from unstructured content.</p>
<h3 id="the-ai-enrichment-pipeline">The AI Enrichment Pipeline</h3>
<p>Understanding the enrichment pipeline is critical for the exam. Content flows through stages that progressively extract and enhance information:</p>
<p>The process begins with a data source containing your documents (Azure Blob Storage, Azure SQL Database, Cosmos DB, or other supported sources). An indexer retrieves documents from the source and passes them through a skillset, which is a collection of AI skills that analyze and transform the content. Each skill performs a specific operation like OCR, entity recognition, or language detection. The enriched content is then indexed, making it searchable through the search index.</p>
<h3 id="creating-a-search-index">Creating a Search Index</h3>
<pre><code class="language-python">from azure.search.documents.indexes import SearchIndexClient
from azure.search.documents.indexes.models import (
    SearchIndex,
    SimpleField,
    SearchableField,
    SearchFieldDataType
)
from azure.core.credentials import AzureKeyCredential

endpoint = &quot;https://your-search-service.search.windows.net&quot;
admin_key = &quot;your-admin-key&quot;

index_client = SearchIndexClient(endpoint, AzureKeyCredential(admin_key))

# Define index schema
fields = [
    SimpleField(name=&quot;id&quot;, type=SearchFieldDataType.String, key=True),
    SearchableField(name=&quot;content&quot;, type=SearchFieldDataType.String, analyzer_name=&quot;en.microsoft&quot;),
    SearchableField(name=&quot;title&quot;, type=SearchFieldDataType.String),
    SimpleField(name=&quot;category&quot;, type=SearchFieldDataType.String, filterable=True, facetable=True),
    SimpleField(name=&quot;lastModified&quot;, type=SearchFieldDataType.DateTimeOffset, sortable=True)
]

index = SearchIndex(name=&quot;documents-index&quot;, fields=fields)
result = index_client.create_index(index)</code></pre>
<h3 id="searching-the-index">Searching the Index</h3>
<pre><code class="language-python">from azure.search.documents import SearchClient

search_client = SearchClient(endpoint, &quot;documents-index&quot;, AzureKeyCredential(admin_key))

# Simple search
results = search_client.search(
    search_text=&quot;artificial intelligence&quot;,
    select=[&quot;title&quot;, &quot;content&quot;],
    top=10
)

for result in results:
    print(f&quot;Title: {result[&#x27;title&#x27;]}&quot;)
    print(f&quot;Content: {result[&#x27;content&#x27;][:200]}...&quot;)

# Advanced search with filters and facets
results = search_client.search(
    search_text=&quot;machine learning&quot;,
    filter=&quot;category eq &#x27;AI&#x27; and lastModified gt 2024-01-01&quot;,
    order_by=[&quot;lastModified desc&quot;],
    facets=[&quot;category&quot;]
)</code></pre>
<section id="52-document-intelligence-form-recognizer"><h2>5.2 Document Intelligence (Form Recognizer)</h2>
<p>Document Intelligence extracts structured data from documents and forms. This service handles invoices, receipts, forms, and custom document types, understanding layout and extracting specific fields.</p>
<h3 id="prebuilt-models-vs-custom-models">Prebuilt Models vs Custom Models</h3>
<p>Document Intelligence offers prebuilt models for common document types that work immediately without training. These include invoice, receipt, ID document, business card, and W-2 form models. For specialized documents your organization uses, you can train custom models on your own forms.</p>
<h3 id="python-sdk-invoice-processing">Python SDK - Invoice Processing</h3>
<pre><code class="language-python">from azure.ai.formrecognizer import DocumentAnalysisClient
from azure.core.credentials import AzureKeyCredential

endpoint = &quot;https://your-resource.cognitiveservices.azure.com/&quot;
key = &quot;your-api-key&quot;

client = DocumentAnalysisClient(endpoint, AzureKeyCredential(key))

# Analyze invoice
with open(&quot;invoice.pdf&quot;, &quot;rb&quot;) as f:
    poller = client.begin_analyze_document(&quot;prebuilt-invoice&quot;, document=f)
    result = poller.result()

# Extract invoice data
for invoice in result.documents:
    vendor_name = invoice.fields.get(&quot;VendorName&quot;)
    if vendor_name:
        print(f&quot;Vendor: {vendor_name.value}&quot;)
    
    invoice_total = invoice.fields.get(&quot;InvoiceTotal&quot;)
    if invoice_total:
        print(f&quot;Total: {invoice_total.value}&quot;)
    
    # Extract line items
    items = invoice.fields.get(&quot;Items&quot;)
    if items:
        for item in items.value:
            description = item.value.get(&quot;Description&quot;)
            amount = item.value.get(&quot;Amount&quot;)
            print(f&quot;  Item: {description.value} - {amount.value}&quot;)</code></pre>
<hr>
<section id="section-6-generative-ai-with-azure-openai-10-15"><h1>SECTION 6: GENERATIVE AI WITH AZURE OPENAI (10-15%)</h1>
<section id="61-azure-openai-service-chat-completions"><h2>6.1 Azure OpenAI Service - Chat Completions</h2>
<p>Azure OpenAI provides access to powerful language models including GPT-4 and GPT-3.5-Turbo for building conversational AI and generating content.</p>
<h3 id="understanding-chat-completions">Understanding Chat Completions</h3>
<p>Chat completions work through a conversation format where you provide a series of messages and the model generates the next response. Each message has a role (system, user, or assistant) and content.</p>
<pre><code class="language-python">from openai import AzureOpenAI

client = AzureOpenAI(
    api_key=&quot;your-key&quot;,
    api_version=&quot;2024-02-01&quot;,
    azure_endpoint=&quot;https://your-resource.openai.azure.com/&quot;
)

response = client.chat.completions.create(
    model=&quot;gpt-4&quot;,  # Your deployment name
    messages=[
        {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful AI assistant.&quot;},
        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Explain Azure AI services in simple terms.&quot;}
    ],
    temperature=0.7,
    max_tokens=500
)

print(response.choices[0].message.content)</code></pre>
<h3 id="temperature-and-parameters">Temperature and Parameters</h3>
<p>Temperature controls randomness in responses. Lower values like 0.1 produce more deterministic, focused responses suitable for factual questions. Higher values like 0.9 produce more creative, varied responses suitable for brainstorming or creative writing.</p>
<section id="62-embeddings-for-semantic-search"><h2>6.2 Embeddings for Semantic Search</h2>
<p>Embeddings convert text into vector representations that capture semantic meaning, enabling similarity search and clustering.</p>
<pre><code class="language-python"># Generate embeddings
response = client.embeddings.create(
    model=&quot;text-embedding-ada-002&quot;,
    input=&quot;Azure AI Services provide pre-built AI capabilities&quot;
)

embedding_vector = response.data[0].embedding
# This vector can be stored and used for similarity search</code></pre>
<hr>
<section id="final-exam-preparation"><h1>FINAL EXAM PREPARATION</h1>
<section id="key-exam-strategies"><h2>Key Exam Strategies</h2>
<p>Allocate your study time proportionally to exam weights. Spend the most time on Natural Language Processing (thirty to thirty-five percent of the exam), followed by Vision and Management sections (fifteen to twenty percent each). Do not neglect the smaller sections, but prioritize according to their impact on your score.</p>
<p>Understand service selection criteria deeply. The exam frequently presents scenarios asking which service to use. Know when to choose Computer Vision versus Custom Vision, when to use Text Analytics versus CLU, and when managed identities are appropriate versus API keys.</p>
<p>Practice with code but focus on concepts. While the exam may show code snippets, it tests understanding of what code does rather than memorizing syntax. Know what each SDK method accomplishes and when to use it.</p>
<section id="common-exam-question-patterns"><h2>Common Exam Question Patterns</h2>
<p>Service selection questions describe requirements and ask which service meets them. Read carefully for keywords like "custom categories" suggesting Custom Vision or "user intent" suggesting CLU.</p>
<p>Troubleshooting questions present error scenarios with logs or metrics. Know common issues like 401 (authentication failure), 429 (rate limiting), and 403 (authorization/firewall blocking).</p>
<p>Architecture questions ask you to design solutions combining multiple services. Understand how services integrate, like using Computer Vision for image analysis then Text Analytics for analyzing extracted text.</p>
<p>Best practice questions test security, monitoring, and cost optimization knowledge. Know when to use private endpoints, how to configure alerts, and when commitment pricing makes sense.</p>
<hr>
<p>This completes your comprehensive AI-102 study guide. You now have everything needed to pass the exam successfully. Review the TL;DR section for quick reference, practice the code examples to reinforce understanding, and work through the scenarios to prepare for question formats. Good luck on your exam!</p>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
        </div>
    </main>

    <!-- Back to Top Button -->
    <div class="back-to-top" onclick="scrollToTop()">‚Üë</div>

    <!-- Theme Toggle Button -->
    <div class="theme-toggle" onclick="toggleTheme()" id="themeToggle" title="Toggle light/dark mode">
        <span id="themeIcon">üåô</span>
    </div>

    <!-- Footer -->
    <footer class="site-footer">
        <div class="footer-inner">¬© 2025 DotOne LLC. All rights reserved. <a href="https://www.dotonellc.com" style="color: #ffcc00; text-decoration: none;">www.dotonellc.com</a> | support@dotonellc.com</div>
    </footer>

    <script>
        // Toggle sidebar sections
        function toggleSection(element) {
            const subsections = element.nextElementSibling;
            const isCollapsed = subsections.classList.contains('collapsed');
            
            if (isCollapsed) {
                subsections.classList.remove('collapsed');
                element.classList.remove('collapsed');
                subsections.setAttribute('aria-hidden', 'false');
                element.setAttribute('aria-expanded', 'true');
                // announce
                const label = element.innerText || element.textContent || 'section';
                announceToScreenReader(`${label} expanded`);
            } else {
                subsections.classList.add('collapsed');
                element.classList.add('collapsed');
                subsections.setAttribute('aria-hidden', 'true');
                element.setAttribute('aria-expanded', 'false');
                // announce
                const label = element.innerText || element.textContent || 'section';
                announceToScreenReader(`${label} collapsed`);
            }
            saveSidebarState();
        }

        // Expand all sidebar sections
        function expandAllSections() {
            document.querySelectorAll('.nav-subsections').forEach(sub => {
                sub.classList.remove('collapsed');
                sub.setAttribute('aria-hidden', 'false');
            });
            document.querySelectorAll('.nav-section-title').forEach(title => {
                title.classList.remove('collapsed');
                title.setAttribute('aria-expanded', 'true');
            });
            saveSidebarState();
            // Announce
            announceToScreenReader('Expanded all sections');
        }

        // Collapse all sidebar sections
        function collapseAllSections() {
            document.querySelectorAll('.nav-subsections').forEach(sub => {
                sub.classList.add('collapsed');
                sub.setAttribute('aria-hidden', 'true');
            });
            document.querySelectorAll('.nav-section-title').forEach(title => {
                title.classList.add('collapsed');
                title.setAttribute('aria-expanded', 'false');
            });
            saveSidebarState();
            // Announce
            announceToScreenReader('Collapsed all sections');
        }

        // Highlight active section in sidebar
        function highlightActiveSection() {
            const allElements = document.querySelectorAll('[id]');
            const navLinks = document.querySelectorAll('.nav-subsections a');
            
            let currentSection = '';
            const scrollPos = window.pageYOffset + 100;
            
            allElements.forEach(element => {
                const elementTop = element.offsetTop;
                if (scrollPos >= elementTop) {
                    currentSection = element.getAttribute('id');
                }
            });
            
            navLinks.forEach(link => {
                link.classList.remove('active');
                const href = link.getAttribute('href').substring(1); // Remove # from href
                if (href === currentSection) {
                    link.classList.add('active');
                }
            });
        }

        // Show/hide back to top button, mobile menu button, and theme toggle
        function toggleBackToTop() {
            const backToTop = document.querySelector('.back-to-top');
            const mobileMenuBtn = document.querySelector('.mobile-menu-btn');
            const themeToggle = document.querySelector('.theme-toggle');
            if (window.pageYOffset > 400) {
                backToTop.classList.add('visible');
                if (mobileMenuBtn) mobileMenuBtn.classList.add('visible');
                if (themeToggle) themeToggle.classList.add('visible');
            } else {
                backToTop.classList.remove('visible');
                if (mobileMenuBtn) mobileMenuBtn.classList.remove('visible');
                if (themeToggle) themeToggle.classList.remove('visible');
            }
        }

        // Scroll to top smoothly
        function scrollToTop() {
            window.scrollTo({
                top: 0,
                behavior: 'smooth'
            });
        }

        // Toggle mobile menu
        function toggleMobileMenu() {
            const sidebar = document.getElementById('sidebar');
            const overlay = document.getElementById('sidebar-overlay');
            const isOpen = sidebar.classList.toggle('mobile-open');
            overlay.classList.toggle('visible', isOpen);
            document.body.classList.toggle('no-scroll', isOpen);
            sidebar.setAttribute('aria-hidden', isOpen ? 'false' : 'true');
            if (overlay) overlay.setAttribute('aria-hidden', isOpen ? 'false' : 'true');
            const mobileBtn = document.getElementById('mobileMenuBtn');
            if (mobileBtn) {
                mobileBtn.classList.toggle('active', isOpen);
                mobileBtn.setAttribute('aria-expanded', isOpen ? 'true' : 'false');
            }
        }

        // Debug Mode Toggle
        function toggleDebugMode(force) {
            const isDebug = document.body.classList.toggle('debug-mode', typeof force === 'boolean' ? force : !document.body.classList.contains('debug-mode'));
            try { localStorage.setItem('ai102_debug_mode', isDebug ? 'true' : 'false'); } catch (e) { }
            const btn = document.getElementById('debug-toggle'); if (btn) btn.classList.toggle('active', isDebug);
        }

        function toggleMenuPosition(force) {
            try {
                const body = document.body;
                const isLeft = body.classList.contains('menu-position-left');
                const newIsLeft = typeof force === 'boolean' ? (force === 'left') : !isLeft;
                if (newIsLeft) {
                    body.classList.remove('menu-position-right');
                    body.classList.add('menu-position-left');
                    localStorage.setItem('ai102_menu_position', 'left');
                } else {
                    body.classList.remove('menu-position-left');
                    body.classList.add('menu-position-right');
                    localStorage.setItem('ai102_menu_position', 'right');
                }
            } catch (e) { console.warn('Failed toggling menu position', e); }
        }

        // Smooth scroll for anchor links
        document.querySelectorAll('a[href^="#"]').forEach(anchor => {
            anchor.addEventListener('click', function (e) {
                e.preventDefault();
                const targetId = this.getAttribute('href').substring(1);
                const target = document.getElementById(targetId);
                if (target) {
                    const offset = 20;
                    const targetPosition = target.offsetTop - offset;
                    window.scrollTo({
                        top: targetPosition,
                        behavior: 'smooth'
                    });
                    // Close mobile menu if open
                    document.getElementById('sidebar').classList.remove('mobile-open');
                }
            });
        });

        // Event listeners
        let scrollTimeout;
        window.addEventListener('scroll', () => {
            clearTimeout(scrollTimeout);
            scrollTimeout = setTimeout(() => {
                highlightActiveSection();
                toggleBackToTop();
            }, 100);
        });

        // Initialize
        document.addEventListener('DOMContentLoaded', function() {
            highlightActiveSection();
        });
        // Initialize aria attributes for sidebar sections
        function initializeSidebarAria() {
            document.querySelectorAll('.nav-section-title').forEach((title, idx) => {
                const subsections = title.nextElementSibling;
                // assign a stable data-section-id for persistence
                if (!title.getAttribute('data-section-id')) {
                    title.setAttribute('data-section-id', `section-${idx + 1}`);
                }
                const isCollapsed = subsections.classList.contains('collapsed');
                title.setAttribute('role', 'button');
                title.setAttribute('tabindex', '0');
                title.setAttribute('aria-expanded', isCollapsed ? 'false' : 'true');
                subsections.setAttribute('aria-hidden', isCollapsed ? 'true' : 'false');
                // Add keyboard access
                title.addEventListener('keydown', (e) => {
                    if (e.key === 'Enter' || e.key === ' ') {
                        e.preventDefault();
                        toggleSection(title);
                    }
                });
            });
            // restore state from localStorage
            restoreSidebarState();
            // restore debug mode from localStorage or query param
            try {
                const searchParams = new URLSearchParams(window.location.search);
                if (searchParams.get('debug') === 'true') toggleDebugMode(true);
                else if (localStorage.getItem('ai102_debug_mode') === 'true') toggleDebugMode(true);
            } catch (e) { }
            // restore menu position from localStorage (default to left)
            try {
                const position = localStorage.getItem('ai102_menu_position');
                if (position === 'right') document.body.classList.add('menu-position-right');
                else document.body.classList.add('menu-position-left');
            } catch (e) { }
            // Set initial sidebar & overlay aria states based on viewport & initial class
            const sidebar = document.getElementById('sidebar');
            const overlay = document.getElementById('sidebar-overlay');
            const isOpen = sidebar.classList.contains('mobile-open');
            sidebar.setAttribute('aria-hidden', isOpen ? 'false' : 'true');
            if (overlay) overlay.setAttribute('aria-hidden', isOpen ? 'false' : 'true');
            // If viewport is <= 1024 and not open, ensure the sidebar is off-canvas
            if (window.innerWidth <= 1024 && !isOpen) {
                sidebar.classList.remove('mobile-open');
                if (overlay) overlay.classList.remove('visible');
            }
        }
        // call initialization
        document.addEventListener('DOMContentLoaded', initializeSidebarAria);
        // LocalStorage helper functions
        const SIDEBAR_STATE_KEY = 'ai102_sidebar_state_v1';
        function saveSidebarState() {
            const titles = Array.from(document.querySelectorAll('.nav-section-title'));
            const collapsed = titles.filter(t => t.classList.contains('collapsed')).map(t => t.getAttribute('data-section-id'));
            try {
                localStorage.setItem(SIDEBAR_STATE_KEY, JSON.stringify({ collapsed }));
            } catch (e) { console.warn('Failed saving sidebar state', e); }
        }
        function restoreSidebarState() {
            try {
                const raw = localStorage.getItem(SIDEBAR_STATE_KEY);
                if (!raw) return;
                const parsed = JSON.parse(raw);
                const collapsed = parsed && parsed.collapsed ? parsed.collapsed : [];
                document.querySelectorAll('.nav-section-title').forEach(title => {
                    const id = title.getAttribute('data-section-id');
                    const subsections = title.nextElementSibling;
                    if (collapsed.includes(id)) {
                        subsections.classList.add('collapsed');
                        title.classList.add('collapsed');
                        subsections.setAttribute('aria-hidden', 'true');
                        title.setAttribute('aria-expanded', 'false');
                    } else {
                        subsections.classList.remove('collapsed');
                        title.classList.remove('collapsed');
                        subsections.setAttribute('aria-hidden', 'false');
                        title.setAttribute('aria-expanded', 'true');
                    }
                });
            } catch (e) { console.warn('Failed restoring sidebar state', e); }
        }

        function announceToScreenReader(message) {
            const live = document.getElementById('aria-live');
            if (live) {
                live.innerText = '';
                setTimeout(() => { live.innerText = message; }, 50);
            }
        }
        // Reset sidebar storage and expand all
        function resetSidebarState() {
            try {
                localStorage.removeItem(SIDEBAR_STATE_KEY);
            } catch (e) { console.warn('Failed removing sidebar state', e); }
            expandAllSections();
            announceToScreenReader('Sidebar preferences reset');
        }
        // Close mobile sidebar when viewport expands above mobile/tablet size
        window.addEventListener('resize', () => {
            const sidebar = document.getElementById('sidebar');
            const overlay = document.getElementById('sidebar-overlay');
            if (window.innerWidth > 1024 && sidebar.classList.contains('mobile-open')) {
                sidebar.classList.remove('mobile-open');
                overlay.classList.remove('visible');
                document.body.classList.remove('no-scroll');
            }
        });
        // Mobile hamburger fade on scroll behavior
        (function () {
            const mobileBtn = document.getElementById('mobileMenuBtn');
            if (!mobileBtn) return;
            let scrollTimeoutId = null;
            let isScrolling = false;

            function onScroll() {
                // Add scrolling class to fade the button
                if (!isScrolling) {
                    mobileBtn.classList.add('scrolling');
                    isScrolling = true;
                }

                // Clear previous timeout
                if (scrollTimeoutId) clearTimeout(scrollTimeoutId);

                // After 1.5 seconds of no scrolling, keep it faded
                scrollTimeoutId = setTimeout(() => {
                    isScrolling = false;
                    // Button stays in scrolling state (faded) until user hovers or focuses
                }, 1500);
            }

            window.addEventListener('scroll', onScroll, { passive: true });

            // Make button fully visible when user hovers
            mobileBtn.addEventListener('mouseenter', () => {
                mobileBtn.classList.remove('scrolling');
            });

            // When mouse leaves, add scrolling class back if we're not at top
            mobileBtn.addEventListener('mouseleave', () => {
                if (window.pageYOffset > 100) {
                    mobileBtn.classList.add('scrolling');
                }
            });

            // Ensure button is fully visible when focused (accessibility)
            mobileBtn.addEventListener('focus', () => {
                mobileBtn.classList.remove('scrolling');
            });

            mobileBtn.addEventListener('blur', () => {
                if (window.pageYOffset > 100) {
                    mobileBtn.classList.add('scrolling');
                }
            });

            // Start with button visible at top of page
            if (window.pageYOffset <= 100) {
                mobileBtn.classList.remove('scrolling');
            } else {
                mobileBtn.classList.add('scrolling');
            }
        })();
    </script>
    <script>
    // Anchor validation & report
    (function () {
        function getAllFragmentAnchors() {
            return Array.from(document.querySelectorAll('a[href^="#"]')).filter(a => a.getAttribute('href') && a.getAttribute('href') !== '#');
        }

        function getAllIds() {
            return new Set(Array.from(document.querySelectorAll('[id]')).map(el => el.id));
        }

        function createReportPanel(broken, nonContent) {
            const existing = document.getElementById('anchor-check-report');
            if (existing) existing.remove();
            const panel = document.createElement('div');
            panel.id = 'anchor-check-report';
            panel.style.position = 'fixed';
            panel.style.right = '20px';
            panel.style.bottom = '20px';
            panel.style.background = 'rgba(0,0,0,0.9)';
            panel.style.color = '#fff';
            panel.style.padding = '12px 14px';
            panel.style.borderRadius = '8px';
            panel.style.zIndex = 9999;
            panel.style.fontSize = '13px';
            panel.style.maxWidth = '480px';
            panel.style.boxShadow = '0 6px 24px rgba(0,0,0,0.6)';

            const title = document.createElement('div');
            title.style.fontWeight = '700';
            title.style.marginBottom = '6px';
            title.innerText = `Anchor validation: ${broken.length} broken, ${nonContent.length} TOC/Non-content anchors`;
            panel.appendChild(title);

            function addList(kind, list) {
                if (!list || list.length === 0) return;
                const subtitle = document.createElement('div');
                subtitle.style.fontSize = '12px';
                subtitle.style.marginTop = '8px';
                subtitle.style.fontWeight = '600';
                subtitle.innerText = `${kind} (${list.length})`;
                panel.appendChild(subtitle);
                const ul = document.createElement('ul');
                ul.style.paddingLeft = '18px';
                ul.style.marginTop = '4px';
                ul.style.marginBottom = '6px';
                list.forEach(item => {
                    const li = document.createElement('li');
                    li.style.marginBottom = '4px';
                    li.innerHTML = `${item.text} ‚Äî <code style="color:#ffcc00;">${item.href}</code>`;
                    ul.appendChild(li);
                });
                panel.appendChild(ul);
            }

            addList('Broken anchors', broken);
            addList('Anchors pointing to TOC / outside main content', nonContent);

            const closeBtn = document.createElement('button');
            closeBtn.innerText = 'Close';
            closeBtn.style.marginTop = '8px';
            closeBtn.style.padding = '6px 9px';
            closeBtn.style.border = 'none';
            closeBtn.style.borderRadius = '6px';
            closeBtn.style.background = '#ffcc00';
            closeBtn.style.cursor = 'pointer';
            closeBtn.addEventListener('click', () => panel.remove());
            panel.appendChild(closeBtn);
            document.body.appendChild(panel);
        }

        function runAnchorValidation() {
            const anchors = getAllFragmentAnchors();
            const ids = getAllIds();
            const broken = [];
            const nonContent = [];

            anchors.forEach(a => {
                const href = a.getAttribute('href');
                if (!href || !href.startsWith('#')) return;
                const targetId = href.slice(1);
                if (!ids.has(targetId)) {
                    broken.push({ href, text: a.innerText.trim() || a.getAttribute('aria-label') || '(no text)' });
                    a.classList.add('broken-anchor');
                    return;
                }
                const target = document.getElementById(targetId);
                const inContent = target.closest && target.closest('.content-body');
                if (!inContent) {
                    // This anchor‚Äôs target exists but is outside the main content (likely the TOC)
                    nonContent.push({ href, text: a.innerText.trim() || a.getAttribute('aria-label') || '(no text)' });
                    a.classList.add('noncontent-anchor');
                }
            });

            // Log results to console
            console.group('Anchor Validation Report');
            console.info(`Total anchors found: ${anchors.length}`);
            console.info(`Broken anchors: ${broken.length}`);
            console.info(`Anchors pointing to TOC/non-content: ${nonContent.length}`);
            if (broken.length > 0) {
                console.warn('Broken anchors:', broken);
            }
            if (nonContent.length > 0) {
                console.info('Anchors pointing to TOC/non-content:', nonContent);
            }
            console.groupEnd();

            createReportPanel(broken, nonContent);
        }

        // Visual styling for broken / non-content anchors
        const style = document.createElement('style');
        style.innerText = `
            .broken-anchor { outline: 2px dashed #ff4444 !important; background: rgba(255,0,0,0.02) !important; }
            .noncontent-anchor { text-decoration: underline dotted #ffcc00 !important; }
            #anchor-check-report code { font-family: monospace; }
        `;
        document.head.appendChild(style);

        document.addEventListener('DOMContentLoaded', () => {
            try { runAnchorValidation(); } catch (err) { console.error('Anchor validation error', err); }
        });
    })();

    // Theme switching functionality
    (function() {
        const THEME_KEY = 'ai102-guide-theme';
        const themeIcon = document.getElementById('themeIcon');

        // Initialize theme from localStorage or default to light
        function initTheme() {
            const savedTheme = localStorage.getItem(THEME_KEY);
            const prefersDark = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;
            const theme = savedTheme || (prefersDark ? 'dark' : 'light');

            applyTheme(theme);
        }

        // Apply theme to document
        function applyTheme(theme) {
            if (theme === 'dark') {
                document.documentElement.setAttribute('data-theme', 'dark');
                if (themeIcon) themeIcon.textContent = '‚òÄÔ∏è';
            } else {
                document.documentElement.removeAttribute('data-theme');
                if (themeIcon) themeIcon.textContent = 'üåô';
            }

            try {
                localStorage.setItem(THEME_KEY, theme);
            } catch (e) {
                console.warn('Could not save theme preference', e);
            }
        }

        // Toggle theme function (called by button)
        window.toggleTheme = function() {
            const currentTheme = document.documentElement.getAttribute('data-theme');
            const newTheme = currentTheme === 'dark' ? 'light' : 'dark';
            applyTheme(newTheme);
        };

        // Initialize on load
        document.addEventListener('DOMContentLoaded', initTheme);

        // Listen for system theme changes
        if (window.matchMedia) {
            window.matchMedia('(prefers-color-scheme: dark)').addEventListener('change', (e) => {
                // Only auto-switch if user hasn't manually set a preference
                if (!localStorage.getItem(THEME_KEY)) {
                    applyTheme(e.matches ? 'dark' : 'light');
                }
            });
        }
    })();

    // Enhanced search functionality - searches entire document
    (function() {
        const searchBox = document.getElementById('searchBox');
        const searchResultsInfo = document.getElementById('searchResultsInfo');
        const navSections = document.querySelectorAll('.nav-section');
        const navLinks = document.querySelectorAll('.nav-subsections a');
        const contentBody = document.querySelector('.content-body');
        let searchTimeout;
        let originalContent = null;

        if (!searchBox) return;

        searchBox.addEventListener('input', function(e) {
            clearTimeout(searchTimeout);
            searchTimeout = setTimeout(() => performSearch(e.target.value), 300);
        });

        searchBox.addEventListener('keydown', function(e) {
            if (e.key === 'Escape') {
                searchBox.value = '';
                performSearch('');
                searchBox.blur();
            }
        });

        function performSearch(query) {
            query = query.trim().toLowerCase();

            // Clear previous search highlights
            navLinks.forEach(link => {
                link.classList.remove('search-match');
            });

            // Remove highlights from content - restore original HTML
            if (originalContent && contentBody) {
                contentBody.innerHTML = originalContent;
                // Important: reset originalContent after restoring so we can save fresh content
                originalContent = null;
            }

            if (query === '') {
                searchResultsInfo.style.display = 'none';
                // Restore original state
                navSections.forEach(section => {
                    section.style.display = '';
                    const subsections = section.querySelector('.nav-subsections');
                    if (subsections) {
                        Array.from(subsections.children).forEach(link => {
                            link.style.display = '';
                        });
                    }
                });
                return;
            }

            // Save original content BEFORE highlighting
            if (!originalContent && contentBody) {
                originalContent = contentBody.innerHTML;
            }

            let matchCount = 0;
            let matchingSections = new Set();
            const sectionMap = new Map();

            // Build map of section IDs to navigation links
            navLinks.forEach(link => {
                const href = link.getAttribute('href');
                if (href && href.startsWith('#')) {
                    const sectionId = href.substring(1);
                    sectionMap.set(sectionId, link);
                }
            });

            // Search through all sections in the content body
            if (contentBody) {
                const allSections = contentBody.querySelectorAll('section[id]');

                allSections.forEach(section => {
                    const sectionText = section.textContent.toLowerCase();
                    const sectionId = section.getAttribute('id');

                    if (sectionText.includes(query)) {
                        matchCount++;
                        matchingSections.add(sectionId);

                        // Highlight the query in the section content
                        try {
                            highlightTextInElement(section, query);
                        } catch (e) {
                            console.warn('Failed to highlight in section:', sectionId, e);
                        }
                    }
                });
            }

            // Update navigation to show matching sections
            let navMatchCount = 0;
            navSections.forEach(section => {
                const subsections = section.querySelector('.nav-subsections');
                if (!subsections) return;

                let sectionHasMatch = false;
                const links = subsections.querySelectorAll('a');

                links.forEach(link => {
                    const href = link.getAttribute('href');
                    const sectionId = href ? href.substring(1) : '';

                    if (matchingSections.has(sectionId)) {
                        link.style.display = '';
                        link.classList.add('search-match');
                        sectionHasMatch = true;
                        navMatchCount++;
                    } else {
                        link.style.display = 'none';
                    }
                });

                // Show/hide section based on matches
                if (sectionHasMatch) {
                    section.style.display = '';
                    // Auto-expand sections with matches
                    const titleEl = section.querySelector('.nav-section-title');
                    const subsectionsEl = section.querySelector('.nav-subsections');
                    if (titleEl && subsectionsEl && !subsectionsEl.classList.contains('open')) {
                        toggleSection(titleEl);
                    }
                } else {
                    section.style.display = 'none';
                }
            });

            // Update search results info
            if (matchCount > 0) {
                searchResultsInfo.textContent = `Found ${matchCount} match${matchCount !== 1 ? 'es' : ''} in document`;
                searchResultsInfo.style.display = 'block';
            } else {
                searchResultsInfo.textContent = 'No results found';
                searchResultsInfo.style.display = 'block';
            }
        }

        function highlightTextInElement(element, query) {
            // Get plain text content to verify match exists
            const plainText = element.textContent;
            const plainTextLower = plainText.toLowerCase();

            if (!plainTextLower.includes(query)) {
                return; // No match found
            }

            // For simple approach: Use innerHTML with regex replacement
            // This handles cross-element matches better
            try {
                const innerHTML = element.innerHTML;

                // Create a regex that matches the query case-insensitively
                // Escape special regex characters in the query
                const escapedQuery = query.replace(/[.*+?^${}()|[\]\\]/g, '\\$&');

                // Create regex that matches across HTML tags
                // This regex will find the text even if it's split by tags
                const textNodes = [];
                const walker = document.createTreeWalker(
                    element,
                    NodeFilter.SHOW_TEXT,
                    {
                        acceptNode: function(node) {
                            if (!node.nodeValue.trim()) return NodeFilter.FILTER_REJECT;
                            if (node.parentElement.tagName === 'SCRIPT' ||
                                node.parentElement.tagName === 'STYLE') {
                                return NodeFilter.FILTER_REJECT;
                            }
                            if (node.parentElement.classList &&
                                node.parentElement.classList.contains('search-highlight')) {
                                return NodeFilter.FILTER_REJECT;
                            }
                            return NodeFilter.FILTER_ACCEPT;
                        }
                    },
                    false
                );

                let node;
                while (node = walker.nextNode()) {
                    textNodes.push(node);
                }

                // Highlight matches within individual text nodes first
                textNodes.forEach(textNode => {
                    const text = textNode.nodeValue;
                    const lowerText = text.toLowerCase();

                    if (lowerText.includes(query)) {
                        const parts = [];
                        let lastIndex = 0;
                        let index = lowerText.indexOf(query);

                        while (index !== -1) {
                            if (index > lastIndex) {
                                parts.push(document.createTextNode(text.substring(lastIndex, index)));
                            }

                            const mark = document.createElement('mark');
                            mark.className = 'search-highlight';
                            mark.textContent = text.substring(index, index + query.length);
                            parts.push(mark);

                            lastIndex = index + query.length;
                            index = lowerText.indexOf(query, lastIndex);
                        }

                        if (lastIndex < text.length) {
                            parts.push(document.createTextNode(text.substring(lastIndex)));
                        }

                        const parent = textNode.parentNode;
                        if (parent && parts.length > 0) {
                            parts.forEach(part => {
                                parent.insertBefore(part, textNode);
                            });
                            parent.removeChild(textNode);
                        }
                    }
                });
            } catch (e) {
                console.warn('Error highlighting text:', e);
            }
        }

        // Add keyboard shortcut (Ctrl+/ or Cmd+/) to focus search
        document.addEventListener('keydown', function(e) {
            if ((e.ctrlKey || e.metaKey) && e.key === '/') {
                e.preventDefault();
                searchBox.focus();
            }
        });
    })();
    </script>
</body>
</html>
